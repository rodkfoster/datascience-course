{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Statistics Fundamentals\n",
    "\n",
    "_Instructor:_ Alexander Egorenkov (DC), Amy Roberts (NYC) Tim Book, General Assembly DC_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"learning-objectives\"></a>\n",
    "## Learning Objectives\n",
    "- **Linear algebra:** Dot products, matrix multiplications, and vector norms by hand and using NumPy.\n",
    "- **Summary statistics:** Using NumPy and Pandas: mean, median, mode, max, min, quartile, inter-quartile range, variance, standard deviation, and correlation.\n",
    "- **Discover trends:** Using basic summary statistics and viz.\n",
    "- **Bias/variance tradeoff:** Describe the bias and variance of statistical estimators.\n",
    "- **Identify a normal distribution** within a data set using summary statistics and data visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Linear Algebra Review](#linear-algebra-review)\n",
    "    - [Scalars, Vectors, and Matrices](#scalars-vectors-and-matrices)\n",
    "\t- [Basic Matrix Algebra](#basic-matrix-algebra)\n",
    "\t- [Dot Product](#dot-product)\n",
    "\t- [Matrix Multiplication](#matrix-multiplication)\n",
    "\t- [Vector Norm](#vector-norm)\n",
    "- [Linear Algebra Applications to Machine Learning](#linear-algebra-applications-to-machine-learning)\n",
    "    - [Code-Along: Examining the Cars Data Set](#codealong-examining-the-cars-dataset)\n",
    "- [Descriptive Statistics Fundamentals](#descriptive-statistics-fundamentals)\n",
    "\t- [Measures of Central Tendency](#measures-of-central-tendency)\n",
    "\t- [Math Review](#math-review)\n",
    "\t- [Measures of Dispersion: Standard Deviation and Variance](#measures-of-dispersion-standard-deviation-and-variance)\n",
    "- [Our First Model](#our-first-model)\n",
    "- [A Short Introduction to Model Bias and Variance](#a-short-introduction-to-model-bias-and-variance)\n",
    "- [Correlation and Association](#correlation-and-association)\n",
    "- [The Normal Distribution](#the-normal-distribution)\n",
    "\t- [What is the Normal Distribution?](#what-is-the-normal-distribution)\n",
    "\t- [Skewness](#skewness)\n",
    "\t- [Kurtosis](#kurtosis)\n",
    "- [Determining the Distribution of Your Data](#determining-the-distribution-of-your-data)\n",
    "- [Lesson Review](#topic-review)\n",
    "- [Extra Lab - Feature Reduction](#lab-stats-dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T17:30:16.616907Z",
     "start_time": "2020-05-04T17:30:13.453233Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics # Did you catch this is new? \n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# This makes sure that graphs render in your notebook.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"where-are-we-in-the-data-science-workflow\"></a>\n",
    "## Where Are We in the Data Science Workflow?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Data Science Workflow](./assets/images/data-science-workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"linear-algebra-review\"></a>\n",
    "## Linear Algebra Review\n",
    "---\n",
    "**Objective:** Compute dot products, matrix multiplications, and vector norms by hand and using NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"why-linear-algebra\"></a>\n",
    "### Why Use Linear Algebra in Data Science?\n",
    "\n",
    "- Linear models are efficient and well understood. They can often closely approximate nonlinear solutions, and they scale to high dimensions without difficulty.\n",
    "\n",
    "\n",
    "- **Linear models are all based on linear algebra**, so we should know that too.\n",
    "\n",
    "\n",
    "- Furthermore, even the complicated models rely on the basic models, which in turn rely heavily on linear algebra.\n",
    "\n",
    "\n",
    "- Although we do not have time in this course to comprehensively discuss linear algebra, you may want to take time to understand it better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"scalars-vectors-and-matrices\"></a>\n",
    "### Scalars, Vectors, and Matrices\n",
    "\n",
    "<img src=\"assets/images/scalars-vectors-matrices.png\">\n",
    "\n",
    "\n",
    "A **scalar** is a single number. \n",
    "- Symbols that are lowercase single letters refer to scalars. For example, the symbols $a$ and $v$ are scalars that might refer to arbitrary numbers such as $5.328$ or $7$. \n",
    "\n",
    "- An example scalar would be: $a$\n",
    "\n",
    "- It's usually easy to consider vectors as either a $1 \\times n$ or $n \\times 1$ \"row\" or \"column\" vector, where convenient.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "A **vector** is an ordered sequence of numbers, **like a list**. \n",
    "- Unlike a Python list, a vector can only be numeric. It can be a row or a column.\n",
    "- Here, symbols that are lowercase single letters with an arrow — such as $\\vec{u}$ — refer to vectors. An example vector would be:\n",
    "\n",
    "$$\\vec{u} = \\left[ \\begin{array}{c}\n",
    "1&3&7\n",
    "\\end{array} \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T17:29:41.426239Z",
     "start_time": "2020-05-04T17:29:41.416233Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 7]\n",
      "11\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "# Create a vector using np.array.\n",
    "u = np.array([1, 3, 7])\n",
    "print(u)\n",
    "print(np.sum(u))\n",
    "print(u[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An $m$ x $n$ **matrix** is a rectangular array of numbers with $m$ rows and $n$ columns. Each number in the matrix is an entry. Entries can be denoted $a_{ij}$, where $i$ denotes the row number and $j$ denotes the column number. Note that, because each entry $a_{ij}$ is a lowercase single letter, a matrix is an array of scalars:\n",
    "\n",
    "$$\\mathbf{A}= \\left[ \\begin{array}{c}\n",
    "a_{11} & a_{12} & \\cdots & a_{1n}  \\\\\n",
    "a_{21} & a_{22} & \\cdots & a_{2n}  \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{m1} & a_{m2} & \\cdots & a_{mn}\n",
    "\\end{array} \\right]$$\n",
    "\n",
    "Matrices are referred to using bold uppercase letters, such as $\\mathbf{A}$. A bold font face is used to distinguish matrices from sets. (Sometimes, not always)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T17:29:44.544453Z",
     "start_time": "2020-05-04T17:29:44.529494Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 3, 7],\n",
       "       [4, 6, 3],\n",
       "       [2, 5, 6]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a matrix using np.array.\n",
    "m = np.array([[1, 3, 7], [4, 6, 3], [2, 5, 6]])\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in Python, a matrix is just a list of lists converted to numpy arrays(or a group of vectors)! **In fact, a vector is also matrix!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arrays are More Efficient than Pandas Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T17:30:18.003963Z",
     "start_time": "2020-05-04T17:30:17.997979Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    3\n",
      "2    7\n",
      "dtype: int32\n"
     ]
    }
   ],
   "source": [
    "s = pd.Series(u)\n",
    "print(s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T17:30:19.291889Z",
     "start_time": "2020-05-04T17:30:19.276929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127 ns ± 27.4 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10000 u[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T17:30:21.241522Z",
     "start_time": "2020-05-04T17:30:20.455624Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.1 µs ± 1.9 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10000 s[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"basic-matrix-algebra\"></a>\n",
    "### Basic Matrix Algebra\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Addition and Subtraction\n",
    "Vector **addition** is straightforward. If two vectors are of equal dimensions (The vectors are shown here as column vectors for convenience only):\n",
    "\n",
    "$\\vec{v} = \\left[ \\begin{array}{c}\n",
    "1 \\\\\n",
    "3 \\\\\n",
    "7\n",
    "\\end{array} \\right],  \\vec{w} = \\left[ \\begin{array}{c}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "1\n",
    "\\end{array} \\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T18:01:04.371923Z",
     "start_time": "2020-05-04T18:01:04.366933Z"
    }
   },
   "outputs": [],
   "source": [
    "v = np.array([1, 3, 7])\n",
    "w = np.array([1, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\vec{v} + \\vec{w} =\n",
    "\\left[ \\begin{array}{c}\n",
    "1 \\\\\n",
    "3 \\\\\n",
    "7\n",
    "\\end{array} \\right] + \\left[ \\begin{array}{c}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "1\n",
    "\\end{array} \\right] = \n",
    "\\left[ \\begin{array}{c}\n",
    "1+1 \\\\\n",
    "3+0 \\\\\n",
    "7+1\n",
    "\\end{array} \\right] = \n",
    "\\left[ \\begin{array}{c}\n",
    "2 \\\\\n",
    "3 \\\\\n",
    "8\n",
    "\\end{array} \\right]\n",
    "$\n",
    "\n",
    "(Subtraction is similar.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add the vectors together with +.\n",
    "v+w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now using numpy (np.sum)\n",
    "np.sum([v,w], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T17:31:25.157116Z",
     "start_time": "2020-05-04T17:31:25.150167Z"
    }
   },
   "source": [
    "**Classroom Question**: What happens when **axis=1**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T17:32:04.884922Z",
     "start_time": "2020-05-04T17:32:04.878938Z"
    }
   },
   "source": [
    "**Classroom Exercise**:\n",
    "Subtract the vectors.  Write it out by hand and then allow Python to do the work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract the vectors using the similar methods that were used for addition\n",
    "# the - operator\n",
    "#np.subtract([v,w], axis=0)\n",
    "#help(np.subtract)\n",
    "v-w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scalar Multiplication\n",
    "We scale a vector with **scalar multiplication**, multiplying a vector by a scalar (single quantity):\n",
    "\n",
    "$ 2 \\cdot \\vec{v} = 2\\left[ \\begin{array}{c}\n",
    "1 \\\\\n",
    "3 \\\\\n",
    "7\n",
    "\\end{array} \\right] = \n",
    " \\left[ \\begin{array}{c}\n",
    "2 \\cdot 1 \\\\\n",
    "2 \\cdot 3 \\\\\n",
    "2 \\cdot 7\n",
    "\\end{array} \\right] = \n",
    " \\left[ \\begin{array}{c}\n",
    "2 \\\\\n",
    "6 \\\\\n",
    "14\n",
    "\\end{array} \\right]$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Multiply v by 2.\n",
    "2*v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply w and v\n",
    "W*V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dot-product\"></a>\n",
    "### Dot Product\n",
    "The **dot product** of two _n_-dimensional vectors is:\n",
    "\n",
    "$ \\vec{v} \\cdot \\vec{w} =\\sum _{i=1}^{n}v_{i}w_{i}=v_{1}w_{1}+v_{2}w_{2}+\\cdots +v_{n}w_{n} $\n",
    "\n",
    "So, if:\n",
    "\n",
    "$\\vec{v} = \\left[ \\begin{array}{c}\n",
    "1 \\\\\n",
    "3 \\\\\n",
    "7\n",
    "\\end{array} \\right], \\vec{w} = \\left[ \\begin{array}{c}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "1\n",
    "\\end{array} \\right]$\n",
    "\n",
    "$ \\vec{v} \\cdot \\vec{w} = 1 \\cdot 1 + 3 \\cdot 0 + 7 \\cdot 1 = 8 $\n",
    "\n",
    "_Tim Note:_ When considering vectors as \"column vectors\", you will often see a dot product written as $\\mathbf{v}^T\\mathbf{w}$. In more pure-math based literature, you might even see $\\langle v, w \\rangle$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T18:01:06.121063Z",
     "start_time": "2020-05-04T18:01:06.115080Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the dot product of v and w using np.dot.\n",
    "np.dot(v,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T18:01:11.313697Z",
     "start_time": "2020-05-04T18:01:11.308742Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try the .dot() method on v to do the same\n",
    "# How could you have found this option?\n",
    "v.dot(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"matrix-multiplication\"></a>\n",
    "### Matrix Multiplication\n",
    "**Matrix multiplication**, $\\mathbf{AB}$, is valid when the left matrix has the same number of columns as the right matrix has rows. Each entry is the dot product of corresponding row and column vectors.\n",
    "\n",
    "![](assets/images/matrix-multiply-a.gif)\n",
    "(Image: mathisfun.com)\n",
    "\n",
    "\n",
    "![](assets/images/matrix-multiplication-song.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot product illustrated above is: $1 \\cdot 7 + 2 \\cdot 9 + 3 \\cdot 11 = 58$. **Can you compute the rest of the dot products by hand?**\n",
    "\n",
    "If the product is the $2$ x $2$ matrix $\\mathbf{C}$, then:\n",
    "\n",
    "+ Matrix entry $c_{12}$ (its FIRST row and SECOND column) is the dot product of the FIRST row of $\\mathbf{A}$ and the SECOND column of $\\mathbf{B}$.\n",
    "\n",
    "+ Matrix entry $c_{21}$ (its SECOND row and FIRST column) is the dot product of the SECOND row of $\\mathbf{A}$ and the FIRST column of $\\mathbf{B}$.\n",
    "\n",
    "**Lets compute the example above, with the $2$ x $3$ matrix multiplied by $3$ x $2$ matrix, which results in a $2$ x $2$ matrix. Can you see why?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply the two above matrices\n",
    "\n",
    "#Make them first\n",
    "A = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "B = np.array([[7, 8],[9,10], [11, 12]])\n",
    "\n",
    "#now multiply!\n",
    "\n",
    "C= np.dot(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T17:34:37.499257Z",
     "start_time": "2020-05-04T17:34:37.495267Z"
    }
   },
   "outputs": [],
   "source": [
    "# Subset C to show the value in the first row and second column (upper right value!)\n",
    "print(C)\n",
    "C[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"n-dimensional-space\"></a>\n",
    "### N-Dimensional Space\n",
    "\n",
    "We often refer to vectors as elements of an $n$-dimensional space. The symbol $\\mathbb{R}$ refers to the set of all real numbers (written in uppercase \"blackboard bold\" font). Because this contains all reals, $3$ and $\\pi$ are **contained in** $\\mathbb{R}$. We often write this symbolically as $3 \\in \\mathbb{R}$ and $\\pi \\in \\mathbb{R}$.\n",
    "\n",
    "To get the set of all pairs of real numbers, we would essentially take the product of this set with itself (called the Cartesian product) — $\\mathbb{R}$ x $\\mathbb{R}$, abbreviated as $\\mathbb{R}^2$. This set — $\\mathbb{R}^2$ — contains all pairs of real numbers, so $(1, 3)$ is **contained in** this set. We write this symbolically as $(1, 3) \\in \\mathbb{R}^2$.\n",
    "\n",
    "+ In 2-D space ($\\mathbb{R}^2$), a point is uniquely referred to using two coordinates: $(1, 3) \\in \\mathbb{R}^2$.\n",
    "+ In 3-D space ($\\mathbb{R}^3$), a point is uniquely referred to using three coordinates: $(8, 2, -3) \\in \\mathbb{R}^3$.\n",
    "+ In $n$-dimensional space ($\\mathbb{R}^n$), a point is uniquely referred to using $n$ coordinates.\n",
    "\n",
    "Note that these coordinates of course are isomorphic to our vectors! After all, coordinates are ordered sequences of numbers, just as we define vectors to be ordered sequences of numbers. So, especially in machine learning, we often visualize vectors of length $n$ as points in $n$-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"vector-norm\"></a>\n",
    "### Vector Norm \n",
    "\n",
    "The **magnitude** of a vector, $\\vec{v} \\in \\mathbb{R}^{n}$, can be interpreted as its length in $n$-dimensional space. Therefore it is calculable via the Euclidean distance from the origin:\n",
    "\n",
    "$\\vec{v} = \\left[ \\begin{array}{c}\n",
    "v_{1} \\\\\n",
    "v_{2} \\\\\n",
    "\\vdots \\\\\n",
    "v_{n}\n",
    "\\end{array} \\right]$\n",
    "\n",
    "then $\\| \\vec{v} \\| = \\sqrt{v_{1}^{2} + v_{2}^{2} + ... + v_{n}^{2}} = \\sqrt{\\vec{v}^T\\vec{v}}$\n",
    "\n",
    "E.g. if $\\vec{v} = \n",
    "\\left[ \\begin{array}{c}\n",
    "3 \\\\\n",
    "4\n",
    "\\end{array} \\right]$, then $\\| \\vec{v} \\| = \\sqrt{3^{2} + 4^{2}} = 5$\n",
    "\n",
    "This is also called the vector **norm**. You will often see this used in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the norm of the vector x with np.linalg.norm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"linear-algebra-applications-to-machine-learning\"></a>\n",
    "## Linear Algebra Applications to Machine Learning\n",
    "---\n",
    "\n",
    "Linear Algebra will give you better intuition for machine learning algorithms and see them beyond \"black boxes\".  Models have parameters, or hyperparameters that you can tune, and understanding the inner workings can help you refine your models.\n",
    "\n",
    "You can also code algorithms from scratch, if you choose to become more advanced.\n",
    "\n",
    "<a id=\"distance-between-actual-values-and-predicted-values\"></a>\n",
    "### Distance Between Actual Values and Predicted Values\n",
    "We often need to know the difference between predicted values and actual values. \n",
    "\n",
    "![](assets/images/vector-norms.png)\n",
    "\n",
    "\n",
    "#### L² Norm (Least Squares)\n",
    "Most commonly, we use the **L²** norm, which is the sum of the squared values.  In 2-D space, we compute this as:\n",
    "$$ L^2 norm = \\|\\vec{actual} - \\vec{predicted} \\| = \\sqrt{(actual_1 - predicted_1)^2 + (actual_2 - predicted_2)^2 ... + (actual_n - predicted_n)^2}$$\n",
    "\n",
    "Note that this is just the **straight-line distance** or **as-the-crow-flies distance** between the actual point and the predicted point.\n",
    "\n",
    "\n",
    "#### L¹ Norm (Least Absolute Deviations)\n",
    "Another less used method is the **L¹** norm, aka **taxicab distance** because it describes the number of blocks to travel to reach the destination.\n",
    "\n",
    "$$ L^1 norm = \\|\\vec{actual} - \\vec{predicted} \\| = |(actual_1 - predicted_1)| + |(actual_2 - predicted_2)| ... + |(actual_n - predicted_n)| $$\n",
    "\n",
    "\n",
    "![](assets/images/L1-vs-L2-properties1.png)\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "### Mean Absolute Error\n",
    "MAE measures the average magnitude of the errors in a set of predictions, without considering their direction. All individual differences have equal weight.\n",
    "\n",
    "$$MAE = \\frac{1} {n} \\| \\hat{y}(\\mathbf{X}) - \\vec{y} \\|$$\n",
    "\n",
    "\n",
    "<a id=\"mean-squared-error\"></a>\n",
    "### Mean Squared Error\n",
    "Another method for measuring distance, or error between predicted and actual, is the mean of the squared errors.  **This is often used to measure the quality of regression models.** Where $\\hat{y}(\\mathbf{X})$ is a vector of predicted values (a function of the data matrix $\\mathbf{X}$) and $\\vec{y}$ is the actual values:\n",
    "\n",
    "$$MSE = \\frac{1} {n} ( \\hat{y}(\\mathbf{X}) - \\vec{y} )^2$$\n",
    "\n",
    "\n",
    "\n",
    "### Root Mean Squared Error\n",
    "Another similar method for measuring distance, or error between predicted and actual, is the square root of the mean of the squared errors.  **This is another common method to measure the quality of regression models.** Where $\\hat{y}(\\mathbf{X})$ is a vector of predicted values (a function of the data matrix $\\mathbf{X}$) and $\\vec{y}$ is the actual values:\n",
    "\n",
    "$$RMSE = \\sqrt{\\frac{1} {n} ( \\hat{y}(\\mathbf{X}) - \\vec{y} )^2}$$\n",
    "\n",
    "\n",
    "Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to large errors. This means the RMSE should be more useful than MAE and MSE when large errors are not desired.\n",
    "\n",
    "<a id=\"least-squares\"></a>\n",
    "### Least Squares\n",
    "Regression models use least squares to optimize the fit of the model, and are based on the following form:\n",
    "\n",
    "$$\\min \\| \\hat{y}(\\mathbf{X}) - \\vec{y} \\|^2$$\n",
    "\n",
    "The goal is to minimize the distance between model predictions and actual data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see this in [scikit-learn](http://scikit-learn.org/stable/modules/linear_model.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"codealong-examining-the-cars-dataset\"></a>\n",
    "### Follow-Along: Examining the Cars dataset\n",
    "---\n",
    "\n",
    "This is a follow-along vs a code-along for the sake of time so we can get back to statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the Motor Trend Cars data. \n",
    "\n",
    "**Data Source**: https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html\n",
    "\n",
    "**Description**\n",
    "The data was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973–74 models).\n",
    "\n",
    "**Format**\n",
    "A data frame with 32 observations on 11 (numeric) variables.\n",
    "\n",
    "- **mpg**\tMiles/(US) gallon\n",
    "- **cyl**\tNumber of cylinders\n",
    "- **disp**\tDisplacement (cu.in.)\n",
    "- **hp**\tGross horsepower\n",
    "- **drat**\tRear axle ratio\n",
    "- **wt**\tWeight (1000 lbs)\n",
    "- **qsec**\t1/4 mile time\n",
    "- **vs**\tEngine (0 = V-shaped, 1 = straight)\n",
    "- **am**\tTransmission (0 = automatic, 1 = manual)\n",
    "- **gear**\tNumber of forward gears\n",
    "- **carb**\tNumber of carburetors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T17:42:49.786161Z",
     "start_time": "2020-05-04T17:42:49.773196Z"
    }
   },
   "outputs": [],
   "source": [
    "mtcars = pd.read_csv('data/mtcars.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine we were trying to predict mpg of a car.  Lets create 2 random columns, *predicted mpg* and *predicted_mpg_2* , that we will assume were predicted with 2 different machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create synthetic values for each prediction (demo purpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "mtcars['predicted_mpg'] = np.random.randint(15, 30, mtcars.shape[0])\n",
    "mtcars['predicted_mpg_2'] = np.random.randint(18, 26, mtcars.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcars.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print out the dimensions of the DataFrame using the `.shape` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preview data dimensions.\n",
    "mtcars.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print out the data types of the columns using the `.dtypes` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What are the column data types?\n",
    "mtcars.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pull up descriptive statistics for each variable using the built-in `.describe()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull up descriptive statistics for mpg, predicted_mpg and predicted_mpg_2\n",
    "mtcars[['mpg','predicted_mpg','predicted_mpg_2']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Euclidean distance between the predicted columns and actual column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#L2 Norm aka Euclidean Distance aka Straight-Line Distance\n",
    "print('Model 1 L2 Norm:', np.linalg.norm(mtcars.mpg-mtcars.predicted_mpg))\n",
    "\n",
    "print('Model 2 L2 Norm:', np.linalg.norm(mtcars.mpg-mtcars.predicted_mpg_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises:\n",
    "\n",
    "#### 1. Going the Distance\n",
    "\n",
    "Calculate the L1 Norm for each prediction.  Look at the help for np.linalg.norm, and specifically the **ord** parameter. (hint:L**1**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model 1 L1 Norm:', np.linalg.norm(mtcars.mpg-mtcars.predicted_mpg, ord=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model 2 L1 Norm:', np.linalg.norm(mtcars.mpg-mtcars.predicted_mpg_2, ord=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Calculate the MAE using numpy.  (hint: nest np.abs into np.mean and np.abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Model 1 MAE:', np.mean(np.abs(mtcars.mpg-mtcars.predicted_mpg)))\n",
    "print('Model 2 MAE:', np.mean(np.abs(mtcars.mpg-mtcars.predicted_mpg_2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Calculate the MSE using numpy.  (hint: use np.mean and np.square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Model 1 MSE:', np.mean(np.square(mtcars.mpg-mtcars.predicted_mpg)))\n",
    "print('Model 2 MSE:', np.mean(np.square(mtcars.mpg-mtcars.predicted_mpg_2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Calculate the RMSE using numpy.  (hint: use the MSE calculation and take the square root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model 1 MSE:', np.sqrt(np.mean(np.square(mtcars.mpg-mtcars.predicted_mpg))))\n",
    "print('Model 2 MSE:', np.sqrt(np.mean(np.square(mtcars.mpg-mtcars.predicted_mpg_2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Based on these metrics, which of these 2 simple models is better at explaining the behavior?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"descriptive-statistics-fundamentals\"></a>\n",
    "## Statistics Fundamentals\n",
    "---\n",
    "\n",
    "- **Objective:** Code summary statistics using NumPy and Pandas: mean, median, mode, max, min, quartile, inter-quartile range, variance, standard deviation, and correlation.\n",
    "\n",
    "### Statistics\n",
    "\n",
    "Statistics is essentially the study of distributions. We leverage distributions to tie the frequency of a value to the actual value observed. Our goal is to understand how to pull meaning out of distributions of various datasets to arrive at the formal definition of statistics\n",
    "\n",
    "\n",
    ">**Statistics** is a branch of mathematics dealing with the collection, analysis, interpretation, presentation, and organization of data.\n",
    "\n",
    "That said there is ALOT of nuance within statistics. For this class you won't need to intimately understand statistics - but as you progress through your Data Science career it will increase in frequency. While the *litmus test* is a Data Scientist is better at statistics than a programmer you'll be able to go much further with an indepth review.\n",
    "\n",
    "Statistical References:\n",
    "* A great start [Elements of Statistical Learning](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)\n",
    "* [Bayesian Data Analysis, by Andrew Gelman](http://www.stat.columbia.edu/~gelman/book/)\n",
    "* [Machine Learning: a Probabilistic Perspective](https://www.cs.ubc.ca/~murphyk/MLbook/)\n",
    "* [Pattern Recognition and Machine Learning](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf)\n",
    "* And of course my personal [favorite](http://mtvernon.wsu.edu/wp-content/uploads/2016/12/Statistics_for_Terrified_Biologists.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Quick Review of Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of a constant, $k$, $n$ times:\n",
    "$$\\sum_{i=1}^nk$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# k + k + k + k + ... + k\n",
    "# For i from 1 up to and including n, add k to the sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It is often helpful to think of these sums as `for` loops. For example, the equation can be compactly computed like so:\n",
    "\n",
    "```\n",
    "total = 0\n",
    "\n",
    "# For i from 1 up to and including n, add k to the sum.\n",
    "for i in range(1, n+1):\n",
    "    total += k\n",
    "```\n",
    "\n",
    "> Or, even more succinctly (using a generator comprehension):\n",
    "\n",
    "```\n",
    "total = sum(k for i in range(1, n+1))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=5\n",
    "n=10\n",
    "total=0\n",
    "for i in range(1, n+1):\n",
    "    total += k\n",
    "    print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of all numbers from 1 up to and including $n$:\n",
    "$$\\sum_{i=1}^ni$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 + 2 + 3 + ... + n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ```\n",
    "total = sum(i for i in range(1, n+1))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=10\n",
    "total=0\n",
    "total = sum(i for i in range(1, n+1))\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of all $x$ from the first $x$ entry to the $n$th $x$ entry:\n",
    "$$\\sum_{i=0}^nx_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x_1 + x_2 + x_3 + ... + x_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ```\n",
    "total = sum(xi in x)      # or just sum(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code-Along\n",
    "\n",
    "_Optional: Write down the mathematical notation for the following questions:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the sum of seven 4s using base Python.\n",
    "sum([4, 4, 4, 4, 4, 4, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum_{i=1}^{7}{4}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the sum of seven 4s using NumPy\n",
    "print(np.sum([4, 4, 4, 4, 4, 4, 4]))\n",
    "print(np.multiply(4,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum_{i=1}^{7}{4}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the sum of 1 through 10 using base Python.\n",
    "sum(i+1 for i in range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum_{i=0}^{10}{x_i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using the titanic.fare column, compute the total fare paid by passengers.\n",
    "titanic = pd.read_csv('data/titanic.csv')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer here\n",
    "print(np.sum(titanic.fare))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"measures-of-central-tendency\"></a>\n",
    "### Measures of Central Tendency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mean\n",
    "- Median\n",
    "- Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean\n",
    "The mean is defined as:\n",
    "$$\\bar{x} =\\frac 1n\\sum_{i=1}^nx_i$$\n",
    "\n",
    "It is determined by summing all data points in a population and then dividing the total by the number of points. The resulting number is known as the mean or the average.\n",
    "\n",
    "Be careful — the mean can be highly affected by outliers. For example, the mean of a very large number and some small numbers will be much larger than the \"typical\" small numbers. Earlier, we saw that the mean squared error (MSE) was used to optimize linear regression. Because this mean is highly affected by outliers, the resulting linear regression model is, too.\n",
    "\n",
    "We say the mean is **sensitive** to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Median\n",
    "The median refers to the midpoint in a series of numbers. Notice that the median is not affected by outliers, so it more so represents the \"typical\" value in a set.\n",
    "\n",
    "$$ 0,1,2,[3],5,5,1004 $$\n",
    "\n",
    "$$ 1,3,4,[4,5],5,5,7 $$\n",
    "\n",
    "To find the median:\n",
    "\n",
    "- Arrange the numbers in order from smallest to largest.\n",
    "    - If there is an odd number of values, the middle value is the median.\n",
    "    - If there is an even number of values, the average of the middle two values is the median.\n",
    "\n",
    "Although the median has many useful properties, the mean is easier to use in optimization algorithms. The median is more often used in analysis than in machine learning algorithms.\n",
    "\n",
    "The median isn't really affected by a few outliers. We say the median is **resistant** to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mode\n",
    "The mode of a set of values is the value that occurs most often.\n",
    "A set of values may have more than one mode, or no mode at all.\n",
    "\n",
    "$$1,0,1,5,7,8,9,3,4,1$$ \n",
    "\n",
    "$1$ is the mode, as it occurs the most often (three times)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code-Along"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T18:03:25.925356Z",
     "start_time": "2020-05-04T18:03:25.910398Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's bring the titanic dataset into memory\n",
    "titanic = pd.read_csv('data/titanic.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Find the mean of the `titanic.fare` series using base Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(titanic.fare)/len(titanic.fare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Find the mean of the `titanic.fare series` using NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(titanic.fare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Find the mean of the `titanic.fare` series using Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.fare.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### What was the median fare paid (using Pandas)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.fare.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The mean and median are not the same, what does this tell you about the fares?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It's not a normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Pandas to find the most common fare paid on the Titanic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T18:03:27.359526Z",
     "start_time": "2020-05-04T18:03:27.350546Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    8.05\n",
       "dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.fare.mode()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Notice that this returns a series instead of a single number, why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T18:04:56.763511Z",
     "start_time": "2020-05-04T18:04:56.756556Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8.05 is the most frequent observation but it has to calculate all of those first. 0 is the index of the count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the built-in  `.value_counts()` function to count the values of each type in the `pclass` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T18:05:51.765651Z",
     "start_time": "2020-05-04T18:05:51.755650Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    491\n",
       "1    216\n",
       "2    184\n",
       "Name: pclass, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.pclass.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pull up descriptive statistics for each variable using the built-in `.describe()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T18:05:57.575176Z",
     "start_time": "2020-05-04T18:05:57.519325Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         survived      pclass         age       sibsp       parch        fare\n",
       "count  891.000000  891.000000  714.000000  891.000000  891.000000  891.000000\n",
       "mean     0.383838    2.308642   29.699118    0.523008    0.381594   32.204208\n",
       "std      0.486592    0.836071   14.526497    1.102743    0.806057   49.693429\n",
       "min      0.000000    1.000000    0.420000    0.000000    0.000000    0.000000\n",
       "25%      0.000000    2.000000   20.125000    0.000000    0.000000    7.910400\n",
       "50%      0.000000    3.000000   28.000000    0.000000    0.000000   14.454200\n",
       "75%      1.000000    3.000000   38.000000    1.000000    0.000000   31.000000\n",
       "max      1.000000    3.000000   80.000000    8.000000    6.000000  512.329200"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnosing Data Problems\n",
    "\n",
    "- Whenever you get a new data set, the fastest way to find mistakes and inconsistencies is to look at the **descriptive statistics**.\n",
    "  - If anything looks too high or too low relative to your experience, there may be issues with the data collection.\n",
    "- Your data may contain a lot of **missing values** and may need to be cleaned meticulously before they can be combined with other data.\n",
    "  - You can take a quick average or moving average to smooth out the data and combine that to preview your results before you embark on your much longer data-cleaning journey.\n",
    "  - Sometimes filling in missing values with their means or medians will be the best solution for dealing with missing data. Other times, you may want to drop the offending rows or do real imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"math-review\"></a>\n",
    "### Math Review\n",
    "\n",
    "#### How Do We Measure Distance?\n",
    "\n",
    "One method is to take the difference between two points:\n",
    "\n",
    "$$X_2 - X_1$$\n",
    "\n",
    "However, this can be inconvenient because of negative numbers.\n",
    "\n",
    "We often use the following square root trick to deal with negative numbers. Note this is equivalent to the absolute value (if the points are 1-D):\n",
    "\n",
    "$$\\sqrt{(X_2-X_1)^2} = | X_2 - X_1 |$$\n",
    "\n",
    "#### What About Distance in Multiple Dimensions?\n",
    "\n",
    "We can turn to the Pythagorean theorem.\n",
    "\n",
    "$$a^2 + b^2 = c^2$$\n",
    "\n",
    "To find the distance along a diagonal, it is sufficient to measure one dimension at a time:\n",
    "\n",
    "$$\\sqrt{a^2 + b^2} = c$$\n",
    "\n",
    "More generally, we can write this as the norm (You'll see this in machine learning papers):\n",
    "\n",
    "$$\\|X\\|_2 = \\sqrt{\\sum{x_i^2}} = c$$\n",
    "\n",
    "What if we want to work with points rather than distances? For points $\\vec{x}: (x_1, x_1)$ and $\\vec{y}: (y_1, y_2)$ we can write:\n",
    "\n",
    "$$\\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2} = c$$\n",
    "or\n",
    "$$\\sqrt{\\sum{(x_i - y_i)^2}} = c$$\n",
    "or\n",
    "$$\\| \\vec{x} - \\vec{y} \\| = c$$\n",
    "\n",
    "> You may be more familiar with defining points as $(x, y)$ rather than $(x_1, x_2)$. However, in machine learning it is much more convenient to define each coordinate using the same base letter with a different subscript. This allows us to easily represent a 100-dimensional point, e.g., $(x_1, x_2, ..., x_{100})$. If we use the grade school method, we would soon run out of letters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"measures-of-dispersion-standard-deviation-and-variance\"></a>\n",
    "### Measures of Dispersion: Standard Deviation and Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation (SD, $σ$ for population standard deviation, or $s$ for sample standard deviation) is a measure that is used to quantify the amount of variation or dispersion from the mean of a set of data values. A low standard deviation means that most of the numbers are close to the average. A high standard deviation means that the numbers are spread out.\n",
    "\n",
    "Standard deviation is the square root of variance:\n",
    "\n",
    "$$\\text{variance} = s^2 = \\frac {\\sum{(x_i - \\bar{x})^2}} {n-1}$$\n",
    "\n",
    "$$s = \\sqrt{\\frac {\\sum{(x_i - \\bar{x})^2}} {n-1}}$$\n",
    "\n",
    "> **Standard deviation** is often used because it is in the same units as the original data! By glancing at the standard deviation, we can immediately estimate how \"typical\" a data point might be by how many standard deviations it is from the mean. Furthermore, standard deviation is the only value that makes sense to visually draw alongside the original data.\n",
    "\n",
    "> **Variance** is often used for efficiency in computations. The square root in the SD always increases with the function to which it is applied. So, removing it can simplify calculations (e.g., taking derivatives), particularly if we are using the variance for tasks such as optimization.\n",
    "\n",
    "We can also write variance in standard mathematic notation \n",
    "\n",
    "$$\\sigma = \\frac {\\sum{(x_i - \\bar{X})^2}} {n}$$\n",
    "\n",
    "Or:\n",
    "\n",
    "$$Var(\\mathbf{X}) = \\mathbb{E}[(X - \\mathbb{E}[X])^2]$$\n",
    "\n",
    "Or:\n",
    "\n",
    "$$Var(\\mathbf{X}) = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2$$\n",
    "\n",
    "The final equation is obtained by expanding the squared quantity in the first equation then simplifying the summed terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**That can be a lot to take in, so let's break it down in Python.**\n",
    "\n",
    "#### Assign the first 5 rows of titanic age data to a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take the first five rows of titanic age data.\n",
    "first_five = titanic.age[:5,]\n",
    "print(first_five)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the mean by hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean by hand.\n",
    "mean = (22 + 38 + 26 + 35 + 35) / 5.0\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the variance by hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate variance by hand\n",
    "(np.square(22 - mean) +\n",
    "np.square(38 - mean) +\n",
    "np.square(26 - mean) +\n",
    "np.square(35 - mean) +\n",
    "np.square(35 - mean)) / 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the variance and the standard deviation using Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Verify with Pandas\n",
    "print(first_five.var())\n",
    "print(first_five.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **quartile** is a type of **quantile**. Quartiles in statistics are values that divide your data into quarters. The **first quartile (Q1)** is defined as the middle number between the smallest number and the median of the data set. The **second quartile (Q2)** is the median of the data. The **third quartile (Q3)** is the middle value between the median and the highest value of the data set. \n",
    "\n",
    "**Quartiles** represent the value for which 25% of the data is below (Q1) and the value for which 25% of the data is above (Q3)\n",
    "\n",
    "The four quarters that divide a data set into quartiles are:\n",
    "\n",
    "1. The lowest 25% of numbers.\n",
    "2. The next lowest 25% of numbers (up to the median).\n",
    "3. The second highest 25% of numbers (above the median).\n",
    "4. The highest 25% of numbers.\n",
    "\n",
    "**Use the titanic passenger ages to calculate the first and third quartiles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T17:01:24.937225Z",
     "start_time": "2020-05-04T17:01:24.932238Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using the pd.qcut() or pd.quantile() methods from pandas to find the 1st (Q1) and 3rd (Q3) quartiles\n",
    "set(pd.qcut(titanic.age,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.age.quantile([0.25,0.5,0.75])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **interquartile range (IQR)** is the difference between the upper (Q3) and lower (Q1) quartiles, and describes the middle 50% of values when ordered from lowest to highest. The IQR is often seen as a better measure of spread than the range as it is not affected by outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the interquartile range of the titanic passenger ages\n",
    "np.array(titanic.age.quantile([0.75]))-np.array(titanic.age.quantile([0.25]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"our-first-model\"></a>\n",
    "## Our First Model\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mathematical models are tools to help us understand the world around us. They:\n",
    " 1. Help to explain a system\n",
    " 2. Allow us to study the effects of different components\n",
    " 3. Grant the ability to make predictions about behaviour\n",
    " 4. Give us experimental tool for testing theories and assessing quantitive conjectures\n",
    " 5. Provide us a process where their formulation clarifies assumptions, variables, and parameters\n",
    " \n",
    "While all that is helpful it cannot be overstated enough that **models are not reality**; they are an extreme simplification of reality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will make a **mathematical model** of data. When we say **model**, we mean it in the same sense that a map is a **model** of the real world. Google Maps can get us to that restaurant without getting lost, but it can't tell us where each individual pothole is. This is good enough.\n",
    "\n",
    "As another example for when we say **model**, we mean it in the same sense that a toy car is a **model** of a real car. If we mainly care about appearance, the toy car model is an excellent model. However, the toy car fails to accurately represent other aspects of the car. For example, we cannot use a toy car to test how the actual car would perform in a collision.\n",
    "\n",
    "<img src=\"http://www.azquotes.com/picture-quotes/quote-all-models-are-wrong-but-some-are-useful-george-e-p-box-53-42-27.jpg\">\n",
    "\n",
    "### Example of a model\n",
    "In data science, we might take a rich, complex person and model that person solely as a two-dimensional vector: _(age, smokes cigarettes)_. For example: $(90, 1)$, $(28, 0)$, and $(52, 1)$, where $1$ indicates \"smokes cigarettes.\" This model of a complex person obviously fails to account for many things. However, if we primarily care about modeling health risk, it might provide valuable insight.\n",
    "\n",
    "Now that we have superficially modeled a complex person, we might determine a formula that evaluates risk. For example, an older person tends to have worse health, as does a person who smokes. So, we might deem someone as having risk should `age + 50*smokes > 100`. \n",
    "\n",
    "This is a **mathematical model**, as we use math to assess risk. It could be mostly accurate. However, there are surely elderly people who smoke who are in excellent health.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make our first model from scratch. We'll use it predict the `fare` column in the Titanic data. So what data will we use? Actually, none."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest model we can build is an estimation of the mean, median, or most common value. If we have no feature matrix and only an outcome, this is the best approach to make a prediction using only empirical data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems silly, but we'll actually use it all the time to create a baseline of how well we do with no data and determine whether or not our more sophisticated models make an improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find out more about dummy estimators [here](http://scikit-learn.org/stable/modules/model_evaluation.html#dummy-estimators)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the `fare` column from the Titanic data and store it in variable `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T18:07:51.684162Z",
     "start_time": "2020-05-04T18:07:51.680172Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the fare column from the Titanic data and store it as y:\n",
    "y=titanic['fare']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create predictions `y_pred` (in this case just the mean of `y`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T18:07:52.655974Z",
     "start_time": "2020-05-04T18:07:52.651984Z"
    }
   },
   "outputs": [],
   "source": [
    "# Stored predictions in y_pred:\n",
    "y_pred = y.mean()\n",
    "y_pred2 = y.median() # add another model for fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises:\n",
    "\n",
    "#### 3. Baseline Comparisons\n",
    "\n",
    "#### Find the average squared distance between each prediction and its actual value:\n",
    "\n",
    "This is known as the mean squared error (MSE).\n",
    "\n",
    "The **Mean Squared Error (MSE)** is a measure of how close a fitted line is to data points. For every data point, you take the distance vertically from the point to the corresponding y value on the curve fit (the error), and square the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T18:07:53.810304Z",
     "start_time": "2020-05-04T18:07:53.788362Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2466.6653116850434\n"
     ]
    }
   ],
   "source": [
    "# Squared error is hard to read; let's look at mean squared error:\n",
    "print(np.mean(np.square(y-y_pred)))\n",
    "print(np.mean(np.square(y-y_pred2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Root Mean Squared error (RMSE)** is just the square root of the mean square error. That is probably the most easily interpreted statistic, since it has the same units as the quantity plotted on the vertical axis.\n",
    "\n",
    "> Key point: The RMSE is thus the distance, on average, of a data point from the fitted line, measured along a vertical line.\n",
    "\n",
    "*The RMSE is directly interpretable in terms of measurement units* making ita better measure of goodness of fit than a correlation coefficient. One can compare the RMSE to observed variation in measurements of a typical point. The two should be similar for a reasonable fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the root mean squared error (RMSE), the square root of the MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(np.sqrt(np.mean(np.square(y-y_pred))))\n",
    "print(np.sqrt(np.mean(np.square(y-y_pred2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"a-preface-on-modeling\"></a>\n",
    "### A Preface on Modeling\n",
    "---\n",
    "As we venture down the path of modeling, it can be difficult to determine which choices are \"correct\" or \"incorrect\".  A primary challenge is to understand how different models will perform in different circumstances and different types of data. It's essential to practice modeling on a variety of data.\n",
    "\n",
    "As a beginner it is essential to learn which metrics are important for evaluating your models and what they mean. The metrics we evaluate our models with inform our actions.  \n",
    "\n",
    "*Exploring datasets on your own with the skills and tools you learn in class is highly recommended!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='documentation'></a>\n",
    "\n",
    "## Digging into Documentation\n",
    "\n",
    "---\n",
    "\n",
    "Get familiar with looking up things in Documentation. As we progress into class it will be impossible to cover even 50% of possibilities with the libraries we'll be using. Two in particular are the `sklearn` and `statsmodels` documentation. You are going to be doing a lot of it over the course of class and beyond.\n",
    "\n",
    "[The statsmodels documentation can be found here.](http://statsmodels.sourceforge.net/devel/) Many recommend using the bleeding-edge version of statsmodels. [For that you can reference the code on github.](https://github.com/statsmodels/statsmodels/)\n",
    "\n",
    "[The sklearn documentation can be found here.](http://scikit-learn.org/stable/documentation.html)\n",
    "\n",
    "The packages have fairly different approaches and syntax for constructing models. Below are examples for linear regression in each package:\n",
    "* [Linear regression in statsmodels](http://statsmodels.sourceforge.net/devel/examples/#regression)\n",
    "* [Linear regression in scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "\n",
    "If you haven't yet, familliarize yourself with the format of the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"a-short-introduction-to-model-bias-and-variance\"></a>\n",
    "## A Short Introduction to Model Bias and Variance \n",
    "\n",
    "---\n",
    "\n",
    "- **Objective:** Describe the bias and variance of statistical estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In simple terms, **bias** shows how accurate a model is in its predictions. (It has **low bias** if it hits the bullseye!)\n",
    "\n",
    "**Variance** shows how reliable a model is in its performance. (It has **low variance** if the points are predicted consistently!)\n",
    "\n",
    "These characteristics have important interactions, but we will save that for later.\n",
    "\n",
    "![Bias and Variance](assets/images/biasVsVarianceImage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember how we just calculated mean squared error to determine the accuracy of our prediction? It turns out we can do this for any statistical estimator, including mean, variance, and machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even decompose mean squared error to identify the source of error - reducible error & irreducible error.\n",
    "\n",
    "* Irreducible error or inherent uncertainty is associated with a natural variability in a system. \n",
    "* Reducible error is not only something we can address but should be addressed to maximize accuracy. Given what we're talking bout it shouldn't surprise you to learn it's components are **error due to squared bias** and **error due to variance**.\n",
    "\n",
    "### Primer on Variance/Bias Tradeoff\n",
    "Models that exhibit small variance and high bias *underfit* the truth.  Models that exhibit high variance and low bias *overfit* the truth target. Both prevent us from making strong predictions\n",
    "\n",
    "![Over/underfit](assets/images/underoverfit.png)\n",
    "\n",
    "The **“tradeoff”** between bias and variance can be viewed in this manner – a learning algorithm with low bias must be “flexible” so that it can fit the data well. But if the learning algorithm is too flexible (for instance, too linear), it will fit each training data set differently, and hence have high variance. A key characteristic of many supervised learning methods is a built-in way to control the bias-variance tradeoff either automatically or by providing a special parameter that the data scientist can adjust.\n",
    "\n",
    "\n",
    "Note that if your target truth is highly nonlinear, and you select a linear model to approximate it, then you’re introducing a bias resulting from the linear model’s inability to capture nonlinearity. In fact, your linear model is underfitting the nonlinear target function over the training set. Likewise, if your target truth is linear, and you select a nonlinear model to approximate it, then you’re introducing a bias resulting from the nonlinear model’s inability to be linear where it needs to be. In fact, the nonlinear model is overfitting the linear target function over the training set.\n",
    "\n",
    "In the figure below, we see a plot of the model’s performance using prediction capability on the vertical axis as a function of model complexity on the horizontal axis. Here, we depict the case where we use a number of different orders of polynomial functions to approximate the target function. Shown in the figure are the calculated square bias, variance, and error on the test set for each of the estimator functions.\n",
    "\n",
    "We see that as the model complexity increases, the variance slowly increases and the squared bias decreases. This points to the tradeoff between bias and variance due to model complexity, i.e. models that are too complex tend to have high variance and low bias, while models that are too simple will tend to have high bias and low variance. The best model will have both low bias and low variance. \n",
    "![bias_variance_tradeoff](assets/images/Bia_variance_tradeoff_fig.jpg)\n",
    "\n",
    "\n",
    "[Primer Sourced from: The Clever Machine reference](https://theclevermachine.wordpress.com/2013/04/21/model-selection-underfitting-overfitting-and-the-bias-variance-tradeoff/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bias-variance-decomposition\"></a>\n",
    "### Bias-Variance Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following notation, $f$ refers to a perfect model, while $\\hat{f}$ refers to our model.\n",
    "\n",
    "**Bias**\n",
    "\n",
    "Error caused by bias is calculated as the difference between the expected prediction of our model and the correct value we are trying to predict:\n",
    "\n",
    "$$Bias = (\\text{the truth}) - (\\text{our estimate})$$\n",
    "\n",
    "**Variance**\n",
    "\n",
    "Error caused by variance is taken as the variability of a model prediction for a given point:\n",
    "\n",
    "$$Variance = E[\\left((\\text{our estimate}) - (\\text{average estimate})\\right)^2]$$\n",
    "\n",
    "**Mean Squared Error**\n",
    "$$ MSE = Variance + Bias^2 + \\text{irreducible error}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The MSE is actually composed of three sources of error: The **variance**, **bias**, and some **irreducible error** that the model can never render given the available features.\n",
    "\n",
    "This topic will come up again, but for now it's enough to know that we can decompose MSE into the bias of the estimator and the variance of the estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"example-using-bessels-correction\"></a>\n",
    "### Discussion of Bessel's Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's rarely practical to measure every single item in a population to gather a statistic. We will usually sample a few items and use those to infer a population value.\n",
    "\n",
    "For example, we can take a class of 200 students and measure their height, but rather than measuring everyone, we select students at random to estimate the average height in the class and the variance of the height in the class.\n",
    "\n",
    "We know we can take the mean as follows:\n",
    "\n",
    "$$E[X] = \\bar{X} =\\frac 1n\\sum_{i=1}^nx_i$$\n",
    "\n",
    "What about the variance?\n",
    "\n",
    "Intuitively and by definition, population variance looks like this (the average distance from the mean):\n",
    "\n",
    "$$\\frac {\\sum{(x_i - \\bar{X})^2}} {n}$$\n",
    "\n",
    "It's actually better to use the following for a sample (why?):\n",
    "\n",
    "$$\\frac {\\sum{(x_i - \\bar{X})^2}} {n-1}$$\n",
    "\n",
    "In some cases, we may even use:\n",
    "\n",
    "$$\\frac {\\sum{(x_i - \\bar{X})^2}} {n+1}$$\n",
    "\n",
    "Detailed explanations can be found here:\n",
    "\n",
    "- [Bessel correction](https://en.wikipedia.org/wiki/Bessel%27s_correction).\n",
    "- [Mean Squared Error](https://en.wikipedia.org/wiki/Mean_squared_error)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show an example of computing the variance by hand.\n",
    "\n",
    "Suppose we have the following data:\n",
    "\n",
    "$$X = [1, 2, 3, 4, 4, 10]$$\n",
    "\n",
    "First, we compute its mean: \n",
    "\n",
    "$$\\bar{X} = (1/6)(1 + 2 + 3 + 4 + 4 + 10) = 4$$\n",
    "\n",
    "Because this is a sample of data rather than the full population, we'll use the second formula. Let's first \"mean center\" the data:\n",
    "\n",
    "$$X_{centered} = X - \\bar{X} = [-3, -2, -1, 0, 0, 6]$$\n",
    "\n",
    "Now, we'll just find the average squared distance of each point from the mean:\n",
    "\n",
    "$$variance = \\frac {\\sum{(x_i - \\bar{X})^2}} {n-1} = \\frac {(-3)^2 + (-2)^2 + (-1)^2 + 0^2 + 0^2 + 6^2}{6-1} = \\frac{14 + 36}{5} = 10$$\n",
    "\n",
    "So, the **variance of $X$** is $10$. However, we cannot compare this directly to the original units because it is in the original units squared. So, we will use the **standard deviation of $X$**, $\\sqrt{10} \\approx 3.16$ to see that the value of $10$ is farther than one standard deviation from the mean of $4$. So, we can conclude it is somewhat far from most of the points (more on what it really might mean later).\n",
    "\n",
    "---\n",
    "\n",
    "A variance of zero means there is no spread. If we take instead $X = [1, 1, 1, 1]$, then clearly the mean $\\bar{X} = 1$. So, $X_{centered} = [0, 0, 0, 0]$, which directly leads to a variance of 0. (Make sure you understand why! Remember that variance is the average squared distance of each point from the mean.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"correlation-and-association\"></a>\n",
    "## Correlation and Association\n",
    "---\n",
    "\n",
    "- **Objective:** Describe characteristics and trends in a data set using visualizations.\n",
    "\n",
    "Correlation measures how variables related to each other.\n",
    "\n",
    "Typically, we talk about the Pearson correlation coefficient — a measure of **linear** association.\n",
    "\n",
    "We refer to perfect correlation as **colinearity**.\n",
    "\n",
    "The following are a few correlation coefficients. Note that if both variables trend upward, the coefficient is positive. If one trends opposite the other, it is negative. \n",
    "\n",
    "It is important that you always look at your data visually — the coefficient by itself can be misleading:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Example correlation values](./assets/images/correlation_examples.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"codealong-correlation-in-pandas\"></a>\n",
    "### Code-Along: Correlation in Pandas\n",
    "\n",
    "**Objective:** Explore options for measuring and visualizing correlation in Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the correlation matrix for all Titanic variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:\n",
    "titanic.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Seaborn to plot a heat map of the correlation matrix:\n",
    "\n",
    "The `sns.heatmap()` function will accomplish this.\n",
    "\n",
    "- Generate a correlation matrix from the Titanic data using the `.corr()` method.\n",
    "- Pass the correlation matrix into `sns.heatmap()` as its only parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use Seaborn to plot a correlation heat map\n",
    "sns.heatmap(titanic.corr());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take a closer look at the survived and fare variables using a scatter plot\n",
    "titanic.plot(kind='scatter', x='fare', y='survived');\n",
    "\n",
    "# Is correlation a good way to inspect the association of fare and survival?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"the-normal-distribution\"></a>\n",
    "## The Normal Distribution\n",
    "---\n",
    "\n",
    "- **Objective:** Identify a normal distribution within a data set using summary statistics and data visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is an event space?\n",
    "  - A listing of all possible occurrences.\n",
    "- What is a probability distribution?\n",
    "  - A function that describes how events occur in an event space.\n",
    "- What are general properties of probability distributions?\n",
    "  - All probabilities of an event are between 0 and 1.\n",
    "  - All events in the event space combined have probability 1.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"what-is-the-normal-distribution\"></a>\n",
    "### What is the Normal Distribution?\n",
    "- A normal distribution is often a key assumption to many models.\n",
    "  - In practice, if the normal distribution assumption is not met, it's not the end of the world. Your model is just less efficient in most cases.\n",
    "\n",
    "- The normal distribution is **completely summarized by its mean and standar deviation**.\n",
    "\n",
    "- The **mean** controls its **center**.\n",
    "\n",
    "- The **standard deviation** controls how **spread out** it is.\n",
    "\n",
    "- Normal distributions are **symmetric, bell-shaped curves**.\n",
    "\n",
    "![normal distribution](assets/images/normal.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why do we care about normal distributions?\n",
    "\n",
    "- They often show up in nature.\n",
    "- Aggregated processes tend to distribute normally, regardless of their underlying distribution (**Central Limit Theorem**)\n",
    "    - The **Central Limit Theorem** states that the sampling distribution of the sample means approaches a normal distribution as the sample size gets larger — no matter what the shape of the population distribution. ([More Info](https://www.analyticsvidhya.com/blog/2019/05/statistics-101-introduction-central-limit-theorem/))\n",
    "    \n",
    "- They offer effective simplification that makes it easy to make approximations.\n",
    "- It can improve our machine learning algorithms\n",
    "<br>\n",
    "Machine learning algorithms are usually designed to be smart enough to find out how to deal with any distribution present in the features by themselves. At the same time even if it isn't necessary to transform the actual distributions for an algorithm to work properly, it can still be beneficial for these reasons:\n",
    "\n",
    "\n",
    "* To make the cost function minimize better the error of the predictions\n",
    "* To make the algorithm converge properly and faster\n",
    "\n",
    "We'll discuss various ways to transform or rescale data (i.e. normalization, standardization) later in the course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot a histogram of 1,000 samples from a random normal distribution:\n",
    "\n",
    "The `np.random.randn(numsamples)` function will draw from a random normal distribution with a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "- To plot a histogram, pass a NumPy array with 1000 samples as the only parameter to `plt.hist()`.\n",
    "- Change the number of bins using the keyword argument `bins`, e.g. `plt.hist(mydata, bins=50)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1c984e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot a histogram of several random normal samples from NumPy.\n",
    "samples = np.random.randn(10000)\n",
    "plt.hist(samples,100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"skewness\"></a>\n",
    "###  Skewness\n",
    "- Skewness is a measure of the asymmetry of the distribution of a random variable about its mean.\n",
    "- Skewness can be positive or negative, or even undefined.\n",
    "- Notice that the mean, median, and mode are the same when there is no skew.\n",
    "\n",
    "![skewness](assets/images/skewness---mean-median-mode.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot a lognormal distribution generated with NumPy.\n",
    "\n",
    "Take 1,000 samples using `np.random.lognormal(size=numsamples)` and plot them on a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T18:10:55.035363Z",
     "start_time": "2020-05-04T18:10:54.815965Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot a lognormal distribution generated with NumPy\n",
    "plt.hist(np.random.lognormal(size=1000),10);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Real World Application - When mindfullness beats complexity\n",
    "- Skewness is surprisingly important.\n",
    "- Most algorithms implicitly use the mean by default when making approximations.\n",
    "- If you know your data is heavily skewed, you may have to either transform your data or set your algorithms to work with the median."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"kurtosis\"></a>\n",
    "### Kurtosis\n",
    "- Kurtosis is a measure of whether the data are peaked or flat, relative to a normal distribution.\n",
    "- Data sets with high kurtosis tend to have a distinct peak near the mean, decline rather rapidly, and have heavy tails. \n",
    "\n",
    "![kurtosis](assets/images/kurtosis.jpg)\n",
    "\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Kurtosis) includes additional pictures and explanations that may best drive this concept home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Real-World Application: Risk Analysis\n",
    "- Long-tailed distributions with high kurtosis elude intuition; we naturally think the event is too improbable to pay attention to.\n",
    "- It's often the case that there is a large cost associated with a low-probability event, as is the case with hurricane damage.\n",
    "- It's unlikely you will get hit by a Category 5 hurricane, but when you do, the damage will be catastrophic.\n",
    "- Pay attention to what happens at the tails and whether this influences the problem at hand.\n",
    "- In these cases, understanding the costs may be more important than understanding the risks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"determining-the-distribution-of-your-data\"></a>\n",
    "## Determining the Distribution of Your Data\n",
    "---\n",
    "\n",
    "**Objective:** Create basic data visualizations, including scatterplots, box plots, and histograms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/images/distributions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the `.hist()` function of your Titantic DataFrame to plot histograms of all the variables in your data.\n",
    "\n",
    "- The function `plt.hist(data)` calls the Matplotlib library directly.\n",
    "- However, each DataFrame has its own `hist()` method that by default plots one histogram per column. \n",
    "- Given a DataFrame `my_df`, it can be called like this: `my_df.hist()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot all variables in the Titanic data set using histograms:\n",
    "titanic.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the built-in `.plot.box()` function of your Titanic DataFrame to plot box plots of your variables.\n",
    "\n",
    "- Given a DataFrame, a box plot can be made where each column is one tick on the x axis.\n",
    "- To do this, it can be called like this: `my_df.plot.box()`.\n",
    "- Try using the keyword argument `showfliers`, e.g. `showfliers=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T18:11:40.193780Z",
     "start_time": "2020-05-04T18:11:39.994350Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting all histograms can be unweildly; box plots can be more concise:\n",
    "titanic[['survived', 'pclass','sibsp']].plot.box(showfliers=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exercise\"></a>\n",
    "### Exercise\n",
    "\n",
    "1. Look at the Titanic data variables.\n",
    "- Are any of them normal?\n",
    "- Are any skewed?\n",
    "- How might this affect our modeling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work on your answers here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/images/visualization_flow_chart.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"topic-review\"></a>\n",
    "## Lesson Review\n",
    "---\n",
    "\n",
    "- We covered several different types of summary statistics, what are they?\n",
    "- We covered three different types of visualizations, which ones?\n",
    "- Describe bias and variance and why they are important.\n",
    "- What are some important characteristics of distributions?\n",
    "\n",
    "**Any further questions?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"lab-stats-dim\"></a>\n",
    "## Extra Lab: Using Stats for Feature Reduction\n",
    "---\n",
    "\n",
    "- In this section, we will apply four of the techniques from a PyData DC 2016 talk, [\"A Practical Guide to Dimensionality Reduction\"](https://pyvideo.org/pydata-dc-2016/a-practical-guide-to-dimensionality-reduction-techniques.html).\n",
    "\n",
    "- Your solutions do not have to be fully automated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_df = pd.read_csv('../datasets/chicago.csv')\n",
    "\n",
    "chicago_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Percent missing values\n",
    "\n",
    "The presenter suggests to drop features when > 95% of the values are missing.\n",
    "\n",
    "#### 1a. For each column, what % are missing? \n",
    "- For this exercise, suppose only `np.nan` indicates missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. - chicago_df.count() / len(chicago_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1b. For each column with missing values, create an indicator column that is `True` if missing and `False` otherwise. \n",
    "\n",
    "- Make the column name the original followed by `_Missing`. For example, `Address` would become `Address_Missing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_df['HouseType_Missing'] = chicago_df.HouseType.isnull()\n",
    "chicago_df['Bath_Missing'] = chicago_df.Bath.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Amount of variation\n",
    "\n",
    "#### 2a. What is the variance of each numeric column?\n",
    "\n",
    "- Drop any columns that have zero variance. \n",
    "- Are there any non-numeric columns with zero variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variances = chicago_df.std().sort_values(ascending=False)\n",
    "\n",
    "variances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Pairwise correlation\n",
    "\n",
    "#### 3a. Which pairs of features are highly correlated?\n",
    "- For this exercise, use >= 0.65.\n",
    "- Keep in mind -0.8 and 0.8 are both highly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age/LotSizeSqft\n",
    "# HouseSizeSqft/LotSizeSqft\n",
    "chicago_df.corr().abs() > 0.65"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3b. For each pair, drop the feature that is less correlated with the target (Price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age/LotSizeSqft - 0.24/0.44\n",
    "# HouseSizeSqft/LotSizeSqft - 0.46/0.44\n",
    "\n",
    "print(chicago_df.corr().abs().loc['Price'])\n",
    "chicago_df.drop(columns=['Age', 'LotSizeSqft'], inplace=True)\n",
    "\n",
    "chicago_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Correlation with the target\n",
    "\n",
    "#### 4a. Which pairs of features are lowly correlated with MEDV?\n",
    "\n",
    "- For this exercise, suppose < 0.25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_corrs = chicago_df.corr().loc['Price']\n",
    "\n",
    "target_corrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4b. Plot the (absolute values) of the correlations in descending order using a line plot. \n",
    "\n",
    "- Is there an \"elbow\" in the curve where the correlations flatten out or do not drop as steeply?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_corrs.abs().sort_values(ascending=False).plot(rot=45);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
