{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "## Practice Loading and Describing Data \n",
    "\n",
    "_Authors: Matt Brems (DC)_\n",
    "\n",
    "---\n",
    "\n",
    "In this lab you will practice loading data using python and describing it with statistics.\n",
    "\n",
    "It might be a good idea to first check the [source of the Boston housing data](https://archive.ics.uci.edu/ml/datasets/Housing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the boston housing data (provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data and save to a file called \"housing.data.\"\n",
    "\n",
    "from urllib import request\n",
    "data_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\"\n",
    "\n",
    "# this saves a file called 'housing.data' locally'\n",
    "request.urlretrieve(data_url, '../datasets/housing.data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data file does not contain the column names in the first line, so we'll need to add those in manually. You can find the names and explanations [here](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names). We've extracted the names below for your convenience. You may choose to edit the names, should you decide it would be more helpful to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\",\n",
    "         \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\", \"MEDV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. CRIM      - per capita crime rate by town\n",
    "# 2. ZN        - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "# 3. INDUS     - proportion of non-retail business acres per town\n",
    "# 4. CHAS      - Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "# 5. NOX       - nitric oxides concentration (parts per 10 million)\n",
    "# 6. RM        - average number of rooms per dwelling\n",
    "# 7. AGE       - proportion of owner-occupied units built prior to 1940\n",
    "# 8. DIS       - weighted distances to five Boston employment centres\n",
    "# 9. RAD       - index of accessibility to radial highways\n",
    "# 10. TAX      - full-value property-tax rate per 10,000 dollars. \n",
    "# 11. PTRATIO  - pupil-teacher ratio by town\n",
    "# 12. B        - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "# 13. LSTAT    - Percent lower status of the population\n",
    "# 14. MEDV     - Median value of owner-occupied homes in $1000's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load the `housing.data` file with python\n",
    "\n",
    "Using any method of your choice.\n",
    "> _**Hint:** despite this file having a strange `.data` extension, using python's `open() as file` and `file.read()` or `file.readlines()` we can load this in and see that it is a text file formatted much the same as a CSV. You can use string operations to format the data._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the local file 'housing.data'\n",
    "data = []\n",
    "with open('../datasets/housing.data', 'r') as f:\n",
    "    rows = f.readlines()\n",
    "    for row in rows:\n",
    "        row = [float(x) for x in row.split()]\n",
    "        data.append(row)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the first two rows:\n",
    "data[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the data into a dictionary with keys identified by column names:\n",
    "d = {key_name:[row[index] for row in data] for index, key_name in enumerate(names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['NOX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (d.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  Conduct a brief integrity check of your data. \n",
    "\n",
    "This integrity check should include, but is not limited to, checking for missing values and making sure all values make logical sense. (i.e. is one variable a percentage, but there are observations above 100%?)\n",
    "\n",
    "Summarize your findings in a few sentences, including what you checked and, if appropriate, any \n",
    "steps you took to rectify potential integrity issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for features with improperly recorded observations:\n",
    "    \n",
    "# Given the information about the features and observing a few of their \n",
    "# observations I believe there are 7 features which take place on a \n",
    "# normalized scale (0-1 or 0-100)\n",
    "\n",
    "# - CHAS (0-1),\n",
    "# - CRIM(0-100),\n",
    "# - ZN(0-100), \n",
    "# - INDUS(0-100), \n",
    "# - RM(0-100), \n",
    "# - LSTAT(0-100), \n",
    "# - PTRATIO(0-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_one = ['CHAS']\n",
    "max_hund = ['ZN','INDUS','RM','LSTAT','PTRATIO', 'CRIM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in max_one:\n",
    "    for observation in d[feature]:\n",
    "        if observation > 1 or observation < 0:\n",
    "            print ('Abnormal Value found in ', feature, 'with value of', observation)\n",
    "# if nothing is returned than nothing unusual was found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in max_hund:\n",
    "    for observation in d[feature]:\n",
    "        if observation > 100 or observation < 0:\n",
    "            print ('Abnormal Value found in ', feature, 'with value of', observation)\n",
    "# Also returned no values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing how many features we should have\n",
    "print ('Number of Expected Features: ' + str(len(names)))\n",
    "\n",
    "# printing how many features we do have (keys)\n",
    "print ('Number of Actual Features: ' + str(len(d.keys())))\n",
    "\n",
    "# prints the lengths of all the value lists in the dictionary\n",
    "for key, data in d.items():\n",
    "    print (\"{} rows in {}\".format(len(data),key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " _There is no missing data and there are no other data integrity issues that we checked for._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. For what two attributes does it make the *least* sense to calculate mean and median? Why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Potential Solution: **_The dummy variable `CHAS` and the categorical variable `RAD`. _\n",
    "- `CHAS` is a dummy (categorical) variable that makes no sense quantitatively. \n",
    "- `RAD` is a variable that indexes the distance to highways. It has many low values and, after a large gap, has higher values. It stands to reason that this is not a \"_true_\" quantitative variable in the sense that the difference between `RAD = 1` and `RAD = 2` may not be the same as the difference between `RAD = 2` and `RAD = 3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Which two variables have the strongest linear association? \n",
    "\n",
    "Report both variables, the metric you chose as the basis for your comparison, and the value of that metric. *(Hint: Make sure you consider only variables for which it makes sense to find a linear association.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "relationships = []\n",
    "\n",
    "# Choosing to use the pearson correlation coefficient, \"np.corrcoef\":\n",
    "for name in d.keys():\n",
    "    # do not consider RAD or CHAS:\n",
    "    if name not in ['RAD','CHAS']:\n",
    "        for other in d.keys():\n",
    "            if (name != other) and (other not in ['RAD','CHAS']):\n",
    "                relationships.append([name, other, np.corrcoef(d[name], d[other])[0,1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_rel = sorted(relationships, key=lambda x: np.abs(x[2]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print (sort_rel[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Solution:** _`NOX` (Nitric Oxides Concentration) and `DIS` (Weighted Distances to Five Boston Employment Centers)have the strongest _linear_ association. The correlation between NOX and DIS is -0.76923._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Look at distributional qualities of variables.\n",
    "\n",
    "Answer the following questions:\n",
    "1. Which variable has the most symmetric distribution? \n",
    "2. Which variable has the most left-skewed (negatively skewed) distribution? \n",
    "3. Which variable has the most right-skewed (positively skewed) distribution? \n",
    "\n",
    "Defend your method for determining this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symmetric: the metric with the smallest abs(mean - median)\n",
    "# Left: the metric with the smallest mean - median\n",
    "# Right: the metric with the largest mean - median\n",
    "\n",
    "symmetric = sorted([[key_name, np.abs(np.mean(values) - np.median(values))] for key_name,values in d.items()],\n",
    "                   key=lambda x: x[1])\n",
    "print ('Symmetric:', symmetric[0])\n",
    "\n",
    "left = sorted([[k, np.mean(v) - np.median(v)] for k,v in d.items()],\n",
    "              key=lambda x: x[1])\n",
    "print ('Left:', left[0])\n",
    "\n",
    "right = sorted([[k, np.mean(v) - np.median(v)] for k,v in d.items()],\n",
    "               key=lambda x: x[1], reverse=True)\n",
    "print ('Right:', right[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Repeat question 6 but scale the variables by their range first.\n",
    "\n",
    "As you may have noticed, the spread of the distribution contributed significantly to the results in question 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_diff(v):\n",
    "    return (np.mean(v) - np.median(v))/np.ptp(v)\n",
    "\n",
    "symmetric = sorted([[k, np.abs(scaled_diff(v))] for k,v in d.items()],\n",
    "                   key=lambda x: x[1])\n",
    "print ('Symmetric:', symmetric[0])\n",
    "\n",
    "left = sorted([[k, scaled_diff(v)] for k,v in d.items()],\n",
    "              key=lambda x: x[1])\n",
    "print ('Left:', left[0])\n",
    "\n",
    "right = sorted([[k, scaled_diff(v)] for k,v in d.items()],\n",
    "               key=lambda x: x[1], reverse=True)\n",
    "print ('Right:', right[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Univariate analysis of your choice\n",
    "\n",
    "Conduct a full univariate analysis on MEDV, CHAS, TAX, and RAD. \n",
    "\n",
    "For each variable, you should answer the three questions generally asked in a univariate analysis using the most appropriate metrics.\n",
    "- A measure of central tendency\n",
    "- A measure of spread\n",
    "- A description of the shape of the distribution (plot or metric based)\n",
    "\n",
    "If you feel there is additional information that is relevant, include it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "_**Sketch of Answer:**_   \n",
    "You should report at least one **measure of center**, one **measure of spread**, and a description (metric-based or plot-based) of the shape of the **distribution** of each variable.  \n",
    "- Defending which of these choices is better. (i.e. median is a better measure of center than mean because...) \n",
    "- Including multiple measures of center and/or spread and interpreting what these reveal about the distribution of a variable is especially good.\n",
    "- Including a plot that goes along with these metrics and this description would turn this answer from a \"good\" one into a \"great\" one. A report to a supervisor should ideally include these points.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Have you been using inferential statistics, descriptive statistics, or both?\n",
    "\n",
    "For each exercise, identify the branch of statistics on which you relied for your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Solution:** _For all exercises, we relied only on descriptive statistics._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Reducing the number of observations\n",
    "\n",
    "It seems likely that this data is a census - that is, the data set includes the entire target population. Suppose that the 506 observations was too much for our computer (as unlikely as this might be) and we needed to pare this down to fewer observations. \n",
    "\n",
    "**11.A Use the `random.sample()` function to select 50 observations from `'AGE'`.**\n",
    "\n",
    "([This documentation](https://docs.python.org/2/library/random.html) may be helpful.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = d['AGE']\n",
    "\n",
    "import random\n",
    "\n",
    "age_sample = random.sample(age, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(age_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11.B Identify the type of sampling we just used.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "**Solution:** _Simple random sampling was used._  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. [BONUS] Of the remaining types of sampling, describe (but do not execute) how you might implement at least one of these types of sampling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Potential Solution:** Stratified random sampling is a method used when we want to protect ourselves from a potentially \"bad\" or \"skewed\" simple random sample. The variable CHAS takes on two values: 1 and 0. Rather\n",
    "than selecting 50 observations at random, we could look at the proportion of 1s and 0s for the CHAS variable, select 50 * (proportion of 1s) observations where CHAS = 1 and then select 50 * (proportion of 0s)\n",
    "obervations where CHAS = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
