{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# Unicode Handling\n",
    "from __future__ import unicode_literals\n",
    "import codecs\n",
    "\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "# spacy is used for pre-processing and traditional NLP\n",
    "import spacy\n",
    "from spacy.en import English\n",
    "\n",
    "# Gensim is used for LDA and word2vec\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading the tweet data\n",
    "filename = '../../assets/dataset/captured-tweets.txt'\n",
    "tweets = []\n",
    "for tweet in codecs.open(filename, 'r', encoding=\"utf-8\"):\n",
    "    tweets.append(tweet)\n",
    "# Setting up spacy\n",
    "nlp_toolkit = English()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1a\n",
    "\n",
    "Write a function that can take a take a sentence parsed by `spacy` and identify if it mentions a company named 'Google'. Remember, `spacy` can find entities and codes them as `ORG` if they are a company. Look at the slides for class 12 if you need a hint:\n",
    "\n",
    "### Bonus (1b)\n",
    "\n",
    "Parameterize the company name so that the function works for any company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mentions_company(parsed):\n",
    "    for entity in parsed.ents:\n",
    "        if entity.text == \"Google\" and entity.label_ == 'ORG':\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# 1b\n",
    "\n",
    "def mentions_company(parsed, company='Google'):\n",
    "    for entity in parsed.ents:\n",
    "        if entity.text == company and entity.label_ == 'ORG':\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#always test your function!\n",
    "sentence = 'This company is totally Google and not IBM'\n",
    "parsed=nlp_toolkit(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentions_company(parsed,company='Google')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1c\n",
    "\n",
    "Write a function that can take a sentence parsed by `spacy` \n",
    "and return the verbs of the sentence (preferably lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_actions(parsed):\n",
    "    actions = []\n",
    "    for el in parsed:\n",
    "        if el.pos == spacy.parts_of_speech.VERB:\n",
    "            actions.append(el.text)\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Testing again\n",
    "sentence = 'He merrily ran down the lane to live another day'\n",
    "parsed=nlp_toolkit(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ran', 'live']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_actions(parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1d\n",
    "For each tweet, parse it using spacy and print it out if the tweet has 'taking' or 'joins' as a verb. You'll need to use your `mentions_company` and `get_actions` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7tFYwu\n",
      "\n",
      "RT @MFordFuture: Google takes quantum leap into artificial intelligence\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @MFordFuture: Google takes quantum leap into artificial intelligence\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @MFordFuture: Google takes quantum leap into artificial intelligence\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "RT @ramprasad_c: .@sardesairajdeep It takes 58 minutes now as per Google maps...unless of course Google is a Modi agent. https://t.co/flMt7…\n",
      "\n",
      "Microsoft joins Google and Facebook with warnings to users about government spying https://t.co/tXjvgeHFOu https://t.co/TW7aJb37q9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tweet in tweets:\n",
    "    parsed = nlp_toolkit(tweet)\n",
    "    if mentions_company(parsed, 'Google'):\n",
    "        actions = get_actions(parsed)\n",
    "        if 'takes' in actions or 'joins' in actions:\n",
    "            print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 1e\n",
    "Write a function that identifies countries - HINT: the entity label for countries is GPE (or GeoPolitical Entity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mentions_country(parsed, country):\n",
    "    for entity in parsed.ents:\n",
    "        if entity.text == country and entity.label_ == 'GPE':\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing it out\n",
    "sentence = 'A man, a plan, a canal, Panama'\n",
    "parsed=nlp_toolkit(sentence)\n",
    "mentions_country(parsed, country='Panama')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1f\n",
    "\n",
    "Re-run (d) to find country tweets that discuss 'Iran' announcing or releasing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @cerenomri: \"Literally every US ally in Mideast is on brink of hot war w/ Iran, so we're going to release $100 billion to Iran this mont…\n",
      "\n",
      "GOBE! Iran warns Nigeria to release Shiite leader El-Zakzaky - SEE https://t.co/TRshnC6sVU\n",
      "\n",
      "GOBE! Iran warns Nigeria to release Shiite leader El-Zakzaky - SEE https://t.co/SlvcQtk3vE\n",
      "\n",
      "RT @cerenomri: \"Literally every US ally in Mideast is on brink of hot war w/ Iran, so we're going to release $100 billion to Iran this mont…\n",
      "\n",
      "Hhmmm. Iran claiming to have 'warned Nigeria' to release detained Shiite leader.... @afalli\n",
      "\n",
      "RT @cerenomri: \"Literally every US ally in Mideast is on brink of hot war w/ Iran, so we're going to release $100 billion to Iran this mont…\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tweet in tweets:\n",
    "    parsed = nlp_toolkit(tweet)\n",
    "\n",
    "    if mentions_country(parsed, 'Iran'):\n",
    "        actions = get_actions(parsed)\n",
    "        if 'release' in actions or 'announce' in actions:\n",
    "            print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "Build a word2vec model of the tweets we have collected using gensim.\n",
    "First take the collection of tweets and tokenize them using spacy.\n",
    "\n",
    "### Exercise 2a:\n",
    "* Think about how this should be done. \n",
    "* Should you only use upper-case or lower-case? \n",
    "* Should you remove punctuations or symbols? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_split = [[x.text if x.pos != spacy.parts_of_speech.VERB else x.lemma_ \n",
    "                for x in nlp_toolkit(t)] for t in tweets]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2b:\n",
    "Build a word2vec model.\n",
    "Test the window size as well - this is how many surrounding words need to be used to model a word. What do you think is appropriate for Twitter? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(text_split, size=100, window=4, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2c:\n",
    "Test your word2vec model with a few similarity functions. \n",
    "* Find words similar to 'Syria'.\n",
    "* Find words similar to 'war'.\n",
    "* Find words similar to \"Iran\".\n",
    "* Find words similar to 'Verizon'. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'hitting' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-f2b192fd5ea6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hitting'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[1;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    503\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 505\u001b[1;33m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    506\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    507\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    451\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 453\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpositive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word 'hitting' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['Syria'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2d\n",
    "\n",
    "Adjust the choices in (b) and (c) as necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Filter tweets to those that mention 'Iran' or similar entities and another word of your choosing.\n",
    "* Do this using just spacy.\n",
    "* Do this using word2vec similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @f396: 'Iran has a long record in attacking foreign diplomatic missions,' Saudi ... - https://t.co/3gaSRB3osT via https://t.co/UjStGmTT2f\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using spacy\n",
    "for tweet in tweets:\n",
    "    parsed = nlp_toolkit(tweet)\n",
    "    if mentions_country(parsed, 'Iran') or mentions_country(parsed, 'Iraq'): # ... you could add more\n",
    "        if 'attacking' in get_actions(parsed):\n",
    "            print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LebanonHashtag: RT probrandz: #Facebook Open Sources Artificial #Intelligence Servers Before #Google | Re/code https://t.co/f8RTJxEzSx\n",
      "\n",
      "KalGuntuku: RT bunnymiilk \"RT 30seclovelive: [RT to win!] 30 Seconds of Love Live! 5k Followers Giveaway #1: $25 iTunes/Google Play gift ca…\n",
      "\n",
      "LebanonHashtag: RT probrandz: #Facebook Open Sources Artificial #Intelligence Servers Before #Google | Re/code https://t.co/VgWBxAG52q\n",
      "\n",
      "RT @f396: Iran blames America, Britain and 'Zionists' for Nimr execution - https://t.co/BwXEicgAOA via https://t.co/UjStGmTT2f\n",
      "\n",
      "RT @f396: Saudi Arabia severs diplomatic ties with Iran over embassy fire - https://t.co/r0iZugJa3v via https://t.co/UjStGmTT2f\n",
      "\n",
      "RT @f396: 'Iran has a long record in attacking foreign diplomatic missions,' Saudi ... - https://t.co/3gaSRB3osT via https://t.co/UjStGmTT2f\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using word2vec similarity scores\n",
    "for tweet in tweets[:200]:\n",
    "    parsed = nlp_toolkit(tweet)\n",
    "\n",
    "    similarity_to_iran = max([model.wv.similarity('Iran', tok.text) for tok in parsed if tok.text in model.wv.vocab])\n",
    "    similarity_to_war = max([model.wv.similarity('war', tok.text) for tok in parsed if tok.text in model.wv.vocab])\n",
    "    if similarity_to_iran > 0.99 and similarity_to_war > 0.99:\n",
    "        print (tweet)\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
