{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "#  Classifiers & K-Nearest Neighbors with `scikit-learn`\n",
    "\n",
    "_Authors: Alex Sherman (DC)_, Steven Longstreet (DC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"learning-objectives\"></a>\n",
    "## Learning Objectives\n",
    "\n",
    "1. Utilize the KNN model on the iris data set.\n",
    "2. Implement scikit-learn's KNN model.\n",
    "3. Assess the fit of a KNN Model using scikit-learn.\n",
    "4. Optimize KNN using GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Learning Objectives](#learning-objectives)\n",
    "- [Loading the Iris Data Set](#overview-of-the-iris-dataset)\n",
    "\t- [Terminology](#terminology)\n",
    "- [Exercise: \"Human Learning\" With Iris Data](#exercise-human-learning-with-iris-data)\n",
    "- [Human Learning on the Iris Data Set](#human-learning-on-the-iris-dataset)\n",
    "- [K-Nearest Neighbors (KNN) Classification](#k-nearest-neighbors-knn-classification)\n",
    "\t- [Using the Train/Test Split Procedure (K=1)](#using-the-traintest-split-procedure-k)\n",
    "- [Tuning a KNN Model](#tuning-a-knn-model)\n",
    "\t- [What Happens If We View the Accuracy of our Training Data?](#what-happen-if-we-view-the-accuracy-of-our-training-data)\n",
    "\t- [Training Error Versus Testing Error](#training-error-versus-testing-error)\n",
    "- [Standardizing Features](#standardizing-features)\n",
    "\t- [Use `StandardScaler` to Standardize our Data](#use-standardscaler-to-standardize-our-data)\n",
    "- [Comparing KNN With Other Models](#comparing-knn-with-other-models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to Classifiers!\n",
    "\n",
    "Previously we looked at linear models to predict **continuous numbers** using regression. What if we need to predict for **discrete values**?\n",
    "\n",
    "We have a few options:\n",
    "\n",
    "1. Build a classifier ourselves using a basic hueristic\n",
    "2. Write a complex classifier\n",
    "3. Leverage a Machine Learning Classifier\n",
    "\n",
    "Our approach may surprise you. From Google's [Rules of Machine Learning](https://developers.google.com/machine-learning/rules-of-ml/) there are a few steps to look at before implementing a machine learning algorithm. Often it's better to start with a simple hueristic to solve a goal, understand your data or serve as a baseline. Eventually that becomes too complex and hard to maintain so abiding by **rule three** we move onto machine learning and choose the right algorithm. But which model should we pick? There are a few available from SkLearn\n",
    "\n",
    "* [AdaBoostClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)\n",
    "* [BaggingClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)\n",
    "* [BernoulliNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html)\n",
    "* [CalibratedClassifierCV](http://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html)\n",
    "* [DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "* [ExtraTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.ExtraTreeClassifier.html)\n",
    "* [ExtraTreesClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)\n",
    "* [GaussianNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)\n",
    "* [GaussianProcessClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessClassifier.html)\n",
    "* [GradientBoostingClassifier](http://scikit-learn.org/0.15/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)\n",
    "* [KNeighborsClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "* [LabelPropagation](http://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.LabelPropagation.html)\n",
    "* [LabelSpreading](http://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.LabelSpreading.html)\n",
    "* [LinearDiscriminantAnalysis](http://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html)\n",
    "* [LinearSVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html)\n",
    "* [LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "* [LogisticRegressionCV](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html)\n",
    "* [MLPClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)\n",
    "* [MultinomialNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n",
    "* [NearestCentroid](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestCentroid.html)\n",
    "* [NuSVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html)\n",
    "* [PassiveAggressiveClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.PassiveAggressiveClassifier.html)\n",
    "* [Perceptron](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html)\n",
    "* [QuadraticDiscriminantAnalysis](http://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html)\n",
    "* [RadiusNeighborsClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.RadiusNeighborsClassifier.html)\n",
    "* [RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "* [RidgeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html)\n",
    "* [RidgeClassifierCV](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifierCV.html)\n",
    "* [SGDClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)\n",
    "* [SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
    "\n",
    "Whew! - That's alot of classifiers? And we're only using a smaller list in this class. However you may want to review these if your capstone project is using a classification algorithm. Here are a few terms to get you started in understanding the models. As your data science lexicon grows you should be able to return to this list and have a high level understanding of what each classifier is doing from title alone!\n",
    "\n",
    "* NB- Uses Naive Bayes\n",
    "* CV - Uses cross validation\n",
    "* SV - Uses a support vector (Support Vector Classification)\n",
    "\n",
    "#### Models often used in the Data Science Part Time Series\n",
    "* [DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "* [GaussianNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)\n",
    "* [KNeighborsClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "* [LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "* [LogisticRegressionCV](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html)\n",
    "* [MultinomialNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n",
    "* [NearestCentroid](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestCentroid.html)\n",
    "* [RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "* [RidgeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html)\n",
    "* [SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about today?\n",
    "\n",
    "Using your brilliant data science minds I believe your powers of deduction have helped you answer this question.\n",
    "\n",
    "In this lesson, we will get an intuitive and practical feel for the **k-Nearest Neighbors** model. kNN is a **non-parametric model** which means that it does not make any assumptions on the underlying data distribution. So, the model is not represented as an equation with parameters (e.g. the $\\beta$ values in linear regression).\n",
    "\n",
    "We'll introduce it to you later - but first lets go back to Google's Rules of Machine Learning. \n",
    "\n",
    ">**Before Machine Learning - Rule #2 - First, design and implement metrics**\n",
    "> As you work to understand your data a simple hueristic is easier to implement\n",
    "\n",
    ">**Rule #3: Choose machine learning over a complex hueristic**\n",
    "> After you get a basic understanding we move onto machine learning. \n",
    "\n",
    ">```A simple heuristic can get your product out the door. A complex heuristic is unmaintainable. Once you have data and a basic idea of what you are trying to accomplish, move on to machine learning. As in most software engineering tasks, you will want to be constantly updating your approach, whether it is a heuristic or a machine­learned model, and you will find that the machine­-learned model is easier to update and maintain - Google Machine Learning Rules```\n",
    "\n",
    "First, we will make a model by hand to classify iris flower data. Next, we will automatedly make a model using kNN.\n",
    "\n",
    "> You may have heard of the clustering algorithm **k-Means Clustering**. These techniques have nothing in common, aside from both having a parameter k!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"overview-of-the-iris-dataset\"></a>\n",
    "## Loading the Iris Data Set\n",
    "---\n",
    "\n",
    "#### Read the iris data into a pandas DataFrame, including column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the iris data into a DataFrame.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display plots in-notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Increase default figure and font sizes for easier viewing.\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.rcParams['font.size'] = 14\n",
    "\n",
    "data = 'data/iris.data'\n",
    "iris = pd.read_csv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.head(30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"terminology\"></a>\n",
    "### Terminology\n",
    "\n",
    "- **150 observations** (n=150): Each observation is one iris flower.\n",
    "- **Four features** (p=4): sepal length, sepal width, petal length, and petal width.\n",
    "- **Response**: One of three possible iris species (setosa, versicolor, or virginica)\n",
    "- **Classification problem** because response is categorical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sometimes a picture helps us understand the problem\n",
    "\n",
    "Knowing that plants **will** have variations - telling species apart from the image seems challenging. Luckily biology labs have endless supplies of High School, College and Master's students mixed in with PhD Candidates to spend their days measuring plants in a field. DNA sequencing for flowers is expensive so let's see if we can help them analyze their data\n",
    "\n",
    "![15NN classification map](./assets/iris-machinelearning.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"exercise-human-learning-with-iris-data\"></a>\n",
    "## Guided Practice: \"Human Learning\" With Iris Data\n",
    "\n",
    "**Question:** Can we predict the species of an iris using petal and sepal measurements? Together, we will:\n",
    "\n",
    "1. Read the iris data into a Pandas DataFrame, including column names.\n",
    "2. Gather some basic information about the data.\n",
    "3. Use sorting, split-apply-combine, and/or visualization to look for differences between species.\n",
    "4. Write down a set of rules that could be used to predict species based on iris measurements.\n",
    "\n",
    "**BONUS:** Define a function that accepts a row of data and returns a predicted species. Then, use that function to make predictions for all existing rows of data and check the accuracy of your predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Gather some basic information about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 150 observations, 5 columns (the 4 features & response)\n",
    "iris.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What kinds of data are we working with?\n",
    "iris.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the basic stats look appropriate\n",
    "iris.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for imbalanced classes (hint - see what happens when you switch the normalize parameter to true)\n",
    "iris.species.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify we are not missing any data\n",
    "iris.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Use sorting, split-apply-combine, and/or visualization to look for differences between species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by petal_width.\n",
    "iris.sort_values(by='petal_width', ascending=True, inplace=True)\n",
    "iris.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Split-apply-combine: Explore the data while using a `groupby` on `'species'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of sepal_length, grouped by species.\n",
    "iris.groupby(by='species')['sepal_length'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of all numeric columns, grouped by species.\n",
    "iris.groupby('species').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe() of all numeric columns, grouped by species.\n",
    "iris.groupby('species').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot of petal_width, grouped by species.\n",
    "iris.boxplot(column=['petal_width', 'petal_length'], by='species', figsize=(10,8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot of all numeric columns, grouped by species.\n",
    "# I believe this is the first time we've used rot before. Remove it and see if you understand why\n",
    "iris.boxplot(by='species', rot=45, figsize=(10,8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map species to a numeric value so that plots can be colored by species.\n",
    "iris['species_num'] = iris.species.map({'Iris-setosa':0,\n",
    "                                        'Iris-versicolor':1,\n",
    "                                        'Iris-virginica':2})\n",
    "\n",
    "# Alternative method:\n",
    "#iris['species_num'] = iris.species.factorize()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot of petal_length vs. petal_width, colored by species\n",
    "iris.plot(kind='scatter', x='petal_length', y='petal_width', c='species_num', colormap='brg');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter matrix of all features, colored by species.\n",
    "pd.plotting.scatter_matrix(iris.drop('species_num', axis=1), c=iris.species_num, figsize=(12, 10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Class Exercise: Using the graphs above, can you write down a set of rules that can accurately predict species based on iris measurements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to do more analysis if needed to make good rules!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Bonus: If you have time during the class break or after class, try to implement these rules to make your own classifier!\n",
    "\n",
    "Write a function that accepts a row of data and returns a predicted species. Then, use that function to make predictions for all existing rows of data and check the accuracy of your predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_flower(df):\n",
    "    preds = ['Iris-setosa'] * len(df)   # temporary!\n",
    "    \n",
    "    # for each row of df, make a prediction\n",
    "\n",
    "    # add a column to the DataFrame with the predictions\n",
    "    df['prediction'] = preds\n",
    "    \n",
    "    \n",
    "predict_flower(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what percentage your manual classifier gets correct!\n",
    "# 0.3333 means 1/3 are classified correctly\n",
    "\n",
    "sum(iris.species == iris.prediction) / 150."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"human-learning-on-the-iris-dataset\"></a>\n",
    "## Human Learning on the Iris Data Set\n",
    "---\n",
    "\n",
    "How did we (as humans) predict the species of an iris?\n",
    "\n",
    "1. We observed that the different species had (somewhat) dissimilar measurements.\n",
    "2. We focused on features that seemed to correlate with the response.\n",
    "3. We created a set of rules (using those features) to predict the species of an unknown iris.\n",
    "\n",
    "We assumed that if an **unknown iris** had measurements similar to **previous irises**, then its species was most likely the same as those previous irises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow plots to appear in the notebook.\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Increase default figure and font sizes for easier viewing.\n",
    "plt.rcParams['figure.figsize'] = (10, 8)\n",
    "plt.rcParams['font.size'] = 14\n",
    "\n",
    "# Create a custom color map.\n",
    "from matplotlib.colors import ListedColormap\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"k-nearest-neighbors-knn-classification\"></a>\n",
    "## Introducing K-Nearest Neighbors (KNN) Classification\n",
    "---\n",
    "\n",
    "K-nearest neighbors classification is (as its name implies) a classification model that uses the \"K\" most similar observations in order to make a prediction.\n",
    "\n",
    "KNN is a supervised learning method; therefore, the training data must have known target values.\n",
    "\n",
    "\n",
    "The process of of prediction using KNN is fairly straightforward:\n",
    "\n",
    "1. Pick a value for K.\n",
    "2. Search for the K observations in the data that are \"nearest\" to the measurements of the unknown iris.\n",
    "    - Euclidian distance is often used as the distance metric, but other metrics are allowed.\n",
    "3. Use the most popular response value from the K \"nearest neighbors\" as the predicted response value for the unknown iris.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Model\n",
    "\n",
    "Conceptually, KNN is very simple. Given a dataset for which class labels are known, you want to predict the class of a new data point.\n",
    "\n",
    "The strategy is to compare the new observation to those observations already labeled. The predicted class will be based on the known classes of the nearest k neighbors (i.e. based on the class labels of the other data points most similar to the one you're trying to predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of KNN in practice\n",
    "Imagine a bunch of widgets which belong to either the \"Blue\" or the \"Red\" class. Each widget has 2 variables associated with it: **x** and **y**. We can plot our widgets in 2D.\n",
    "\n",
    "![Basic Graph](./assets/knn_reds_and_blues.png)\n",
    "\n",
    "\n",
    "#### Adding a new point\n",
    "Now let's say I get another widget (**black dot**) that also has **x** and **y** variables. How can we tell whether this widget is blue or red?\n",
    "\n",
    "![New Point](./assets/knn_new_point.png)\n",
    "\n",
    "#### Visualizing KNN in action\n",
    "The KNN approach to classification calls for comparing this new point to the other nearby points. If we were using KNN with 3 neighbors, we'd grab the 3 nearest dots to our black dot and look at the colors. The nearest dots would then \"vote\", with the more predominant color being the color we'll assign to our new black dot\n",
    "\n",
    "![New Point](./assets/knn_new_point_with_circle.png)\n",
    "\n",
    "#### Presto - Chango: We get a red dot!\n",
    "\n",
    "![New Point](./assets/knn_new_point_pred.png)\n",
    "\n",
    "\n",
    "#### Let's try a prediction with 5 neighbors\n",
    "\n",
    "If we had chosen 5 neighbors instead of 3 neighbors, things would have turned out differently. Looking at the plot below, we can see that the vote would tally blue: 3, red: 2. So we would classify our new dot as blue.\n",
    "\n",
    "##### New Circle\n",
    "![New Point](./assets/knn_5_circle.png)\n",
    "\n",
    "##### New Color\n",
    "![New Point](./assets/knn_new_point_pred_blue.png)\n",
    "\n",
    "[source](http://blog.yhat.com/posts/classification-using-knn-and-python.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does KNN make it's choice? It's democratic\n",
    "\n",
    "![New Point](./assets/knn2.jpg)\n",
    "\n",
    "[source](https://www.kdnuggets.com/2016/01/implementing-your-own-knn-using-python.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the right value of k\n",
    "\n",
    "KNN requires us to choose a k value. Logically, your next question is \"how do we choose k?\" Brilliant Question!\n",
    "\n",
    "It turns out that choosing the right number of neighbors matters. A lot. But, choosing the right k is as challenging as it is important.\n",
    "\n",
    ">...choosing the best value for k is [\"not easy but laborious\"](https://books.google.com/books?id=Vhw8KlcYaxQC&pg=PA128&lpg=PA128&dq=%22Choosing+the+best+value+for+k+is+not+easy+but+laborious%22&source=bl&ots=UtjE33ajQT&sig=eAAJdj0zazrJatCHy7ZC3-r03WI&hl=en&sa=X&ei=fFXxUfKPFLT_4APbjoHICg&ved=0CCoQ6AEwAA#v=onepage&q=%22Choosing%20the%20best%20value%20for%20k%20is%20not%20easy%20but%20laborious%22&f=false)\n",
    "\n",
    "> Phillips and Lee, Mining Positive Associations of Urban Criminal Activities\n",
    "\n",
    "With KNN our goal is to maximize the accuracy of the prediction. \n",
    "- Part of that is understanding what we can glean from our dataset. \n",
    "- Part is understanding what error can tell us. \n",
    "- Part is starting with some good understandings of k.\n",
    "\n",
    "Generally it's good to try an odd number for k to start out. This helps avoid situations where your classifier \"ties\" as a result of having the same number of votes for two different classes. This is particularly true if your dataset has only two classes (i.e. if k=4 and an observation has nearest neighbors ['blue', 'blue', 'red', 'red'], you've got a tie on your hands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at it with our IRIS data\n",
    "\n",
    "The visualizations below show how a given area can change in its prediction as K changes.\n",
    "\n",
    "- Colored points represent true values and colored areas represent a **prediction space**. (This is called a Voronoi Diagram.)\n",
    "- Each prediction space is where the majority of the \"K\" nearest points are the color of the space.\n",
    "- To predict the class of a new point, we guess the class corresponding to the color of the space it lies in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"knn-classification-map-for-iris-k\"></a>\n",
    "### KNN Classification Map for Iris (K=1)\n",
    "\n",
    "![1NN classification map](./assets/iris_01nn_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Classification Map for Iris (K=5)\n",
    "\n",
    "![5NN classification map](./assets/iris_05nn_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Classification Map for Iris (K=15)\n",
    "\n",
    "![15NN classification map](./assets/iris_15nn_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"knn-classification-map-for-iris-k\"></a>\n",
    "### KNN Classification Map for Iris (K=50)\n",
    "\n",
    "![50NN classification map](./assets/iris_50nn_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, as K increases, the classification spaces' borders become more distinct. However, you can also see that the spaces are not perfectly pure when it comes to the known elements within them.\n",
    "\n",
    "**How are outliers affected by K?** As K increases, outliers are \"smoothed out\". Look at the above three plots and notice how outliers strongly affect the prediction space when K=1. When K=50, outliers no longer affect region boundaries. This is a classic bias-variance tradeoff -- with increasing K, the bias increases but the variance decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What's the \"best\" value for K in this case?\n",
    "\n",
    "**Answer:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guided Intro to KNN: NBA Position KNN Classifier\n",
    "\n",
    "For the rest of the lesson, we will be using a dataset containing the 2015 season statistics for ~500 NBA players. This dataset leads to a nice choice of K, as we'll see below. The columns we'll use for features (and the target 'pos') are:\n",
    "\n",
    "\n",
    "| Column | Meaning |\n",
    "| ---    | ---     |\n",
    "| pos | C: Center. F: Front. G: Guard |\n",
    "| ast | Assists per game | \n",
    "| stl | Steals per game | \n",
    "| blk | Blocks per game |\n",
    "| tov | Turnovers per game | \n",
    "| pf  | Personal fouls per game | \n",
    "\n",
    "For information about the other columns, see [this glossary](https://www.basketball-reference.com/about/glossary.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the NBA data into a DataFrame.\n",
    "import pandas as pd\n",
    "\n",
    "path = 'data/NBA_players_2015.csv'\n",
    "nba = pd.read_csv(path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map positions to numbers\n",
    "nba['pos_num'] = nba.pos.map({'C':0, 'F':1, 'G':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature matrix (X).\n",
    "feature_cols = ['ast', 'stl', 'blk', 'tov', 'pf']\n",
    "X = nba[feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create response vector (y).\n",
    "y = nba.pos_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"using-the-traintest-split-procedure-k\"></a>\n",
    "### Using the Train/Test Split Procedure (K=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Split X and y into training and testing sets (using `random_state` for reproducibility)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Train the model on the training set (using K=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Test the model on the testing set and check the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class = knn.predict(X_test)\n",
    "print((metrics.accuracy_score(y_test, y_pred_class)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** If we had trained on the entire dataset and tested on the entire dataset, using 1-KNN what accuracy would we likely get? If the resulting accuracy is not this number, what must some data points look like?\n",
    "\n",
    "**Answer:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Repeating for K=50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=50)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_class = knn.predict(X_test)\n",
    "print((metrics.accuracy_score(y_test, y_pred_class)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Suppose we again train and test on the entire data set, but using 50-KNN. Would we expect the accuracy to be higher, lower, or the same as compared to 1-KNN?\n",
    "\n",
    "**Answer:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Comparing Testing Accuracy With Null Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Null accuracy is the accuracy that can be achieved by **always predicting the most frequent class**. For example, if most players are Centers, we would always predict Center.\n",
    "\n",
    "The null accuracy is a benchmark against which you may want to measure every classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine the class distribution from the training set.\n",
    "\n",
    "Remember that we are comparing KNN to this simpler model. So, we must find the most frequent class **of the training set**.\n",
    "\n",
    "**Reminder** - we mapped positions as {'C':0, 'F':1, 'G':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_freq_class = y_train.value_counts().index[0]\n",
    "\n",
    "print(y_train.value_counts())\n",
    "most_freq_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute null accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()[most_freq_class] / len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tuning-a-knn-model\"></a>\n",
    "## Tuning a KNN Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model (using the value K=5).\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Fit the model with data.\n",
    "knn.fit(X, y)\n",
    "\n",
    "# Store the predicted response values.\n",
    "y_pred_class = knn.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Which model produced the correct predictions for the two unknown irises?\n",
    "\n",
    "**Answer:** ...\n",
    "\n",
    "**Question:** Does that mean that we have to guess how well our models are likely to do?\n",
    "\n",
    "**Answer:** ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate predicted probabilities of class membership.\n",
    "# Each row sums to one and contains the probabilities of the point being a 0-Center, 1-Front, 2-Guard.\n",
    "knn.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How can we use these predictions?\n",
    "# Calculate predicted probabilities of class membership.\n",
    "# Each row sums to one and contains the probabilities of the point being a 0-Center, 1-Front, 2-Guard.\n",
    "\n",
    "preds = pd.DataFrame(knn.predict_proba(X))\n",
    "X_withpreds = pd.concat([X.reset_index(), preds.reset_index()], axis=1)\n",
    "X_withpreds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the predicted class\n",
    "preds = pd.DataFrame(knn.predict(X), columns=['pos'])\n",
    "X_withpreds = pd.concat([X.reset_index(), preds.reset_index()], axis=1)\n",
    "X_withpreds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"what-happen-if-we-view-the-accuracy-of-our-training-data\"></a>\n",
    "### What Happens If We View the Accuracy of our Training Data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for k in range(1,101):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X,y)\n",
    "    pred = knn.predict(X)\n",
    "    score = float(sum(pred == y)) / len(y)\n",
    "    scores.append([k, score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(scores,columns=['k','score'])\n",
    "data.plot.line(x='k',y='score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** As K increases, why does the accuracy fall?\n",
    "\n",
    "**Answer:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Search for the \"best\" value of K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate TRAINING ERROR and TESTING ERROR for K=1 through 100.\n",
    "\n",
    "k_range = list(range(1, 101))\n",
    "training_error = []\n",
    "testing_error = []\n",
    "\n",
    "# Find test accuracy for all values of K between 1 and 100 (inclusive).\n",
    "for k in k_range:\n",
    "\n",
    "    # Instantiate the model with the current K value.\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate training error (error = 1 - accuracy).\n",
    "    y_pred_class = knn.predict(X)\n",
    "    training_accuracy = metrics.accuracy_score(y, y_pred_class)\n",
    "    training_error.append(1 - training_accuracy)\n",
    "    \n",
    "    # Calculate testing error.\n",
    "    y_pred_class = knn.predict(X_test)\n",
    "    testing_accuracy = metrics.accuracy_score(y_test, y_pred_class)\n",
    "    testing_error.append(1 - testing_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow plots to appear in the notebook.\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of K, training error, and testing error.\n",
    "column_dict = {'K': k_range, 'training error':training_error, 'testing error':testing_error}\n",
    "df = pd.DataFrame(column_dict).set_index('K').sort_index(ascending=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the relationship between K (HIGH TO LOW) and TESTING ERROR.\n",
    "df.plot(y='testing error', linewidth=2, figsize=(10, 8));\n",
    "plt.xlabel('Value of K for KNN');\n",
    "plt.ylabel('Error (lower is better)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the minimum testing error and the associated K value.\n",
    "df.sort_values('testing error').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative method:\n",
    "min(list(zip(testing_error, k_range)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"training-error-versus-testing-error\"></a>\n",
    "### Training Error Versus Testing Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the relationship between K (HIGH TO LOW) and both TRAINING ERROR and TESTING ERROR.\n",
    "df.plot();\n",
    "plt.xlabel('Value of K for KNN');\n",
    "plt.ylabel('Error (lower is better)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Training error** decreases as model complexity increases (lower value of K)\n",
    "- **Testing error** is minimized at the optimum model complexity\n",
    "\n",
    "Evaluating the training and testing error is important. For example:\n",
    "\n",
    "- If the training error is much lower than the test error, then our model is likely overfitting. \n",
    "- If the test error starts increasing as we vary a hyperparameter, we may be overfitting.\n",
    "- If either error plateaus, our model is likely underfitting (not complex enough)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Training_testing_error](./assets/training_testing_error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the best value of k for this model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Making Predictions on Out-of-Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Given the statistics of a (truly) unknown NBA player, how do we predict his position?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "\n",
    "# Instantiate the model with the best-known parameters.\n",
    "knn = KNeighborsClassifier(n_neighbors=14)\n",
    "\n",
    "# Re-train the model with X and y (not X_train and y_train). Why?\n",
    "knn.fit(X, y)\n",
    "\n",
    "# Make a prediction for an out-of-sample observation.\n",
    "knn.predict(np.array([2, 1, 0, 1, 2]).reshape(1, -1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What's the probability of that prediction?\n",
    "np.amax(knn.predict_proba([[2, 1, 0, 1, 2]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(list(zip(testing_error, k_range)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "What could we conclude?\n",
    "\n",
    "- When using KNN on this data set with these features, the **best value for K** is likely to be around 14.\n",
    "- Given the statistics of an **unknown player**, we estimate that we would be able to correctly predict his position about 74% of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing GridSearch\n",
    "\n",
    "[GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "\n",
    "#### What is GridSearch?\n",
    "\n",
    "We did alot of work above to find and refine for the best value of **k**. Bad news - that wasn't the only parameter we could optimize!\n",
    "\n",
    "Luckily GridSearch was designed for the intellectually curious and the realistically lazy practicioners such as ourselves. I love the official description from Sklearn:\n",
    "\n",
    "> *Exhaustive search over specified parameter values for an estimator.*\n",
    "\n",
    "GridSearch allows you to define a grid of parameters that will be searched using K-fold cross-validation. Essentially it's going to automate our **for** loop from above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two ways we can explore this section below\n",
    "\n",
    "1. Let's start with the Iris Dataset\n",
    "2. Use the second cell on your own to review against the NBA dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you chose the Iris dataset let's run this in\n",
    "# read in the iris data\n",
    "iris = load_iris()\n",
    "\n",
    "# create X (features) and y (response)\n",
    "X = iris.data\n",
    "y = iris.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're using the Basketball dataset then run this cell - otherwise skip for now. Commented out to protect it\n",
    "\n",
    "# Create feature matrix (X).\n",
    "#feature_cols = ['ast', 'stl', 'blk', 'tov', 'pf']\n",
    "\n",
    "#X = nba[feature_cols]\n",
    "#y = nba.pos_num  # Create response vector (y)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's run a KNN using a cross-validation of 10 with k=5\n",
    "\n",
    "**Notes**\n",
    "- We'll instantiate an object called 'scores' to store our cross validation scoress\n",
    "- Our scoring method will be 'accuracy' since it's a classification problem\n",
    "- Cross validation will take care of our splitting so we don't need a train_test_split. We'll pass our whole X and y into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# instantiate model\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Capture our cross validation scores\n",
    "scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We use average accuracy as an estimate of out-of-sample accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What kind of object is scores?\n",
    "type(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's a numpy array! We know those! Let's calculate the mean to give us an estimate of out-of-sample accuracy\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Good - well to understand where we're going we have to understand where we've been. Let's run through this quickly the old way\n",
    "\n",
    "In the below we'll capture the accuaracy of k from 1 to 30. To do so we'll\n",
    "- establish a range to pass to k\n",
    "- Create a list to hold our scores\n",
    "- Design a for loop to do the heavy lifting for us\n",
    "- Look at our scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of integers 1 to 30\n",
    "k_range = range(1, 31)\n",
    "# list of scores from k_range\n",
    "k_scores = []\n",
    "# 1. we will loop through reasonable values of k\n",
    "for k in k_range:\n",
    "    # 2. run KNeighborsClassifier with k neighbours\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    # 3. obtain cross_val_score for KNeighborsClassifier with k neighbours\n",
    "    scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n",
    "    # 4. append mean of scores for k neighbors to k_scores list\n",
    "    k_scores.append(scores.mean())\n",
    "print(k_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_dict=dict(zip(k_range,k_scores))\n",
    "k_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What value of k gave us our highest score?\n",
    "max(k_dict, key= k_dict.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we'll plot it to see the accuracy over time\n",
    "\n",
    "How is this different than our NBA model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the value of K for KNN (x-axis) versus the cross-validated accuracy (y-axis)\n",
    "plt.plot(k_range, k_scores)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Cross-Validated Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Even with some practice this isn't as  intuitive. Let's take a step forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have our k_range let's use that to build a parameter grid\n",
    "\n",
    "param_grid=dict(n_neighbors=list(k_range))\n",
    "print(param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we're ready to cook with gas\n",
    "\n",
    "Let's instantiate a GridSearchCV algorithm. From the documentation we see what we need to pass into it\n",
    "\n",
    ">```class sklearn.model_selection.GridSearchCV(estimator, param_grid, scoring=None, fit_params=None, n_jobs=1, iid=True, refit=True, cv=None, verbose=0, pre_dispatch=‘2*n_jobs’, error_score=’raise’, return_train_score=’warn’)```\n",
    "\n",
    "- estimator = KNN\n",
    "- param_grid = pass in our newly minted param_grid\n",
    "- scoring = 'accuracy' (since it's a classification)\n",
    "- cv=10\n",
    "- Everything else (leave as a default for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a GridSearch object - here a classifier (clf)\n",
    "clf = GridSearchCV(knn, param_grid, cv=10, scoring='accuracy', return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What just happened?\n",
    "\n",
    "The 'grid' object is ready to do 10-fold cross validation on a KNN model using classification accuracy as the evaluation metric\n",
    "- In addition, there is a parameter grid to repeat the 10-fold cross validation process 30 times\n",
    "- Each time, the n_neighbors parameter should be given a different value from the list\n",
    "- We can't give GridSearchCV just a list. We had to specify a particular parameter for our model, KNN, and give it values. In this case we told it n_neighbors should take a value of 1 through 30\n",
    "- You can also set n_jobs = -1 to run computations in parallel (if supported by your computer and OS). This is called parallel programming\n",
    "\n",
    "#### Well if it's ready - Let's kick the tires and light the fires!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Want to know everything about what we just tested? Warning! It's exhaustive!\n",
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can slice by the key/value pairs you want to make this a little easier to understand\n",
    "\n",
    "list(zip(clf.cv_results_['mean_test_score'],clf.cv_results_['params']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can find ways to make this easier on ourselves\n",
    "knn_score=list(zip(clf.cv_results_['mean_test_score'],clf.cv_results_['params']))\n",
    "\n",
    "sorted(knn_score,key=lambda x: x[0],reverse=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also skip to the end and stand on the shoulders of the people who came before us\n",
    "print(clf.best_score_)\n",
    "print(clf.best_params_)\n",
    "knn=clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "# this is identical to the one we generated above\n",
    "mean_scores=clf.cv_results_['mean_test_score'].tolist()\n",
    "plt.plot(k_range, mean_scores)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Cross-Validated Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you're not convinced of the power of GridSearch yet - strap yourself in\n",
    "\n",
    "We just tuned KNN for the k value - but what else can we pass to [KNN](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)?\n",
    "\n",
    ">```KNeighborsClassifier(n_neighbors=5, weights=’uniform’, algorithm=’auto’, leaf_size=30, p=2, metric=’minkowski’, metric_params=None, n_jobs=1, **kwargs)```\n",
    "\n",
    "\n",
    "We could try them all but for now let's increase to two parameters\n",
    "1. **n_neighbors**\n",
    "2. **weights**\n",
    "\n",
    "Here's what you need to know about weights from the documentation\n",
    "\n",
    ">**weights** : str or callable, optional (default = ‘uniform’)\n",
    "\n",
    ">weight function used in prediction. Possible values:\n",
    "> - ‘uniform’ : uniform weights. All points in each neighborhood are weighted equally.\n",
    "> - ‘distance’ : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a > - greater influence than neighbors which are further away.\n",
    "> - [callable] : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights.\n",
    "\n",
    "We won't make our own callable function this time but it's important to know we could pass in our own if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's setup our param_grid\n",
    "\n",
    "#First the k's\n",
    "k_range=list(range(1,31))\n",
    "\n",
    "#Now we'll add in weights\n",
    "weight_options=['uniform','distance']\n",
    "\n",
    "#Remember - we need to pass these to the param_grid as a dictionary\n",
    "param_grid=dict(n_neighbors=k_range, weights=weight_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Never hurts to check our work\n",
    "print(param_grid)\n",
    "print(type(param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now to instantiate our grid\n",
    "\n",
    "clf=GridSearchCV(knn,param_grid,cv=10, scoring='accuracy')\n",
    "\n",
    "#Next we fit the grid\n",
    "\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using our cheat sheet to pull out the accuracy and parameters values from the grid\n",
    "knn_score=list(zip(clf.cv_results_['mean_test_score'],clf.cv_results_['params']))\n",
    "\n",
    "sorted(knn_score,key=lambda x: x[0],reverse=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally - what worked best?\n",
    "\n",
    "print(clf.best_score_)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fantastic! - GridSearch found the best parameters for our model\n",
    "\n",
    "#### Next - let's use them to make predictions\n",
    "\n",
    "Steps\n",
    " - Import the model (already done)\n",
    " - Instantiate the model with the best parameters\n",
    " - Fit the model\n",
    " - Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model with best parameters\n",
    "knn = KNeighborsClassifier(n_neighbors=13, weights='uniform')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick note on fitting\n",
    "\n",
    "We are going to make predictions with **new** data. At this point I don't need cross validation or train_test_split. If I train with either method I may throw away potentially valuable data given how KNN works. We want to use all known observations for our classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the model\n",
    "knn.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction. Here we need our four measurements\n",
    "\n",
    "knn.predict([[1,2,3,4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We've taken our ideal parameters and used them in our model to make the more accurate prediction\n",
    "\n",
    "#### But... is there an easier way?\n",
    "\n",
    "Remember how I said GridSearch was for the intellectually curious but... lazy? Well there's another shortcut.\n",
    "\n",
    "Since we passed our model, data and parameter ranges to GridSearch - it has everything it needs to make a prediction on the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here's your shortcut\n",
    "\n",
    "clf.predict([[1,2,3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does that mean? Well either you look at the data dictionary for Iris - or you inspect the data. To keep it easy - let's convert iris back to a dataframe\n",
    "\n",
    "iris.target_names\n",
    "\n",
    "# What was our prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Great Tools\n",
    "\n",
    "1. [RandomizedSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV) Eventually you'll want to search across numerous parameters ranging with len(values)>100 then you may reach the limits of time or your machine. Instead we use something like RandomizedSearchCV\n",
    "> In contrast to GridSearchCV, not all parameter values are tried out, but rather a fixed number of parameter settings is sampled from the specified distributions. The number of parameter settings that are tried is given by n_iter.\n",
    "\n",
    "2. [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) From our preprocessing toolkit this package allows you to address the many machine learning models that are sensitive to scale. More information is outlined in the notebook below as this **WILL** be important for many of your projects\n",
    "    - [PreProcessing Tools](http://scikit-learn.org/stable/modules/preprocessing.html) Actually - it's worth looking through the whole preprocessing toolset\n",
    "\n",
    "\n",
    "\n",
    "3. [LabelProgatation](http://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.LabelPropagation.html#sklearn.semi_supervised.LabelPropagation) Label Propagation is a semi-supervised machine learning algorithm that assigns labels to previously unlabeled data points. At the start of the algorithm, a (generally small) subset of the data points have labels (or classifications). These labels are propagated to the unlabeled points throughout the course of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"standardizing-features\"></a>\n",
    "## Standardizing Features\n",
    "---\n",
    "\n",
    "There is one major issue that applies to many machine learning models: They are sensitive to feature scale. \n",
    "\n",
    "> KNN in particular is sensitive to feature scale because it (by default) uses the Euclidean distance metric. To determine closeness, Euclidean distance sums the square difference along each axis. So, if one axis has large differences and another has small differences, the former axis will contribute much more to the distance than the latter axis.\n",
    "\n",
    "This means that it matters whether our feature are centered around zero and have similar variance to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, most data does not naturally start at a mean of zero and a shared variance. Other models tend to struggle with scale as well, even linear regression, when you get into more advanced methods such as regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortuantely, this is an easy fix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"use-standardscaler-to-standardize-our-data\"></a>\n",
    "### Use `StandardScaler` to Standardize our Data\n",
    "\n",
    "StandardScaler standardizes our data by subtracting the mean from each feature and dividing by its standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate feature matrix and response for scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature matrix (X).\n",
    "feature_cols = ['ast', 'stl', 'blk', 'tov', 'pf']\n",
    "\n",
    "X = nba[feature_cols]\n",
    "y = nba.pos_num  # Create response vector (y)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the train/test split.\n",
    "\n",
    "Notice that we create the train/test split first. This is because we will reveal information about our testing data if we standardize right away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Instantiate and fit `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's investigate what we did\n",
    "pd.DataFrame(X_train).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit a KNN model and look at the testing error.\n",
    "Can you find a number of neighbors that improves our results from before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate testing error.\n",
    "knn = KNeighborsClassifier(n_neighbors=15)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred_class = knn.predict(X_test)\n",
    "testing_accuracy = metrics.accuracy_score(y_test, y_pred_class)\n",
    "testing_error = 1 - testing_accuracy\n",
    "\n",
    "print(testing_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"comparing-knn-with-other-models\"></a>\n",
    "## Comparing KNN With Other Models\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages of KNN:**\n",
    "\n",
    "- It's simple to understand and explain.\n",
    "- Lazy Learner so model training is fast.\n",
    "- It can be used for classification and regression (for regression, take the average value of the K nearest points!).\n",
    "- Being a non-parametric method, it is often successful in classification situations where the decision boundary is very irregular.\n",
    "\n",
    "**Disadvantages of KNN:**\n",
    "\n",
    "- It must store all of the training data.\n",
    "- Its prediction phase can be slow when n is large.\n",
    "- It is sensitive to irrelevant features.\n",
    "- It is sensitive to the scale of the data. (checkout our standardization method at the end of this notebook)\n",
    "- Accuracy is (generally) not competitive with the best supervised learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Exercise - Predicting Customer Churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "\n",
    "cell = pd.read_csv('./data/cell_phone_churn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assign X and Y variables.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = cell['churn']\n",
    "X = cell.iloc[:, 0:19]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert y variable if needed and create dummy variables as needed**\n",
    "\n",
    "Learning a new tools with [Label Encoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T15:56:29.626424Z",
     "start_time": "2019-11-19T15:56:26.809305Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Convert the y variable to a binary variable\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummify Categorical Variables in X dataframe, so it can be input into model\n",
    "X = pd.get_dummies(X)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split into train test using 70/30 split. The solutions use random_state=1234.  You can select any value but just note the results may not match, but should hopefully be close.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test =  train_test_split(X,y, test_size=0.3, random_state=1984)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate and fit `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Baseline Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell.churn.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_test).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "k = numpy.arange(20) + 1\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "params = {'n_neighbors': k}\n",
    "gs = GridSearchCV(    \n",
    "    estimator= knn,\n",
    "    param_grid= params,\n",
    "    cv=5)\n",
    "\n",
    "X= scaler.transform(X)\n",
    "\n",
    "gs.fit(X, y)\n",
    "gs.cv_results_['mean_test_score'] #Outputs mean accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which K is best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k[gs.best_index_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use that K with the train/test split.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate testing error.\n",
    "knn = KNeighborsClassifier(n_neighbors=2)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "train_pred = knn.predict(X_train)\n",
    "test_pred = knn.predict(X_test)\n",
    "# Calculate train and test accuracy\n",
    "print('Training Accuracy Score:',accuracy_score(y_train, train_pred))\n",
    "print('Testing Accuracy Score:',accuracy_score(y_test, test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Repeat Using Feature Selection\n",
    "\n",
    "Reassign X and y variable from start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = cell['churn']\n",
    "X = cell.iloc[:, 0:19]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert y variable if needed and create dummy variables as needed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the y variable to a binary variable\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummify Categorical Variables in X dataframe, so it can be input into model\n",
    "X = pd.get_dummies(X)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split into train test using 70/30 split.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=1984)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T16:00:18.056665Z",
     "start_time": "2019-11-19T16:00:18.052676Z"
    }
   },
   "source": [
    "#### Instantiate and fit `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "# feature extraction\n",
    "model = ExtraTreesRegressor(random_state=99)\n",
    "model.fit(X_train, y_train)\n",
    "print(model.feature_importances_) #Higher scores are better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for _,x in sorted(zip(model.feature_importances_,X), reverse=True)][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reassign X with new columns and resplit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X[[x for _,x in sorted(zip(model.feature_importances_,X), reverse=True)][0:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T16:01:17.081641Z",
     "start_time": "2019-11-19T16:01:17.076656Z"
    }
   },
   "source": [
    "**Split into train test using 70/30 split.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=1984)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate and fit `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "k = numpy.arange(20) + 1\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "params = {'n_neighbors': k}\n",
    "gs = GridSearchCV(    \n",
    "    estimator= knn,\n",
    "    param_grid= params,\n",
    "    cv=5)\n",
    "\n",
    "X= scaler.transform(X)\n",
    "\n",
    "gs.fit(X, y)\n",
    "gs.cv_results_['mean_test_score'] #Outputs mean accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which K is best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k[gs.best_index_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using that K run with the train/test split.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate testing error.\n",
    "knn = KNeighborsClassifier(n_neighbors=11)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "train_pred = knn.predict(X_train)\n",
    "test_pred = knn.predict(X_test)\n",
    "# Calculate train and test accuracy\n",
    "print('Training Accuracy Score:',accuracy_score(y_train, train_pred))\n",
    "print('Testing Accuracy Score:',accuracy_score(y_test, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Append a prediction column to the original dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=knn.predict(X)\n",
    "cell['predicted_churn']=pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As a manager, how would you use this model and what strategies would you put in place?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
