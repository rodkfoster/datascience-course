{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Intro to clustering and k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LEARNING OBJECTIVES\n",
    "*After this lesson, you will be able to:*\n",
    "- Format and preprocess data for clustering\n",
    "- Perform a K-Means clustering analysis\n",
    "- Evaluate clusters for fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](http://img.tesco.com/Groceries/pi/476/5011546405476/IDShot_540x540.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Being able to create clusters is a powerful (and delicious!) tool that will make you a stronger data scientist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# What's the difference between supervised and unsupervised learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](assets/images/supervised-vs-unsupervised.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Classification** - create a model to predict which group a point belongs to\n",
    "\n",
    "**Clustering** - find groups that exist in the data already"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How could unsupervised learning or clustering be useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Helpful uses for clustering: \n",
    "   - Find items with similar behavior (users, products, voters, etc)\n",
    "   - Market segmentation\n",
    "   - Understand complex systems\n",
    "   - Discover meaningful categories for your data\n",
    "   - Reduce the number of classes by grouping (e.g. bourbons, scotches -> whiskeys)\n",
    "   - Reduce the dimensions of your problem\n",
    "   - Pre-processing! Create labels for supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Great. Clustering is useful.\n",
    "-\n",
    "# Any ideas how to algorithmically tell which groups are different?\n",
    "\n",
    "![](http://home.deib.polimi.it/matteucc/Clustering/tutorial_html/images/clustering.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# K Means\n",
    ".\n",
    "\n",
    "### My First<sup>TM</sup> Clustering Algorithm\n",
    "\n",
    "1. Pick a value for k (the number of clusters to create)\n",
    "2. Initialize k 'centroids' (starting points) in your data\n",
    "3. Create your clusters. Assign each point to the nearest centroid. \n",
    "4. Make your clusters better. Move each centroid to the center of its cluster. \n",
    "5. Repeat steps 3-4 until your centroids converge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "These tutorial images come from Andrew Moore's CS class at Carnegie Mellon. His slide deck is online here: https://www.autonlab.org/tutorials/kmeans11.pdf. He also links to more of his tutorials on the first page. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's practice a toy example by hand. Take a 1-dimensional array of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "arr = [2, 5, 6, 8, 12, 15, 18, 28, 30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Pick k=3 random starting centroids and let the other points fall into clusters based on the closest centroid. Then, reassign your centroids based on the mean value of your cluster and repeat the process. \n",
    "\n",
    "Check with your neighbors. Do you have the same clusters? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### K Means is a powerful algorithm, but different starting points may give you different clusters. You won't necessarily get an optimal cluster.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Metrics for assessing your clusters\n",
    "\n",
    "**Inertia** -- sum of squared errors for each cluster\n",
    "- ranges from 0 to very high values\n",
    "- low inertia = dense clusters\n",
    "\n",
    "**Silhouette Score** -- measure of how far apart clusters are\n",
    "- ranges from -1 to 1\n",
    "- high silhouette Score = clusters are well separated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Inertia** -- sum of squared errors for each cluster\n",
    "- low inertia = dense cluster\n",
    "\n",
    "$$\\sum_{j=0}^{n} (x_j - \\mu_i)^2$$\n",
    "\n",
    "where $\\mu_i$ is a cluster centroid. (K-means explicitly tries to minimize this.)\n",
    "\n",
    "`.inertia_` is an attribute of sklearn's kmeans models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "**Silhouette Score** -- measure of how far apart clusters are\n",
    "- ranges from -1 to 1\n",
    "- high silhouette Score = clusters are well separated\n",
    "\n",
    "The definition is a little involved$^*$, but intuitively the score is based on how much closer data points are to their own clusters than to the nearest neighbor cluster.\n",
    "\n",
    "We can calculate it in sklearn with `metrics.silhouette_score(X_scaled, labels, metric='euclidean')`.\n",
    "\n",
    "$^*$<https://en.wikipedia.org/wiki/Silhouette_(clustering)>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How do I know which K to pick?\n",
    "\n",
    "Sometimes you have good context:\n",
    "   - I need to create 3 profiles for marketing to target\n",
    "\n",
    "Other times you have to figure it out:\n",
    "   - My scatter plots show 2 linearly separable clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn import cluster, datasets, preprocessing, metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Guided practice\n",
    "Let's do some clustering with the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Check out the dataset and our target values\n",
    "df = pd.read_csv(\"./data/iris.csv\")\n",
    "print (df['Name'].value_counts())\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's plot the data to see the distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "cols = df.columns[:-1]\n",
    "sns.pairplot(df[cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Next, since each of our features have different units and ranges, let's do some preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_scaled = preprocessing.MinMaxScaler().fit_transform(df[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(X_scaled, columns=cols).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now that we've formatted our data and understand its structures, we can finally go ahead and cluster.\n",
    "\n",
    "We're going to set k = 2, given the pattern we were seeing above in our graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "k = 3\n",
    "kmeans = cluster.KMeans(n_clusters=k)\n",
    "kmeans.fit(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can use Scikit's built-in functions to determine the locations of the labels, centroids, and cluster inertia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "labels = kmeans.labels_\n",
    "centroids = kmeans.cluster_centers_\n",
    "inertia = kmeans.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And to compute the clusters' silhouette coefficient:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "metrics.silhouette_score(X_scaled, labels, metric='euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "...and we're done! You've completed your first clustering analysis.\n",
    "\n",
    "Let's see how it looks. First, let's put the labels columns into our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df['label'] = labels\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's plot each cluster in a different color. Seaborn has a 'hue' parameter we can use for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "cols = df.columns[:-2]\n",
    "sns.pairplot(df, x_vars=cols, y_vars= cols, hue='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For comparison, here's the data colored by name of the plant. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df, x_vars=cols, y_vars= cols, hue='Name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Independent practice\n",
    "\n",
    "A) Repeat our clustering analysis for the foods nutrients dataset (below). There are no \"true\" labels for this one!\n",
    "\n",
    "B) Then go back up and separate our iris observations into *different* numbers of clusters.\n",
    "\n",
    "- How do the inertia and silhouette scores change?\n",
    "- What if you don't scale your features?\n",
    "- Is there a 'right' k? Why or why not?\n",
    "\n",
    "Repeat this for the foods nutrients dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# http://people.sc.fsu.edu/~jburkardt/datasets/hartigan/file06.txt\n",
    "import pandas as pd\n",
    "foods  = pd.read_csv('./data/nutrients.txt', sep=r'\\s+')\n",
    "foods.head()\n",
    "\n",
    "k = 2\n",
    "X_scaled = preprocessing.MinMaxScaler().fit_transform(foods.drop('Name',axis=1))\n",
    "kmeans = cluster.KMeans(n_clusters=k)\n",
    "kmeans.fit(X_scaled)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "centroids = kmeans.cluster_centers_\n",
    "inertia = kmeans.inertia_\n",
    "foods['label'] = labels\n",
    "foods.head()\n",
    "\n",
    "cols = foods.columns[1:-1]\n",
    "sns.pairplot(foods, x_vars=cols, y_vars= cols, hue='label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Further reading\n",
    "\n",
    "- The [sklearn documentation](http://scikit-learn.org/stable/modules/clustering.html) has a great summary of many other clustering algorithms.\n",
    "- [DBSCAN](https://en.wikipedia.org/wiki/DBSCAN) is one popular alternative.\n",
    "- This [PyData talk](https://www.youtube.com/watch?v=Mf6MqIS2ql4) is good overview of clustering, different algorithms, and how to think about the quality of your clusters."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
