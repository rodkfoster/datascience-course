{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Clustering\n",
    "\n",
    "_Instructor:_ Sinan Uozdemir (San Francisco), Alex Sherman (DC), Steven Longstreet (DC)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"learning-objectives\"></a>\n",
    "### Learning Objectives\n",
    "- Know the difference between supervised and unsupervised learning.\n",
    "- Understand and know how to apply k-means clustering.\n",
    "- Understand and know how to apply density-based clustering (DBSCAN).\n",
    "- Define the Silhouette Coefficient and how it relates to clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Unsupervised Learning](#unsupervised-learning)\n",
    "\t- [Unsupervised Learning Example: Coin Clustering](#unsupervised-learning-example-coin-clustering)\n",
    "\t- [Common Types of Unsupervised Learning](#common-types-of-unsupervised-learning)\n",
    "\t- [Using Multiple Types of Learning Together](#using-multiple-types-of-learning-together)\n",
    "- [Clustering](#clustering)\n",
    "- [K-Means: Centroid Clustering](#k-means-centroid-clustering)\n",
    "\t- [Visual Demo](#visual-demo)\n",
    "\t- [K-Means Assumptions](#assumptions-are-important-k-means-assumes)\n",
    "- [K-Means Demo](#k-means-demo)\n",
    "\t- [K-Means Clustering](#k-means-clustering)\n",
    "\t- [Repeat With Scaled Data](#repeat-with-scaled-data)\n",
    "- [DBSCAN: Density-Based Clustering](#dbscan-density-based-clustering)\n",
    "\t- [Visual Demo](#visual-demo)\n",
    "- [DBSCAN Clustering Demo](#dbscan-clustering-demo)\n",
    "- [Hierarchical Clustering](#hierarchical-clustering)\n",
    "- [Clustering Metrics](#clustering-metrics)\n",
    "- [Clustering, Classification, and Regression](#clustering-classification-and-regression)\n",
    "- [Comparing Clustering Algorithms](#comparing-clustering-algorithms)\n",
    "- [Lesson Summary](#lesson-summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"unsupervised-learning\"></a>\n",
    "## Unsupervised Learning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised learning focuses on finding a relationship between a matrix of features and a response variable. \n",
    "\n",
    "There is typically additional (latent) structure hiding in the feature matrix. For example, some features might be related to each other or even redundant. There also could be groups of observations that seem to be related.\n",
    "\n",
    "Taking advantage of these latent structures allows us to study data without an explicit response in mind and to find better representations for our data to improve predictive performance.\n",
    "\n",
    "**Unsupervised learning** is designed to identify these kinds of structural relationships in our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **The primary goal of unsupervised learning is \"representation.\"** Unsupervised learning extracts structure from data. For example, you could segment grocery-store shoppers into \"clusters\" of shoppers who exhibit similar behaviors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have primarily studied supervised algorithms: Each observation (row of data) comes with one or more labels -- either categorical variables (classes) or measurements (regression).\n",
    "\n",
    "Unsupervised learning has a different goal: feature discovery.\n",
    "\n",
    "> One common and fundamental example of unsupervised learning is **clustering**. Clustering algorithms are used to find meaningful groups within data.\n",
    "\n",
    "Another type of unsupervised learning is **dimensionality reduction**, the most popular type of this (principal components), we will cover in a minitopic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unsupervised learning is clearly differentiated from supervised learning.** With unsupervised learning:\n",
    "\n",
    "- There's no clear objective.\n",
    "- There's no \"right answer\" (which means it's hard to tell how well you're doing).\n",
    "- There's no response variable — only observations with features.\n",
    "- Labeled data is not required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"unsupervised-learning-example-coin-clustering\"></a>\n",
    "### An Example of Unsupervised Learning: Coin Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Observations: Coins\n",
    "- Features: Size and mass\n",
    "- Response: None (no hand-labeling required!)\n",
    "\n",
    "- Perform unsupervised learning:\n",
    "  - Cluster the coins based on “similarity.”\n",
    "  - You’re done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/unsupervised-coin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would you imagine a plot of US coins to look like? Are these coins likely US coins (pennies, nickels, dimes, and quarters)?\n",
    "\n",
    "**Answer:** ---\n",
    "\n",
    "What conclusions could you make about this group of coins?\n",
    "\n",
    "**Answer:** ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"common-types-of-unsupervised-learning\"></a>\n",
    "### Common Types of Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clustering:** Group “similar” data points together.\n",
    "\n",
    "**Dimensionality Reduction:** Reduce the dimensionality of a data set by extracting features that capture most of the variance in the data. (Again, more on this next week)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"clustering\"></a>\n",
    "## Clustering\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to cover three major clustering approaches:\n",
    "\n",
    "- **Centroid clustering using k-means:** Looks for the centers of k pre-specified groups.\n",
    "\n",
    "- **Density-based clustering using DBSCAN:** Looks at gaps, or lack thereof, between datapoints.\n",
    "\n",
    "- **Hierarchical clustering using agglomerative clustering:** Forms groups of groups of groups in a hierarchy to determine clusters.\n",
    "\n",
    "\n",
    "Additional Clustering approaches available in  [scikit-learn](http://scikit-learn.org/stable/modules/clustering.html)\n",
    " - [Great guide comparing performance against HBSCAN with guided applications](https://hdbscan.readthedocs.io/en/latest/comparing_clustering_algorithms.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means Clustering\n",
    "\n",
    "Similarly to k-nearest neighbors, this partitions the entire space into regions (Voronoi partitions). In k-means clustering, k refers to the number of clusters. Also, since this is unsupervised learning, the regions are determined by the k-means algorithm instead of being provided by the training data.\n",
    "\n",
    "**Question:** Why might data often appear in centered clusters?\n",
    "\n",
    "![](./assets/images/clustering-centroids.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Density-Based Clustering\n",
    "\n",
    "In DBSCAN (Density-Based Spatial Clustering of Applications with Noise), clusters are created from areas of high density. This can lead to irregularly shaped regions. Also, many parts of space may not belong to any region.\n",
    "\n",
    "**Question:** Why might data often appear in density-based clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/density-clusters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchical Clustering\n",
    "\n",
    "In hierarchical clustering, clusters are composed by joining two smaller clusters together.\n",
    "\n",
    "Below, we see a tree data structure that stores clusters of points:\n",
    "- Each node represents a cluster of one or more data points.\n",
    "- Each leaf represents a single data point.\n",
    "- The root is the cluster containing all data points.\n",
    "- Each parent combines its children's clusters to create a new (larger) cluster.\n",
    "\n",
    "**Question:** When might hierarchical clustering be useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/hierarchical-clustering.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"k-means-centroid-clustering\"></a>\n",
    "## K-Means: Centroid Clustering\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means clustering is a popular centroid-based clustering algorithm.\n",
    " \n",
    "In k-means clustering, we find $k$ clusters (where $k$ is user-specified), each distributed around a single point (called a **centroid**, an imaginary \"center point\" or the cluster's \"center of mass\").\n",
    "\n",
    "> **K-means seeks to minimize the sum of squares of each point about its cluster centroid.**\n",
    "\n",
    "If we manage to minimize this, then we claim to have found good clusters.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step Two: Error of one cluster\n",
    "\n",
    "We need to measure the \"tightness\" of each cluster -- the closer its points are to the centroid, the better. So, we'll measure how far away each point is from the centroid. Further, we'll square each distance to particularly penalize far away points.\n",
    "\n",
    "So, the sum of the distances of each point $x$ to $\\mu$ is just:\n",
    "\n",
    "$$E_i(S) = {\\sum_{x \\in S} {\\|x - \\mu\\|^2}}$$\n",
    "\n",
    "> This is read: \"The sum of the square distances of each point in S to the centroid of S.\"\n",
    "\n",
    "**Question:** How does this relate to the goal statement?\n",
    "\n",
    "**Answer:** ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step Three: Sum of all cluster errors\n",
    "\n",
    "Now, let's find this sum for each cluster. If we sum these sums together, that is the total error for all $k$ clusters:\n",
    "\n",
    "$$E_{total}(S_1, ..., S_k) = \\sum_{i=1}^k E_i(S_i)$$\n",
    "\n",
    "$$= \\sum_{i=1}^k {\\sum_{x \\in S} {\\|x - \\mu\\|^2}}$$\n",
    "\n",
    "**Question:** How does this relate to the goal statement?\n",
    "\n",
    "**Answer:** ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step Four: Find the clusters that minimize total error\n",
    "\n",
    "Precisely, find $k$ partitions $S_1, …, S_k$ of the data with centroids $\\mu_1, …, \\mu_k$ that minimize $E_{total}$. In other words:\n",
    "\n",
    "$$\\text{argmin}_{S_1, …, S_k} \\sum_{i=1}^k {\\sum_{x \\in S_i} {\\|x - \\mu_i\\|^2}}$$\n",
    "\n",
    "> $\\text{argmin}_{S_1, …, S_k}\\ f(S_1, ..., S_k)$: Find the values of $S_1, ..., S_k$ that minimize $f(S_1, ..., S_k)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means: Under the hood\n",
    "This is a computationally difficult problem to solve, so we often rely on heuristics.\n",
    "\n",
    "The \"standard\" heuristic is called **Lloyd’s Algorithm**:\n",
    "1. Start with $k$ initial (random) points* (we'll call these \"centroids\").\n",
    "2. Assign each datapoint to a cluster by finding its \"closest\" centroid (e.g. using Euclidean distance).\n",
    "3. Calculate new centroids based on the datapoints assigned to each cluster.\n",
    "4. Repeat 2-4 until clusters do not change.\n",
    "\n",
    "\\* There are a number of techniques for choosing initial points. For example, see the `k-means++` technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"visual-demo\"></a>\n",
    "### Visual Demo\n",
    "\n",
    "[Click through](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/) for a demo of k-means clustering in action.\n",
    "\n",
    "![voronoi](assets/voronoi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"assumptions-are-important-k-means-assumes\"></a>\n",
    "### K-Means Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means assumes:\n",
    "\n",
    "- k is the correct number of clusters.\n",
    "- The data is isotropically distributed (circular/spherical distribution).\n",
    "- The variance is the same for each variable.\n",
    "- Clusters are roughly the same size.\n",
    "\n",
    "View these resources to see counterexamples/cases where assumptions are not met:\n",
    "- [Variance Explained](http://varianceexplained.org/r/kmeans-free-lunch/)\n",
    "- [Scikit-Learn](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do we choose k?\n",
    "\n",
    "Finding the correct k to use for k-means clustering is not a simple task.\n",
    "\n",
    "We do not have a ground-truth we can use, so there isn't necessarily a \"correct\" number of clusters. However, we can find metrics that try to quantify the quality of our groupings.\n",
    "\n",
    "Our application is also an important consideration. For example, during customer segmentation we want clusters that are large enough to be targetable by the marketing team. In that case, even if the most natural-looking clusters are small, we may try to group several of them together so that it makes financial sense to target those groups.\n",
    "\n",
    "**Common approaches include:**\n",
    "- Figuring out the correct number of clusters from previous experience.\n",
    "- Using the elbow method to find a number of clusters that no longer seems to improve a clustering metric by a noticeable degree.\n",
    "  - The silhouette coefficient is a commonly used measure but others can be used to approximate the same.\n",
    "  - For an example, check out this [silhouette analysis](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html) documentation on sklearn.\n",
    "  - If we're using clustering to improve performance on a supervised learning problem, then we can use our usual methods to test predictions.\n",
    "  \n",
    "**It's tempting to \"tune\" k as we have in supervised learning:**\n",
    "  - If we are working on a supervised learning problem, then this is possible.\n",
    "  - If we are using clustering to explore our data, then tuning is of little benefit since we do not know precisely what we are looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"k-means-demo\"></a>\n",
    "## K-Means Demo\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beer data set\n",
    "import pandas as pd\n",
    "url = './data/beer.txt'\n",
    "beer = pd.read_csv(url, sep=' ')\n",
    "beer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How would you cluster these beers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X.\n",
    "X = beer.drop('name', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What happened to Y?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"k-means-clustering\"></a>\n",
    "### K-Means Clustering\n",
    "#### K-means with three clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=3, random_state=1)\n",
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review the cluster labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the cluster labels and sort by cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer['cluster'] = km.labels_\n",
    "beer.sort_values('cluster').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer.sort_values('cluster').tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do the clusters seem to be based on? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review the cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the mean of each feature for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer.groupby('cluster').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the `DataFrame` of cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = beer.groupby('cluster').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Allow plots to appear in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a \"colors\" array for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "colors = np.array(['red', 'green', 'blue', 'yellow'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter plot of calories versus alcohol, colored by cluster (0=red, 1=green, 2=blue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(beer.calories, beer.alcohol, c=colors[beer.cluster], s=50);\n",
    "\n",
    "# Cluster centers, marked by \"+\"\n",
    "plt.scatter(centers.calories, centers.alcohol, linewidths=3, marker='+', s=300, c='black');\n",
    "\n",
    "# Add labels.\n",
    "plt.xlabel('calories')\n",
    "plt.ylabel('alcohol')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter plot matrix (0=red, 1=green, 2=blue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(X, c=colors[beer.cluster], figsize=(10,10), s=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"repeat-with-scaled-data\"></a>\n",
    "### Repeat With Scaled Data\n",
    "\n",
    "Unscaled features cause most algorithms to put too much weight onto one feature. We can scale our data to make sure k-means accounts for all features.\n",
    "\n",
    "Remember that k-means is looking for isotropic groups, meaning that they disperse from the center in all directions evenly. \n",
    "\n",
    "There is more than one choice of scaling method (min/max, z-score, log, etc.), but the best choice is the one that makes your clusters isotropic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Center and scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means with three clusters on scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=3, random_state=1)\n",
    "km.fit(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the cluster labels and sort by cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer['cluster'] = km.labels_\n",
    "beer.sort_values('cluster').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the \"characteristics\" of each cluster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review the cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer.groupby('cluster').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter plot matrix of new cluster assignments (0=red, 1=green, 2=blue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(X, c=colors[beer.cluster], figsize=(10,10), s=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the elbow method to k-means\n",
    "\n",
    "The [elbow method](https://en.wikipedia.org/wiki/Elbow_method_(clustering)) is one possible method to help narrow in on the ideal value of **K**. The method is built around the concept of diminishing returns where the percentage of explained variance is reviewed as a function of the number of clusters: One should choose a number of clusters where the next cluster doesn't significantly explain additional variance. \n",
    "\n",
    "[Determining the right number of clusters](https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k means determine k\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Hey look! A new package. Cdist computes distance between each pair of the two collections of inputs.\n",
    "# We're going to use it to compute the distance between each observation and the cluster center\n",
    "\n",
    "distortions = []\n",
    "K = range(1,10)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k)\n",
    "    kmeanModel.fit(X)\n",
    "    distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0])\n",
    "\n",
    "# Plot the elbow\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's analyze the above\n",
    "\n",
    "- Do you see the **\"elbow\"**?\n",
    "\n",
    "- What's the best value of k?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can make this a little easier by using the .interia() method available in KMeans\n",
    "\n",
    "distortions = []\n",
    "for k in range(1, 10):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(X)\n",
    "    distortions.append(kmeans.inertia_)\n",
    "\n",
    "\n",
    "plt.plot(range(1, 10), distortions)\n",
    "plt.grid(True)\n",
    "plt.title('Elbow curve')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dbscan-density-based-clustering\"></a>\n",
    "## DBSCAN: Density-Based Clustering\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/dbscan.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DBSCAN: Density-Based Spatial Clustering of Applications With Noise (1996)**\n",
    "\n",
    "The main idea of DBSCAN is to group together closely packed points by identifying:\n",
    "- Core points\n",
    "- Reachable points\n",
    "- Outliers (not reachable)\n",
    "\n",
    "**Its two parameters are:**\n",
    "- `min_samples`: At least this many points are required inside a neighborhood to form a dense cluster.\n",
    "- `eps`: epsion. This is the radius of a neighborhood.\n",
    "\n",
    "**How does it work?** \n",
    "\n",
    "1. Choose a random unvisited data point.\n",
    "2. Find all points in its neighborhood (i.e. at most `eps` units away). Then:\n",
    "    - **If there are at least `min_samples` points in its neighborhood:** Add all points in the neighborhood to the current cluster. Mark them as unvisited if they have not been visited.\n",
    "    - **Otherwise:** Mark the current point as visited. If the point is not part of a cluster, label the point as noise and go to Step 1.\n",
    "3. If another point in the current cluster is unvisited, choose another point in the cluster and go to Step 2. Otherwise, start a new cluster and go to Step 1.\n",
    "\n",
    "As a class, try running the algorithm on the board."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"visual-demo\"></a>\n",
    "### Visual Demo\n",
    "\n",
    "[Click through](https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/) for a demo of DBSCAN in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DBSCAN advantages**:\n",
    "- Can find arbitrarily shaped clusters.\n",
    "- Don’t have to specify number of clusters.\n",
    "- Excludes outliers automatically.\n",
    "\n",
    "**DBSCAN disadvantages**:\n",
    "- Doesn’t work well when clusters are of varying densities.\n",
    "- Hard to choose parameters that work for all clusters.\n",
    "- Can be hard to choose correct parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does DBSCAN differ from k-means?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dbscan-clustering-demo\"></a>\n",
    "## DBSCAN Clustering Demo\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN with eps=1 and min_samples=3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "db = DBSCAN(eps=1, min_samples=3)\n",
    "db.fit(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review the cluster labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the cluster labels and sort by cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer['cluster'] = db.labels_\n",
    "beer.sort_values('cluster').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review the cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer.groupby('cluster').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter plot matrix of DBSCAN cluster assignments (0=red, 1=green, 2=blue, -1=yellow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(X, c=colors[beer.cluster], figsize=(10,10), s=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizing eps and min_sample\n",
    "\n",
    "Finding the right paramaters with DBScan is dependent on understanding what you **WANT** to find. It's best solved for with domain knowledge and **NOT** mathematics. There is no general way of choosing min_samples but there are some points to take into consideration.\n",
    "- A low min_sample means it will build more clusters from noise so a small value \n",
    "- A large value could exclude the formation of ANY clusters\n",
    "\n",
    "For epsilon, there are various aspects. The recommended approach is to first select a min_sample based on domain knowledge. Approaches might be\n",
    "   - You can try to do a [knn distance histogram](http://www.jatit.org/volumes/Vol92No2/21Vol92No2.pdf) and choose a \"knee\" there, but there might be no visible one, or multiple.\n",
    "   - Plot a k-distance graph (with k=min_samples) and look for an elbow in this graph. \n",
    "   - Apply domain knowledge to choose epsilon. For instance with geo-spacial data you might know that 10m is a suitable radius. You can then perform a density plot for this radius and look for an elbow there.\n",
    "   - Utlize a package like [**HDBSCAN**](https://hdbscan.readthedocs.io/en/latest/index.html) or [**Optics**](https://en.wikipedia.org/wiki/OPTICS_algorithm) which are DBSCAN variation without an epsilon parameter. The packages take seperate ways of delivering a hierarchical output that can roughly be seen as \"running DBSCAN with every possible epsilon\".\n",
    "\n",
    "\n",
    "\n",
    "[Good read on suggested approaches to optimizing DBSCAN (eps and min_sample)](http://www.sersc.org/journals/IJSIP/vol6_no1/9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"hierarchical-clustering\"></a>\n",
    "## Hierarchical Clustering\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical clustering, like k-means clustering, is another common form of clustering analysis. With this type of clustering we seek to do exactly what the name suggests:\n",
    "\n",
    "- Build hierarchies of clusters.\n",
    "- Connect the clusters in the hierarchy with links.\n",
    "\n",
    "Once the links are determined, we can display them in what is called a dendrogram — a graph that displays all of these links in their hierarchical structure.\n",
    "\n",
    "**As we described earlier:**\n",
    "\n",
    "- Each node represents a cluster of one or more data points.\n",
    "- Each leaf represents a single data point.\n",
    "- The root is the cluster containing all data points.\n",
    "- Each parent combines its children's clusters to create a new (larger) cluster.\n",
    "\n",
    "**A simple algorithm we could use to generate this tree:**\n",
    "\n",
    "1. Create a cluster for each point, containing only that point. (Create all leaf nodes.)\n",
    "2. Choose the two clusters with centroids closest to each other.\n",
    "    - Combine the two clusters into a new cluster that replaces the two individual clusters. (Create a new parent node.)\n",
    "3. Repeat Step 2 until only one cluster remains.\n",
    "\n",
    "Essentially we form groups of groups of groups.\n",
    "\n",
    "![](./assets/hierarchical-clustering.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding Hierarchical Clusters\n",
    "\n",
    "\n",
    "THe goal of agglomerative clustering is to groups similar objects into groups called clusters. We group them based on distance which tends to be euclidean but [other distances](https://en.wikipedia.org/wiki/Distance) can/should be used depending on the problem. The output of the process is a set of clusters, where each cluster is distinct from each other cluster while the objects within each cluster are broadly similar to each other.\n",
    "\n",
    "Let's look at it from a high level - it's a two step process\n",
    "\n",
    "![](./assets/Hierarchical-clustering-4.png)\n",
    "\n",
    "We can break this down visually as we go through the process on an example set of data\n",
    "\n",
    "![](./assets/Hierarchical-clustering-2.png)\n",
    "\n",
    "\n",
    "The output of clustering is a **dendrogram** depicting the hierarchical relationship between the clusters:\n",
    "\n",
    "![](./assets/Hierarchical-clustering-3.png)\n",
    "\n",
    "#### **Knowledge check**\n",
    "\n",
    ">1. What letters are most similar to **F**?\n",
    ">2. Which letters are the least similar to **F**?\n",
    "\n",
    "\n",
    "Ok... so what does agglomerative clustering mean?\n",
    "\n",
    "In agglomerative hierarchical clustering the alogorithm works by sequentially merging similar clusters as we saw above. Inversely, Divisive clustering works the other way by starting with one grouping then successively splitting these clusters. In practice - Divisive clustering is rarely used. \n",
    "\n",
    "**Any thoughts on why?**\n",
    "\n",
    "\n",
    "\n",
    "[source](https://www.displayr.com/what-is-hierarchical-clustering/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "agg = AgglomerativeClustering(n_clusters=4)\n",
    "agg.fit(X_scaled)\n",
    "labels = agg.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cluster labels and sort by cluster.\n",
    "beer['cluster'] = agg.labels_\n",
    "beer.groupby('cluster').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot matrix of DBSCAN cluster assignments (0=red, 1=green, 2=blue, -1=yellow)\n",
    "pd.scatter_matrix(X, c=colors[beer.cluster], figsize=(10,10), s=100);\n",
    "\n",
    "# Wait... why is this giving me an error?\n",
    "# As a package gets more and more resources, sometimes its best to reorganize which happened here\n",
    "# How can we address this error?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring errors\n",
    "\n",
    "By this point in class you'll recognize we can run into errors... alot. As we build our own code sometimes things don't work as intended. Othertimes we copy code that worked at one time (e.g. from an online source) or our old code isn't working against a new version. Either way the process remains the same.\n",
    "\n",
    "1.) Read the error message. Sometimes a fix is available directly from the message or the error lets you make the right correction\n",
    "\n",
    "2.) Google the error. Challenges, particularly at a beginner level, rarely happen in a vaccuum. \n",
    "\n",
    "3.) Understand the correction. Don't simply copy and paste in a fix - it may not do exactly what you thought it would do.\n",
    "\n",
    "4.) Ask a question - instructors, peers and online forums (stackoverflow, reddit.com/r/learningpython , /r/datascience , /r/python etc)\n",
    "\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html#visualization-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updated code\n",
    "pd.plotting.scatter_matrix(X, c=colors[beer.cluster], figsize=(10,10), s=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"clustering-metrics\"></a>\n",
    "## Clustering Metrics\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding cluster evaluation\n",
    "\n",
    "Evaluating a cluster, and many other unsupervised models, is not as trivial as calling a `.score ` method like a Supervisoed model. Unsupervised models are often evaluated by the how the model gets used or consumed by downstream methods - typically other models. Essentially how well a model performs will largely depend on why one is doing unsupervised learning in the first place. What was the context of your end goal for using clustering?\n",
    "\n",
    "- Did the cluster improve accuracy in classification model? \n",
    "    - Improve your true positive rate? \n",
    "    - Improve recall?\n",
    "    - etc...\n",
    "    \n",
    "    \n",
    "- Did the cluster reduce error in a regression model?\n",
    "\n",
    "Sklearn gives you many options for [Clustering performance evaluation](http://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation). Before starting into a new application area of machine learning with clustering (i.e. pattern mining, anomaly detection, image recognition, natural language processing) - review the list and research the best metric for your mode. Today - we're going to start with some initial metrics on evaluating your clustering across two areas:\n",
    "\n",
    ">1. How \"tight\" or \"dense\" your clusters with the metric **Inertia**\n",
    ">2. The distance between clusters with **Silhouette Score**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Metrics\n",
    "**Inertia** -- sum of squared errors for each cluster\n",
    "- ranges from 0 to very high values\n",
    "- low inertia = dense clusters\n",
    "\n",
    "**Silhouette Score** -- measure of how far apart clusters are\n",
    "- ranges from -1 to 1\n",
    "- high silhouette Score = clusters are well separated\n",
    "    - Calculating across the entire feature space uses the sklearn.metrics method silhouette_score\n",
    "    - Calculating for individual samples uses the sklearn.metrics method silhouette_sample\n",
    "        - values close to 1 implies the datum is in an appropriate cluster, while a silhouette close to −1 implies the datum is in the wrong\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inertia  -- sum of squared errors for each cluster\n",
    "- low inertia = dense cluster\n",
    "\n",
    "$$\\sum_{j=0}^{n} (x_j - \\mu_i)^2$$\n",
    "\n",
    "where $\\mu_i$ is a cluster centroid. (K-means explicitly tries to minimize this.)\n",
    "\n",
    "`.inertia_` is an attribute of sklearn's kmeans models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Silhouette Score** -- measure of how far apart clusters are\n",
    "- ranges from -1 to 1\n",
    "- high silhouette Score = clusters are well separated\n",
    "\n",
    "The definition is a little involved$^*$, but intuitively the score is based on how much closer data points are to their own clusters than to the nearest neighbor cluster.\n",
    "\n",
    "We can calculate it in sklearn with `metrics.silhouette_score(X_scaled, labels, metric='euclidean')`.\n",
    "\n",
    "$^*$<https://en.wikipedia.org/wiki/Silhouette_(clustering)>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how the Silhouette Coefficient is measured. Keep in mind how this math definition compares to our high-level idea of a sample's cohesion vs. separation:\n",
    "\n",
    "$$\\frac {b - a} {max(a,b)}$$\n",
    "\n",
    "- $a$ is the mean distance between a sample and all other points in the cluster.\n",
    "\n",
    "- $b$ is the mean distance between a sample and all other points in the nearest cluster.\n",
    "\n",
    "The coefficient ranges between 1 and -1. The larger the coefficient, the better the clustering.\n",
    "\n",
    "To get a score for all clusters rather than for a particular point, we average over all points to judge the cluster algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.silhouette_score(X, labels, metric='euclidean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"clustering-classification-and-regression\"></a>\n",
    "## Clustering, Classification, and Regression\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use clustering to discover new features, then use those features for either classification or regression.\n",
    "\n",
    "For classification, we could use clusters directly to classify new points.\n",
    "\n",
    "For regression, we could use a dummy variable for the clusters as a variable in our regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a function to plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_colors(labels, colors='rgbykcm'):\n",
    "    colored_labels = []\n",
    "    for label in labels:\n",
    "        colored_labels.append(colors[label])\n",
    "    return colored_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create some synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "data = []\n",
    "dist = multivariate_normal(mean=[0, 0], cov=[[0.5, 0.5],[0,0.1]])\n",
    "for i in range(150):\n",
    "    p = list(dist.rvs())\n",
    "    data.append(dist.rvs())\n",
    "dist = multivariate_normal(mean=[1, 5], cov=[[0.5, 0.5],[0,0.1]])\n",
    "for i in range(150):\n",
    "    data.append(dist.rvs())\n",
    "dist = multivariate_normal(mean=[2, 10], cov=[[0.5, 0.5],[0,0.1]])\n",
    "for i in range(150):\n",
    "    data.append(dist.rvs())\n",
    "\n",
    "    \n",
    "df = pd.DataFrame(data, columns=[\"x\", \"y\"])\n",
    "df.head()\n",
    "plt.scatter(df['x'], df['y'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a DBSCAN estimator.\n",
    "estimator = DBSCAN(eps=0.8, min_samples=10)\n",
    "X = df[[\"x\", \"y\"]]\n",
    "estimator.fit(X)\n",
    "# Clusters are given in the labels_ attribute.\n",
    "labels = estimator.labels_\n",
    "\n",
    "colors = set_colors(labels)\n",
    "plt.scatter(df['x'], df['y'], c=colors)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add cluster labels back to the `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that -1 clusters are outliers.\n",
    "df[\"cluster\"] = labels\n",
    "df = pd.concat([df, pd.get_dummies(df['cluster'], prefix=\"cluster\", drop_first=True)], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit a linear model with clusters included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "X = df[[\"x\", \"cluster\", \"cluster_1\", \"cluster_2\"]]\n",
    "y = df['y']\n",
    "model.fit(X, y)\n",
    "\n",
    "print((model.score(X, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = set_colors(labels)\n",
    "plt.scatter(df['x'], df['y'], c=colors)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "\n",
    "plt.scatter(df[\"x\"], model.predict(X), color='black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What happens if we don't include the clusters we estimated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "X = df[[\"x\"]]\n",
    "y = df['y']\n",
    "model.fit(X, y)\n",
    "print((model.score(X, y)))\n",
    "\n",
    "colors = set_colors(labels)\n",
    "plt.scatter(df['x'], df['y'], c=colors)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "\n",
    "plt.scatter(df[\"x\"], model.predict(X), color='black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"comparing-clustering-algorithms\"></a>\n",
    "## Comparing Clustering Algorithms\n",
    "\n",
    "- K-means\n",
    "  - Finds cluster centers.\n",
    "  - Must choose the number of clusters.\n",
    "  - Assumes clusters are isotropic.\n",
    "- DBSCAN\n",
    "  - Inspects local density to find clusters.\n",
    "  - Better than k-means for anisotropic clusters.\n",
    "  - Capable of finding outliers.\n",
    "- Hierarchical clustering\n",
    "  - Finds clusters by forming groups of groups of groups of points.\n",
    "  - Hierarchical clustering works well for non-spherical clusters.\n",
    "  - May be computationally expensive.\n",
    "  - Guaranteed to converge to the same solution (no random initialization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding pros and cons of each model\n",
    "\n",
    "#### K-means\n",
    "- **Pros**:\n",
    "    - Easy to implement\n",
    "    - If variables are huge, then  K-Means most of the times computationally faster than hierarchical clustering, if we keep k smalls.\n",
    "    - K-Means produce tighter clusters than hierarchical clustering, especially if the clusters are spherical\n",
    "    - K-Means may produce higher clusters than hierarchical clustering\n",
    "- **Cons**:\n",
    "    - Difficult to predict K-Value.\n",
    "    - The\torder\tof\tthe\tdata\thas\tan\timpact\ton\tthe\tfinal\tresults\t\n",
    "    - It does not work well with clusters of differing size and density\n",
    "    - Initial\tseeds\thave\ta\tstrong\timpact\ton\tthe\tfinal\tresults\t(i.e. An assigned cluster of an instance can\tchange\tcluster\t(move\tto\tanother\tcluster) when the centroids are\trecomputed)\n",
    "    - Sensitive\tto\tscale:\trescaling\tyour\tdatasets\t(normalizaHon\tor\tstandardizaHon)\twill completely\tchange\tresults.\tWhile\tthis\titself\tis\tnot\tbad,\tnot\trealizing\tthat\tyou\thave\tto\tspend\textra\ta4en(on\tto\tscaling\tyour\tdata\tmight\tbe\tbad.\t\t\n",
    "\n",
    "#### Hierarchical Clustering\n",
    "- **Pros**:\n",
    "    - Hierarchical\tclustering\toutputs\ta\thierarchy,\tie\ta\tstructure\tthat\tis\tmore\tinformaHve\tthan the\tunstructured\tset\tof\tflat\tclusters\treturned\tby\tk-means.\tTherefore,\tit\tis\teasier\tto\tdecide\t on\tthe\tnumber\tof\tclusters\tby\tlooking\tat\tthe\tdendrogram\n",
    "    - Easy\tto\timplement\t\n",
    "\n",
    "- **Cons**:\n",
    "    - It\tis\tnot\tpossible\tto\tundo\tthe\tprevious\tstep:\tonce\tthe\tinstances\thave\tbeen\tassigned\tto\ta cluster,\tthey\tcan\tno\tlonger\tbe\tmoved\taround.\t\t\n",
    "    - Time\tcomplexity:\tnot\tsuitable\tfor\tlarge\tdatasets\t\n",
    "    - Initial\tseeds\thave\ta\tstrong\timpact\ton\tthe\tfinal\tresults\t\n",
    "    - The\torder\tof\tthe\tdata\thas\tan\timpact\ton\tthe\tfinal\tresults\t\n",
    "    - Very\tsensitive\tto\toutliers\n",
    "    \n",
    "#### DBSCAN\n",
    "- **Pros**:\n",
    "    - Can find arbitrarily-shaped clusters\n",
    "    - Can handle clusters of different shapes and sizes\n",
    "    - Resistant to Noise\n",
    "    - Don’t have to specify number of clusters\n",
    "    - Robust to outliers\n",
    "- **Cons**:\n",
    "    - Doesn’t work well when clusters are of varying densities\n",
    "    - hard to chose parameters that work for all clusters\n",
    "    - Sensitive to parameter settings – Hard to determine the correct set of parameters\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lesson-summary\"></a>\n",
    "## Lesson Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Supervised learning vs. unsupervised learning\n",
    "    - The main difference between the two is whether we use response labels.\n",
    "- K-means, DBSCAN, and hierarchical clustering\n",
    "  - Can you summarize how each algorithm roughly works?\n",
    "- The Silhouette Coefficient\n",
    "  - What does the silhouette coefficient measure?\n",
    "- Using clustering along with supervised learning\n",
    "  - Why would we expect predictive power to improve when we include clusters?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
