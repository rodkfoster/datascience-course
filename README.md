# Data Science Course

> Welcome to Data Science!

1. [Master Schedule](#master)
2. [Course Overview](#course)
3. [Your Team](#team)
4. [Projects](#projects)
5. [Tech Requirements](#tech)
6. [Student Expectations](#expectations)
7. [Python Practice Resources](#practice)
8. [Additional Resources](#resources)


## Helpful Links

<a id='master'></a>
Support Docs: 

For curated supplemental readings, see our [extra resources ](#resources) for both a self-explore option and targeted readings for each class.

## Unit 1: Fundamentals

| Class Topic
| ---
| [Welcome to Data Science][1-W1]<br>**ISL**: [Ch. 2.1](http://faculty.marshall.usc.edu/gareth-james/ISL/)
| [Your Development Environment][1-W2]
| [Python Foundations][1-W3]
| [Unit 1 Recap & Project Day][PYTHON-PROBLEMS]<br>**Unit 1 Quiz**

## Unit 2: Working with Data

| Class Topic
| ---
| [Exploratory Data Analysis in Pandas][2-W1]<br>**Milestone:** [Unit 1 Project DUE - 11:59pm][P1]
|  No Class - Memorial Day break
| [Data Visualization in Python][2-W3] 
| [Statistics in Python][2-W4]<br> 
| [Experiments & Hypothesis Testing][2-W2]<br>**Milestone:** [Unit 2 Project DUE][P2]**Milestone:** [Final Project: Project Proposal DUE][P4] 


## Unit 3: Data Science Modeling

| Class Topic
| ---
| [Linear Regression][3-W1]<br>[**ISL**: Ch. 3.1-3, 3.5, 6.1, 6.2](http://faculty.marshall.usc.edu/gareth-james/ISL/) <br>**Unit 2 Quiz**<br>
| [Logistic Regression][3-W4]<br>[**ISL**: Ch. 4.1-3 ](http://faculty.marshall.usc.edu/gareth-james/ISL/)
| [Train-Test Split & Bias-Variance][3-W2]<br>[**ISL**: Ch. 2.2 ](http://faculty.marshall.usc.edu/gareth-james/ISL/)
| [KNN / Classification][3-W3]
| [Clustering][4-W1]
| [Decision Trees & Random Forests][TREES-FLEX]
| [PCA & Anomaly Detection][4-W2]<br>**Milestone:** [Final Project: Initial EDA DUE][P4]<br>**Milestone:** [Unit 3 Project DUE][P3]


## Unit 4: Data Science Applications
| Class Topic
| ---
| [Intro to Natural Language Processing][4-W3]<br>**Unit 3 Quiz**<br>**Milestone:** Final submission date for EDA and Project
| [Intro to Time Series][4-W4]
| [Intro to Neural Networks][4-W5]
| FLEX Lesson <br>**Unit 4 Quiz**<br>**Milestone:**[Final Project: Sign up for weekly checkup time!][P4]

## Capstone Project
| Class Topic
| ---
| Capstone Preparation (Office Hours only)
| Capstone Preparation (Office Hours only)
| Capstone Preparation (Office Hours only)
| [Final Project Presentations][P4]<br>**Milestone:** [Final Project Presentation DUE][P4]


**ISL**: James, Gareth et al. "An Introduction to Statistical Learning." [[PDF](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf)]

[1-W1]: ./lessons/welcome-to-data-science/
[1-W2]: ./lessons/your-development-environment
[1-W3]: ./lessons/python-foundations

[2-W1]: ./lessons/eda-with-pandas
[2-W2]: ./lessons/experiments-hypothesis-tests
[2-W3]: ./lessons/data-visualization-in-python
[2-W4]: ./lessons/statistics-in-python

[3-W1]: ./lessons/linear-regression
[3-W2]: ./lessons/train-test-split-and-bias-variance
[3-W3]: ./lessons/knn-classification
[3-W4]: ./lessons/logistic-regression

[4-W1]: ./lessons/clustering
[4-W2]: ./lessons/flex_pca
[4-W3]: ./lessons/natural-language-processing
[4-W4]: ./lessons/flex_time-series
[4-W5]: ./lessons/flex_neural-networks

[RECSYS-FLEX]: ./lessons/flex_recommendation-systems
[NLP-FLEX]: ./lessons/natural-language-processing
[TREES-FLEX]: ./lessons/flex_decision-trees
[ETHICS-FLEX]: ./lessons/flex_ethics

[RESOURCES]: ./lessons/resources
[PYTHON-PROBLEMS]: ./lessons/python-problems

---
<a id='team'></a>
<a id='course'></a>
## Course Overview
Welcome to our Data Science Fundamentals course! We're building a global community of lifelong learners who are excited about using data to solve real business problems. 

In this program, we will learn to use Python programming to explore datasets, build regression models, and communicate data driven insights. Specifically, you will learn how to:

- Define common approaches and considerations that data scientists use to solve real world problems.
- Perform exploratory data analysis with powerful programmatic tools in Python.
- Build and refine basic regression and time series models to predict patterns from data sets.
- Communicate data driven insights to peers and stakeholders in order to inform business decisions.


### What You Will Learn

**Statistical Analysis with Python**
 - Perform visual and statistical analysis on data using Python and its associated libraries and tools.
 
 
**Data-Driven Decision-Making**
 - Define and determine the trade-offs involving feature selection, model accuracy, and data quality.
 
 
**Data Science Modeling Techniques**
 - Explore supervised learning techniques, focusing primarily on linear and logistic regression.
 
 
**Visualizations & Presentations** 
 - Create visualizations and interactive notebooks to present to business stakeholders.


---

<a id='projects'></a>
## Project Structure

This course will ask you to complete a series of projects in order to practice and apply the skills covered in-class.

### Unit Projects
At the end of each unit, you'll work on short structured projects. These activities will test your understanding of each unit’s most important concepts with in-class practice and instructor support. 

For those of you who want to go above and beyond, we’ve also included stretch options, bonus activities, and other opportunities for further reading and practice.

### Final Project

You'll also complete a [final project](), asking you to apply your skills to a business problem of your choice.

The capstone is an opportunity for you to demonstrate your new skills and tackle a pressing issue relevant to your team, division, or organization. You’ll create a hypothesis, analyze internal data, and generate a working model, prototype, solution, or recommendation.

You will get structured guidance and designated time to work throughout the course. Final project deliverables include:

- **Proposal**: Describe your chosen problem and identify relevant data (while confirming you have access).
- **Brief**: Share a summary of your initial analysis and next steps in order to get assistance from your instructional team.
- **Report**: Submit a cleanly formatted Jupyter notebook (or other files) documenting your code and process for technical/peer stakeholders.
- **Presentation**: Present a summary of your business problem, approach, and recommendation to an audience of non-technical executive stakeholders.

---

### Project Breakdown

1. [Unit Project 1: Python Coding][P1]
2. [Unit Project 2: Exploratory Data Analysis][P2]
3. [Unit Project 3: Modeling][P3]
4. [Final Project: Solve a Business Problem][P5]
    - Part 1: Proposal & Dataset
    - Part 2: Initial EDA
    - Part 3: Solution Prototype
    - Part 4: Presentation

[P1]: ./projects/unit-1_project
[P2]: ./projects/unit-2_project
[P3]: ./projects/unit-3_project
[P4]: ./projects/unit-4_project
[P5]: ./projects/final_project

---

<a id='tech'></a>
## Technology Requirements

See the [data science installation guide](./ds-installation-guide.pdf).

Please note: the curriculum materials for this course are written in Python 3.6.

---

<a id='expectations'></a>
## Expectations

1. Participate in classroom discussions
2. Zoom video on during class
3. Be respectful of others
4. *Ask questions* - it's the essenes of being a good data scientist (and student)
5. Dedicate *2-3 hours* following every class to practice, complete unit projects and work on your capstone
6. Grow first by asking yourself "have I done all I can to answer this question - am I truly stuck?" before asking others. And when you do ask others, have them shepard you to your own conclusion not just give you the answer. This will help the teaching student and you in the long run
7. Be humble - no one here is an expert at everything

---

## Road to Success
The emotional cycle of change: This course is fast and covers a lot of material. There will be times when you may feel discouraged or overwhelmed, but don't give up - this is natural (and part of the design). By the end of the course, you'll feel more confident in your ability to define problems, analyze data, and prototype solutions.
Student learning responsibility: Our lessons cover topic foundations, but there is always more to learn! You are responsible for your learning experience - but don't get overwhelmed! Instead, just make sure you follow along, practice as much as possible, and ask questions.
GA requirements: Show up. Be on time. Participate. Submit your projects. Allow yourself to struggle. Read the docs. Have fun!
Q/A.

---

<a id='practice'></a>
## Python Practice Resources

If you have been enjoying the practice problems at the beginning of some classes, here are some great resources to find a library of more: 

-  https://www.hackerrank.com/
-  https://www.codewars.com/
-  https://www.coderbyte.com/
-  https://www.codefights.com/

<a id='resources'></a>

Many GA instructors collaborate to put together a [master resource site](./resources). If you have trouble navigating it - let one of the Instructional Team know!


From time to time we'll add additional resources based on your wants/needs. To get us started I included a few for the first 3 weeks.

### Class 1: What is Data Science

**Python Resources:**
* [Codecademy's Python course](http://www.codecademy.com/en/tracks/python): Good beginner material, including tons of in-browser exercises.
* [Top Python Libraries Used In Data Science](https://towardsdatascience.com/top-python-libraries-used-in-data-science-a58e90f1b4ba): Important and useful python libraries that can be used in data science
* [DataQuest](https://dataquest.io/): Similar interface to Codecademy, but focused on teaching Python in the context of data science.
* [Google's Python Class](https://developers.google.com/edu/python/): Slightly more advanced, including hours of useful lecture videos and downloadable exercises (with solutions).
* [A Crash Course in Python for Scientists](http://nbviewer.ipython.org/gist/rpmuller/5920182): Read through the Overview section for a quick introduction to Python.
* [Python for Informatics](http://www.pythonlearn.com/book.php): A very beginner-oriented book, with associated [slides](https://drive.google.com/folderview?id=0B7X1ycQalUnyal9yeUx3VW81VDg&usp=sharing) and [videos](https://www.youtube.com/playlist?list=PLlRFEj9H3Oj4JXIwMwN1_ss1Tk8wZShEJ).
* [Python Tutor](http://pythontutor.com/): Allows you to visualize the execution of Python code.
* [My code isn't working](http://www.tecoed.co.uk/uploads/1/4/2/4/14249012/624506_orig.png) is a great flowchart explaining how to debug Python errors.
* [PEP 8](https://www.python.org/dev/peps/pep-0008/) is Python's "classic" style guide, and is worth a read if you want to write readable code that is consistent with the rest of the Python community.

**Advanced Python Material:**
* [Want to understand Python's comprehensions? Think in Excel or SQL](http://blog.lerner.co.il/want-to-understand-pythons-comprehensions-think-like-an-accountant/) may be helpful if you are still confused by list comprehensions.
* If you want to understand Python at a deeper level: Ned Batchelder's [Loop Like A Native](http://nedbatchelder.com/text/iter.html), [Python Names and Values](http://nedbatchelder.com/text/names1.html), 
Raymond Hettinger's [Transforming Code into Beautiful, Idiomatic Python](https://www.youtube.com/watch?v=OSGv2VnC0go) and [Python Epiphanies](https://www.youtube.com/watch?v=Pi9NpxAvYSs) are excellent presentations.
* [Everything is an object in Python](https://www.jeffknupp.com/blog/2013/02/14/drastically-improve-your-python-understanding-pythons-execution-model/)

**Resources:**
* For a useful look at the different types of data scientists, read [Analyzing the Analyzers](http://cdn.oreillystatic.com/oreilly/radarreport/0636920029014/Analyzing_the_Analyzers.pdf) (32 pages).
* For some thoughts on what it's like to be a data scientist, read these short posts from [Win-Vector](http://www.win-vector.com/blog/2012/09/on-being-a-data-scientist/) and [Datascope Analytics](http://datascopeanalytics.com/what-we-think/2014/07/31/six-qualities-of-a-great-data-scientist).
* [Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/) 
* [Data Science vs Statistics](http://bit.ly/1FrZX80)
* [15 Books every Data Scientist Should Read](http://bit.ly/1Fs0bvW)
* [50+ Free Data Science Books](http://bit.ly/1Fs0kzr)
* [Building Data Science Teams](http://oreil.ly/1G1s6Oc)
* [Doing Data Science](http://amzn.to/1MHM1Jz)
* [Getting Started with Data Science](http://treycausey.com/getting_started.html)
* Quora has a [data science topic FAQ](https://www.quora.com/Data-Science) with lots of interesting Q&A.
* Keep up with local data-related events through the Data Community DC [event calendar](http://www.datacommunitydc.org/calendar) or [weekly newsletter](http://www.datacommunitydc.org/newsletter).
* [Stack Overflow - Developer Survey Results 2017](http://stackoverflow.com/insights/survey/2017/?utm_source=so-owned&utm_medium=hero&utm_campaign=dev-survey-2017&utm_content=hero-ind-ques)
* [Nate Silver on the Art and Science of Prediction](https://www.youtube.com/watch?v=eE4qCJBgfIk&index=22&list=LLGUFXF_Wex-mp-gpXFPYZEQ)
* Data science business [application diagram](https://cdn-images-1.medium.com/max/1750/1*_r8gzWK6v7fbU1YqCfZ7WQ.png)
* [Three waves of AI](https://www.youtube.com/watch?v=-O01G3tSYpU)

**Material for Next Class:**
* [Setting up Python for machine learning: scikit-learn and IPython Notebook](https://youtu.be/IsXXlYVBt1M?t=4m57s) This videos includes an overview of Jupyter Notebook, which is used in the homework assignment.
* [Pro Git](http://git-scm.com/book/en/v2) is an excellent book for learning Git. Read the first two chapters to gain a deeper understanding of version control and basic commands.
* Work through GA's friendly [command line tutorial](http://generalassembly.github.io/prework/command-line/#/) using Terminal (Linux/Mac) or Git Bash (Windows), and then browse through this [command line reference](https://git.generalassemb.ly/wave10-remote-mw/your-development-environment/blob/master/Command_line.md).

---
### Class 2: Git, Github, and the Command Line

**Class Resources:**
Set your [Git username and email](https://help.github.com/articles/setting-your-username-in-git/)

**Command Line Resources:**
* Good source for cheat sheets on various topics like git and command line [Master Cheat Sheet site](https://www.cheatography.com/)
* Work through GA's friendly [command line tutorial](http://generalassembly.github.io/prework/command-line/#/) using Terminal (Linux/Mac) or Git Bash (Windows), and then browse through this [command line reference](https://git.generalassemb.ly/wave10-remote-mw/your-development-environment/blob/master/Command_line.md).
* [The Linux command line](http://courseweb.pitt.edu/bbcswebdav/institution/Pitt%20Online/MLIS_Pitt_Online/LIS%202600/Intro%20Module/LIS_2600_%20M1_Shotts%202009.pdf)
* If you want to go much deeper into the command line, [Data Science at the Command Line](http://amzn.to/1gSjcvV) is a great book. 
The [companion website](http://datascienceatthecommandline.com/) provides installation instructions for a "data science toolbox" 
(a virtual machine with many more command line tools), as well as a long reference guide to popular command line tools.
* If you want to do more at the command line with CSV files, try out [csvkit](http://csvkit.readthedocs.org/), which can be installed via 'pip'.

**Git and Markdown Resources:**
* [My favorite Git cheat sheet](http://ndpsoftware.com/git-cheatsheet.html#loc=remote_repo;)
* [Pro Git](http://git-scm.com/book/en/v2) is an excellent book for learning Git. Read the first two chapters to gain a deeper understanding of version control and basic commands.
* [GitHub for Beginners](http://readwrite.com/2013/09/30/understanding-github-a-journey-for-beginners-part-1)
* If you want to practice a lot of Git (and learn many more commands), [Git Immersion](http://gitimmersion.com/) looks promising.
* If you want to understand how to contribute on GitHub, you first have to understand [forks and pull requests](http://www.dataschool.io/simple-guide-to-forks-in-github-and-git/).
* [GitRef](http://gitref.org/) is my favorite reference guide for Git commands, and [Git quick reference for beginners](http://www.dataschool.io/git-quick-reference-for-beginners/) is a shorter guide with commands grouped by workflow.
* [Markdown Cheatsheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet) provides a thorough set of Markdown examples with concise explanations. GitHub's [Mastering Markdown](https://guides.github.com/features/mastering-markdown/) is a simpler and more attractive guide, but is less comprehensive.
* [Introducing GitHub](http://amzn.to/1KCjWg6) is a nice intro to GitHub that reads quickly
* [Version Control with Git](http://amzn.to/1gSkBm2)
* [Cracking the Code to GitHub's Growth](https://growthhackers.com/growth-studies/github) explains why GitHub is so popular among developers.
* [How to remove .DS_Store from GitHub](https://gist.github.com/vybstat/1680bef4715bfbcb0268)
* The simple [Git guide](http://rogerdudler.github.io/git-guide/)

---
### Class 3: Python Foundations

For more information on this topic, check out the following resources:

- [Python Code Academy](https://www.codecademy.com/learn/python)
- [Learn Python the Hard Way](https://learnpythonthehardway.org)
- [Python Data Types and Variables](http://www.python-course.eu/variables.php)
- [Python: IF, ELIF, ELSE](https://www.tutorialspoint.com/python/python_if_else.htm)
- [Python Loops](https://www.tutorialspoint.com/python/python_loops.htm)
- [Python Control Flow](https://python.swaroopch.com/control_flow.html)
- [Function Practice](https://www.w3resource.com/python-exercises/python-functions-exercises.php)
- [Practice Problems with solutions](http://codingbat.com/python)
- [Understanding functions vs methods](https://www.datacamp.com/community/tutorials/functions-python-tutorial#function)
- [Intro to Loops](https://www.learnpython.org/en/Loops)
- [44 practice exercises for loops](https://www.w3resource.com/python-exercises/python-conditional-statements-and-loop-exercises.php)
- [Python list comprehensions: Explained Visually](https://www.analyticsvidhya.com/blog/2016/01/python-tutorial-list-comprehension-examples/)
- [Merging List comprehension explanation with exercises](https://www.analyticsvidhya.com/blog/2016/01/python-tutorial-list-comprehension-examples)
- [List Comprehension Exercises](http://www.learnpython.org/en/List_Comprehensions)

## Class Challenge Exercises

> Some extra exercises to practice on!

### Starter Exercises

1.	Capture all of the numbers from 1-1000 that are divisible by 7
2.	Capture all of the numbers from 1-1000 that have a 3 in them
3.	Provide the count of number of spaces in a string
4.	Remove any vowels in a string
5.	Capture all of the words in a string less than 4 letters

### Challenge Exercises
1.	Use a dictionary comprehension to count the length of each word in a sentence.
2.	Use a nested list comprehension to find all of the numbers from 1-500 that are divisible by any single digit except 1 
3.	Use a nested list/dictionary comprehension to find the highest single digit any number in the range 1-1000 is divisible by

---
### Class 4 Exploratory data analysis with Pandas


**Class Resources:**
* MovieLens 100k movie ratings ([data dictionary](http://files.grouplens.org/datasets/movielens/ml-100k-README.txt), [website](http://grouplens.org/datasets/movielens/))
* Alcohol consumption by country ([article](http://fivethirtyeight.com/datalab/dear-mona-followup-where-do-people-drink-the-most-beer-wine-and-spirits/))
* Reports of UFO sightings ([website](http://www.nuforc.org/webreports.html))

**Pandas Resources:**

* [Pandas Cheat Sheet](https://github.com/pandas-dev/pandas/blob/master/doc/cheatsheet/Pandas_Cheat_Sheet.pdf)
* Browsing or searching the Pandas [API Reference](http://pandas.pydata.org/pandas-docs/stable/api.html) is an excellent way to locate a function even if you don't know its exact name.
* To learn more Pandas, read this [three-part tutorial](http://www.gregreda.com/2013/10/26/intro-to-pandas-data-structures/), 
or review these two excellent (but extremely long) notebooks on Pandas: 
[introduction](https://github.com/fonnesbeck/Bios8366/blob/master/notebooks/Section2_1-Introduction-to-Pandas.ipynb) and 
[data wrangling](https://github.com/fonnesbeck/Bios8366/blob/master/notebooks/Section2_2-Data-Wrangling-with-Pandas.ipynb).
* If you want to go really deep into Pandas (and NumPy), read the book [Python for Data Analysis](http://amzn.to/1JomygU), written by the creator of Pandas.
* This notebook demonstrates the different types of [joins in Pandas](notebooks/05_pandas_merge.ipynb), for when you need to figure out how to merge two DataFrames.
* This is a nice, short tutorial on [pivot tables](https://beta.oreilly.com/learning/pivot-tables) in Pandas.

For more information on this topic, check out the following resources:

- [List of Resources from Data School](http://www.dataschool.io/best-python-pandas-resources/)
- [Another EDA Tutorial](https://www.datacamp.com/community/tutorials/exploratory-data-analysis-python#gs.T3TSKbk)
- [A discussion forum on the topic](https://www.kaggle.com/general/12796)

---

### Class 5 Visualizations

For more information on this topic, check out the following resources:

- [SAS Data Viz Guide](http://www.sas.com/en_us/insights/big-data/data-visualization.html)
- [Professor Shafer's Guide to Viz Attributes](http://mediashift.org/2016/02/checklist-does-your-data-visualization-say-what-you-think-it-says/)
- [Tableau's Guide to Data Viz](https://drive.google.com/file/d/0Bx2SHQGVqWasT1l4NWtLclJJcWM/view)
- Documentation for [Matplotlib](https://matplotlib.org/), [Seaborn](https://seaborn.pydata.org/), and [Pandas Plotting](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.plot.html)

* [Harvard's Data Science course](http://cs109.github.io/2014/) includes an excellent lecture on [Visualization Goals, Data Types, and Statistical Graphs](http://cm.dce.harvard.edu/2015/01/14328/L03/screen_H264LargeTalkingHead-16x9.shtml) (83 minutes), for which the [slides](https://docs.google.com/file/d/0B7IVstmtIvlHLTdTbXdEVENoRzQ/edit) are also available.
* Watch [Look at Your Data](https://www.youtube.com/watch?v=coNDCIMH8bk) (18 minutes) for an excellent example of why visualization is useful for understanding your data.
* For more on Pandas plotting, read this [notebook](https://github.com/fonnesbeck/Bios8366/blob/master/notebooks/Section2_7-Plotting-with-Pandas.ipynb) or the [visualization page](http://pandas.pydata.org/pandas-docs/stable/visualization.html) from the official Pandas documentation.
* To learn how to customize your plots further, browse through this [notebook on matplotlib](https://github.com/fonnesbeck/Bios8366/blob/master/notebooks/Section2_4-Matplotlib.ipynb) or this [similar notebook](https://github.com/jrjohansson/scientific-python-lectures/blob/master/Lecture-4-Matplotlib.ipynb).
* Read [Overview of Python Visualization Tools](http://pbpython.com/visualization-tools-1.html) for a useful comparison of Matplotlib, Pandas, Seaborn, ggplot, Bokeh, Pygal, and Plotly.
* To explore different types of visualizations and when to use them, [Choosing a Good Chart](http://extremepresentation.typepad.com/files/choosing-a-good-chart-09.pdf) and [The Graphic Continuum](http://www.coolinfographics.com/storage/post-images/The-Graphic-Continuum-POSTER.jpg) are nice one-page references, and the interactive [R Graph Catalog](http://shiny.stat.ubc.ca/r-graph-catalog/) has handy filtering capabilities.
* This [PowerPoint presentation](http://www2.research.att.com/~volinsky/DataMining/Columbia2011/Slides/Topic2-EDAViz.ppt) from Columbia's Data Mining class contains lots of good advice for properly using different types of visualizations.
* Jake VanderPlas Presentation: [The Python Visualization Landscape PyCon 2017](https://www.youtube.com/watch?v=FytuB8nFHPQ)


**Seaborn Resources:**
* To get started with Seaborn for visualization, the official website has a series of [detailed tutorials](http://web.stanford.edu/~mwaskom/software/seaborn/tutorial.html) and an [example gallery](http://web.stanford.edu/~mwaskom/software/seaborn/examples/index.html).
* [Data visualization with Seaborn](https://beta.oreilly.com/learning/data-visualization-with-seaborn) is a quick tour of some of the popular types of Seaborn plots.
* [Visualizing Google Forms Data with Seaborn](http://pbpython.com/pandas-google-forms-part2.html) and [How to Create NBA Shot Charts in Python](http://savvastjortjoglou.com/nba-shot-sharts.html) are both good examples of Seaborn usage on real-world data.

---
### Class 6 Flex lesson - Reinforcement & In Class Practice
---
### Class 7: Statistics in Python

## Additional Resources

For more information on this topic, check out the following resources:

- Scikit Learn Documentation:
	- [Linear Model](http://scikit-learn.org/stable/modules/linear_model.html)
	- [Dummy Estimators](http://scikit-learn.org/stable/modules/model_evaluation.html#dummy-estimators)
- Useful Wikipedia Pages:
	- [Bessel's Correction](https://en.wikipedia.org/wiki/Bessel%27s_correction)Sample variance is incredibly complicated once you look into it, but that makes it one of the simplest examples of meaningful bias and variance.
	- [Mean Squared Error](https://en.wikipedia.org/wiki/Mean_squared_error) In many fields we obsess about unbiased estimators, in machine learning we obsess about MSE. More examples of the sample variance estimator. 
- [Think Stats E-Book](http://greenteapress.com/wp/think-stats-2e/)
- [A great tour of self-guided resources to learn stats relevant to data science](http://machinelearningmastery.com/linear-algebra-machine-learning/) 

Statistical References:
* Understanding the differences between descriptive, inference, predictive and other [Statistical Methods](https://www.d.umn.edu/~kgilbert/ened5560-1/The%20Research%20Question-2015-Leek-1314-5.pdf)
* Fantastic book on mathmatics for machine learning [Mathematics for Machine Learning](https://mml-book.github.io/)
* A great start [Elements of Statistical Learning](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)
* [Bayesian Data Analysis, by Andrew Gelman](http://www.stat.columbia.edu/~gelman/book/)
* [Machine Learning: a Probabilistic Perspective](https://www.cs.ubc.ca/~murphyk/MLbook/)
* [Pattern Recognition and Machine Learning](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf)
* And of course my personal [favorite](https://www.wiley.com/en-us/Statistics+for+Terrified+Biologists-p-9781405149563)

**Statistics Resources:**
* Read [How Software in Half of NYC Cabs Generates $5.2 Million a Year in Extra Tips](http://iquantny.tumblr.com/post/107245431809/how-software-in-half-of-nyc-cabs-generates-5-2) for an excellent example of exploratory data analysis.
* Read [Anscombe's Quartet, and Why Summary Statistics Don't Tell the Whole Story](http://data.heapanalytics.com/anscombes-quartet-and-why-summary-statistics-dont-tell-the-whole-story/) for a classic example of why visualization is useful.
* [What I do when I get a new data set as told through tweets](http://simplystatistics.org/2014/06/13/what-i-do-when-i-get-a-new-data-set-as-told-through-tweets/) is a fun (yet enlightening) look at the process of exploratory data analysis.
* [Khan Academy Statistics and Probabiliy](https://www.khanacademy.org/math/statistics-probability) Good refresher if you need it.
* [ThinkStats](http://greenteapress.com/thinkstats/) Good statistics book with Python code in NumPy and Pandas.
* [Bias of a estimator](https://en.wikipedia.org/wiki/Bias_of_an_estimator) More on bias of the sample variance estimator.
* [Understanding the Bias-Variance Tradeoff](http://scott.fortmann-roe.com/docs/BiasVariance.html) Deep topic that we will dive into later in the course, worth a preview.
* [Understanding when to standardize vs when to normalize](https://medium.com/@rrfd/standardize-or-normalize-examples-in-python-e3f174b65dfc)

### Class 8 Experiments Hypothesis Testing

For more information on this topic, check out the following resources:
- [Survivorship Bias - Abraham Wald and the Statistical Research Group](https://medium.com/@penguinpress/an-excerpt-from-how-not-to-be-wrong-by-jordan-ellenberg-664e708cfc3d)
- [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)
- [The more advanced book: Elements of Statistical Learning](http://web.stanford.edu/~hastie/ElemStatLearn/)
- [Spurious Correlations](http://www.tylervigen.com/spurious-correlations)
- Wikipedia pages on [ANOVA](https://en.wikipedia.org/wiki/Analysis_of_variance), [Welch's t-test](https://en.wikipedia.org/wiki/Welch's_t-test), [Mann-Whitney test](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test)
- For a brief introduction to confidence intervals, hypothesis testing, p-values, and R-squared, as well as a comparison between scikit-learn code and [Statsmodels](http://statsmodels.sourceforge.net/) code, read this [lesson on linear regression](https://github.com/justmarkham/DAT7/blob/master/notebooks/10_linear_regression.ipynb).
- Here is a useful explanation of [confidence intervals](http://www.quora.com/What-is-a-confidence-interval-in-laymans-terms/answer/Michael-Hochster) from Quora.
- [Hypothesis Testing: The Basics](http://20bits.com/article/hypothesis-testing-the-basics) provides a nice overview of the topic, and John Rauser's talk on [Statistics Without the Agonizing Pain](https://www.youtube.com/watch?v=5Dnw46eC-0o) (12 minutes) gives a great explanation of how the null hypothesis is rejected.




---
### Class 9: Linear Regression
 

**Linear Regression Resources:**
* [Ben Lorica: Six reasons why I recommend scikit-learn](http://radar.oreilly.com/2013/12/six-reasons-why-i-recommend-scikit-learn.html)
* To go much more in-depth on linear regression, read Chapter 3 of [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/). Alternatively, watch the [related videos](http://www.dataschool.io/15-hours-of-expert-machine-learning-videos/) or read my [quick reference guide](http://www.dataschool.io/applying-and-interpreting-linear-regression/) to the key points in that chapter.
* This [introduction to linear regression](http://people.duke.edu/~rnau/regintro.htm) is more detailed and mathematically thorough, and includes lots of good advice.
- [Analytics Vidhya's Compilation of Linear Regression Blogs](https://www.analyticsvidhya.com/blog/tag/linear-regression/)
- [Data School's "Friendly Introduction to Linear Regression" using Python](http://www.dataschool.io/linear-regression-in-python/)
* This is a relatively quick post on the [assumptions of linear regression](http://pareonline.net/getvn.asp?n=2&v=8).
* Setosa has an [interactive visualization](http://setosa.io/ev/ordinary-least-squares-regression/) of linear regression.
* For a brief introduction to confidence intervals, hypothesis testing, p-values, and R-squared, as well as a comparison between scikit-learn code and [Statsmodels](http://statsmodels.sourceforge.net/) code, read my [DAT7 lesson on linear regression](https://github.com/justmarkham/DAT7/blob/master/notebooks/10_linear_regression.ipynb).
* Here is a useful explanation of [confidence intervals](http://www.quora.com/What-is-a-confidence-interval-in-laymans-terms/answer/Michael-Hochster) from Quora.
* [Hypothesis Testing: The Basics](http://20bits.com/article/hypothesis-testing-the-basics) provides a nice overview of the topic, and John Rauser's talk on [Statistics Without the Agonizing Pain](https://www.youtube.com/watch?v=5Dnw46eC-0o) (12 minutes) gives a great explanation of how the null hypothesis is rejected.
* Earlier this year, a major scientific journal banned the use of p-values:
* Scientific American has a nice [summary](http://www.scientificamerican.com/article/scientists-perturbed-by-loss-of-stat-tools-to-sift-research-fudge-from-fact/) of the ban.
* This [response](http://www.nature.com/news/statistics-p-values-are-just-the-tip-of-the-iceberg-1.17412) to the ban in Nature argues that "decisions that are made earlier in data analysis have a much greater impact on results".
* Andrew Gelman has a readable [paper](http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf) in which he argues that "it's easy to find a p < .05 comparison even if nothing is going on, if you look hard enough".
* [Science Isn't Broken](http://fivethirtyeight.com/features/science-isnt-broken/) includes a neat tool that allows you to "p-hack" your way to "statistically significant" results.
* [Accurately Measuring Model Prediction Error](http://scott.fortmann-roe.com/docs/MeasuringError.html) compares adjusted R-squared, AIC and BIC, train/test split, and cross-validation.

**Regularization Resources:**
* What is a norm: [l0-Norm, l1-Norm, l2-Norm, … , l-infinity Norm](https://rorasa.wordpress.com/2012/05/13/l0-norm-l1-norm-l2-norm-l-infinity-norm/)
* An Introduction to Statistical Learning has useful videos on [ridge regression](https://www.youtube.com/watch?v=cSKzqb0EKS0&list=PL5-da3qGB5IB-Xdpj_uXJpLGiRfv9UVXI&index=6) (13 minutes), and [lasso regression](https://www.youtube.com/watch?v=A5I1G1MfUmA&index=7&list=PL5-da3qGB5IB-Xdpj_uXJpLGiRfv9UVXI) (15 minutes).
* Caltech's Learning From Data course has a great video introducing [regularization](http://work.caltech.edu/library/121.html) (8 minutes) that builds upon their video about the [bias-variance tradeoff](http://work.caltech.edu/library/081.html).
- Scikit-learn examples for [Lasso](http://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_lars.html) and [Ridge](http://scikit-learn.org/stable/auto_examples/linear_model/plot_ridge_path.html) Regression
- Scikit-learn documentation for [Lasso](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html),  [Ridge](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html), and [Elastic Net](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) Regression

---
### Class 10 Logistic Regression

For more information on this topic, check out the following resources:

- [Sklearn Logistic Regression Documentation](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwj-ytGQkZjVAhWHej4KHaOcCnYQFggzMAE&url=http%3A%2F%2Fscikit-learn.org%2Fstable%2Fmodules%2Fgenerated%2Fsklearn.linear_model.LogisticRegression.html&usg=AFQjCNGpSyUzpbaClG8IQEPJmB63CQZlrg)
- [Data School: Logistic Regression In-Depth](http://www.dataschool.io/guide-to-logistic-regression/)
- [Logistic Regression for Machine Learning](http://machinelearningmastery.com/logistic-regression-for-machine-learning/)
- [Video: Andrew Ng on Logistic Regression](https://www.youtube.com/watch?v=LLx4diIP83I)

**Logistic Regression Resources:**

* Better understand [Confusion Matrices](https://machinelearningmastery.com/confusion-matrix-machine-learning/)
* To go deeper into logistic regression, read the first three sections of Chapter 4 of [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/), or watch the [first three videos](http://www.dataschool.io/15-hours-of-expert-machine-learning-videos/) (30 minutes) from that chapter.
* For a more mathematical explanation of logistic regression, watch the first seven videos (71 minutes) from week 3 of Andrew Ng's [machine learning course](https://www.coursera.org/learn/machine-learning/home/info), or read the [related lecture notes](http://www.holehouse.org/mlclass/06_Logistic_Regression.html) compiled by a student.
* For more on interpreting logistic regression coefficients, read this excellent [guide](http://www.ats.ucla.edu/stat/mult_pkg/faq/general/odds_ratio.htm) by UCLA's IDRE and these [lecture notes](http://www.unm.edu/~schrader/biostat/bio2/Spr06/lec11.pdf) from the University of New Mexico.
* The scikit-learn documentation has a nice [explanation](http://scikit-learn.org/stable/modules/calibration.html) of what it means for a predicted probability to be calibrated.
* [Supervised learning superstitions cheat sheet](http://ryancompton.net/assets/ml_cheat_sheet/supervised_learning.html) is a very nice comparison of four classifiers we cover in the course (logistic regression, decision trees, KNN, Naive Bayes) and one classifier we do not cover (Support Vector Machines).
* What is the [C hyperparameter for Logistic Regression](http://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_l1_l2_sparsity.html)
* What is the [difference between the sigmoid and softmax function](http://dataaspirant.com/2017/03/07/difference-between-softmax-function-and-sigmoid-function/)
* How does [multinomial logistic regression works](http://dataaspirant.com/2017/03/14/multinomial-logistic-regression-model-works-machine-learning/)
* Great paper on "A Comparison of Logistic Regression, k-Nearest Neighbor, and Decision Tree Induction for Campaign Management"  (https://pdfs.semanticscholar.org/24b0/2fc8b438d9a432ad72111ef2d80f8c148c1c.pdf)
---
### Class 11 Train Test Split & Bias/Variance Trade Off
For more information on this topic, check out the following resources:

- [Understanding the Bias-Variance Tradeoff](http://scott.fortmann-roe.com/docs/BiasVariance.html) compares adjusted R-squared, AIC and BIC, train/test split, and cross-validation.
- [University of Washington Machine Learning Course Slides](https://courses.cs.washington.edu/courses/cse546/12wi/slides/)
- [An Intuitive Explanation of Overfitting](https://www.quora.com/What-is-an-intuitive-explanation-of-overfitting/answer/Jessica-Su)
- Caltech's Learning From Data course has a great video introducing [regularization](http://work.caltech.edu/library/121.html) (8 minutes) that builds upon their video about the [bias-variance tradeoff](http://work.caltech.edu/library/081.html).
- [Approaches to feature selection for machine learning in Python](https://machinelearningmastery.com/feature-selection-machine-learning-python/)
---
### Class 12 K Nearest Neighbors (KNN), Classifiers, Preprocessing and GridSearch

For more information on this topic, check out the following resources:

- [Data School: Machine Learning With KNN](http://blog.kaggle.com/2015/04/30/scikit-learn-video-4-model-training-and-prediction-with-k-nearest-neighbors/)
- [KNN: Dangerously Simple](https://mathbabe.org/2013/04/04/k-nearest-neighbors-dangerously-simple/)
- [KNN From Scratch](http://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/)
- [Detailed Intro to KNN](https://saravananthirumuruganathan.wordpress.com/2010/05/17/a-detailed-introduction-to-k-nearest-neighbor-knn-algorithm/)is a bit dense, but provides a more thorough introduction to KNN and its applications.
- [Stanford's Machine Learning Course: KNN](http://cs231n.github.io/classification/#nn)


**KNN Resources:**
* For a recap of the key points about KNN and scikit-learn, watch [Getting started in scikit-learn with the famous iris dataset](https://www.youtube.com/watch?v=hd1W4CyPX58) (15 minutes) and [Training a machine learning model with scikit-learn](https://www.youtube.com/watch?v=RlQuVL6-qe8) (20 minutes).
* KNN supports [distance metrics](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html) other than Euclidean distance, such as [Mahalanobis distance](http://stats.stackexchange.com/questions/62092/bottom-to-top-explanation-of-the-mahalanobis-distance), which [takes the scale of the data into account](http://blogs.sas.com/content/iml/2012/02/15/what-is-mahalanobis-distance.html).
* This lecture on [Image Classification](http://cs231n.github.io/classification/) shows how KNN could be used for detecting similar images, and also touches on topics we will cover in future classes (hyperparameter tuning and cross-validation).
* Some applications for which KNN is well-suited are [object recognition](http://vlm1.uta.edu/~athitsos/nearest_neighbors/), [satellite image enhancement](http://land.umn.edu/documents/FS6.pdf), [document categorization](http://www.ceng.metu.edu.tr/~e120321/paper.pdf), and [gene expression analysis](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.208.993).

**scikit-learn Resources:**
* scikit-learn's [machine learning map](http://scikit-learn.org/stable/tutorial/machine_learning_map/) may help you to choose the "best" model for your task.
* [Choosing a Machine Learning Classifier](http://blog.echen.me/2011/04/27/choosing-a-machine-learning-classifier/) is a short and highly readable comparison of several classification models, [Comparing supervised learning algorithms](http://www.dataschool.io/comparing-supervised-learning-algorithms/) is a model comparison table that I created, and [Supervised learning superstitions cheat sheet](http://ryancompton.net/assets/ml_cheat_sheet/supervised_learning.html) is a more thorough comparison (with links to lots of useful resources).
* [Machine Learning Done Wrong](http://ml.posthaven.com/machine-learning-done-wrong), [Machine Learning Gremlins](https://www.youtube.com/watch?v=tleeC-KlsKA) (31 minutes), [Clever Methods of Overfitting](http://hunch.net/?p=22), and [Common Pitfalls in Machine Learning](http://danielnee.com/?p=155) all offer thoughtful advice on how to avoid common mistakes in machine learning.
* [Practical machine learning tricks from the KDD 2011 best industry paper](http://blog.david-andrzejewski.com/machine-learning/practical-machine-learning-tricks-from-the-kdd-2011-best-industry-paper/) and Andrew Ng's [Advice for applying machine learning](http://cs229.stanford.edu/materials/ML-advice.pdf) include slightly more advanced advice than the resources above.
* [An Empirical Comparison of Supervised Learning Algorithms](http://www.cs.cornell.edu/~caruana/ctp/ct.papers/caruana.icml06.pdf) is a readable research paper from 2006, which was also presented as a [talk](http://videolectures.net/solomon_caruana_wslmw/) (77 minutes).
API design for machine learning software: [Experiences from the scikit-learn project](https://arxiv.org/pdf/1309.0238.pdf)
Machine learning: [artificial intelligence bias](https://apple.news/AhIix6CL6STKQo06jfOICDw)


**Fundamental Statistics**
* [causal-data-science](https://medium.com/@akelleh/causal-data-science-721ed63a4027) Great series about DAGs, association, and causation.
* [Khan Academy Statistics and Probabiliy](https://www.khanacademy.org/math/statistics-probability) Still useful for basic topics.


### Class 13 Clustering


**Clustering Resources:**
* K-means: [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html), [visualization 1](http://tech.nitoyon.com/en/blog/2013/11/07/k-means/), [visualization 2](http://www.naftaliharris.com/blog/visualizing-k-means-clustering/)
* DBSCAN: [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html), [visualization](http://www.naftaliharris.com/blog/visualizing-dbscan-clustering/)
* For a very thorough introduction to clustering, read chapter 8 (69 pages) of [Introduction to Data Mining](http://www-users.cs.umn.edu/~kumar/dmbook/index.php) (available as a free download), or browse through the chapter 8 slides.
* scikit-learn's user guide compares many different [types of clustering](http://scikit-learn.org/stable/modules/clustering.html).
* This [PowerPoint presentation](http://www2.research.att.com/~volinsky/DataMining/Columbia2011/Slides/Topic6-Clustering.ppt) from Columbia's Data Mining class provides a good introduction to clustering, including hierarchical clustering and alternative distance metrics.
* An Introduction to Statistical Learning has useful videos on [K-means clustering](https://www.youtube.com/watch?v=aIybuNt9ps4&list=PL5-da3qGB5IBC-MneTc9oBZz0C6kNJ-f2) (17 minutes) and [hierarchical clustering](https://www.youtube.com/watch?v=Tuuc9Y06tAc&list=PL5-da3qGB5IBC-MneTc9oBZz0C6kNJ-f2) (15 minutes).
* This is an excellent interactive visualization of [hierarchical clustering](https://joyofdata.shinyapps.io/hclust-shiny/).
* This is a nice animated explanation of [mean shift clustering](http://spin.atomicobject.com/2015/05/26/mean-shift-clustering/).
* The [K-modes algorithm](http://www.cs.ust.hk/~qyang/Teaching/537/Papers/huang98extensions.pdf) can be used for clustering datasets of categorical features without converting them to numerical values. Here is a [Python implementation](https://github.com/nicodv/kmodes).
* Here are some fun examples of clustering: [A Statistical Analysis of the Work of Bob Ross](http://fivethirtyeight.com/features/a-statistical-analysis-of-the-work-of-bob-ross/) (with [data and Python code](https://github.com/fivethirtyeight/data/tree/master/bob-ross)), [How a Math Genius Hacked OkCupid to Find True Love](http://www.wired.com/2014/01/how-to-hack-okcupid/all/), and [characteristics of your zip code](http://www.esri.com/landing-pages/tapestry/).

## Additional Resources
- [Scikit-learn Clustering Methods](http://scikit-learn.org/stable/modules/clustering.html)
- [K-Means Clustering (video)](https://www.youtube.com/watch?v=0MQEt10e4NM)
- [Clustering Overview](http://www.holehouse.org/mlclass/13_Clustering.html)
- [Cluster Analysis and K-Means (PDF)](http://www-users.cs.umn.edu/~kumar/dmbook/ch8.pdf)
- [K-Means Wikipedia Article](http://en.wikipedia.org/wiki/K-means_clustering)

-----

### Class 14: Decision Trees and Random Forests

**Decision Trees Resources**
* [Introduction to Statistical Learning - Chapter 8 (Tree-Based Methods)](http://www-bcf.usc.edu/~gareth/ISL/) This book provides a fantastic introduction to machine learning models and the statistics behind them. The visuals and explanations are really easy to understand. PDF available to download on the website.
* scikit-learn's documentation on [decision trees](http://scikit-learn.org/stable/modules/tree.html) includes a nice overview of trees as well as tips for proper usage.
* For a more thorough introduction to decision trees, read section 4.3 (23 pages) of [Introduction to Data Mining](http://www-users.cs.umn.edu/~kumar/dmbook/index.php). (Chapter 4 is available as a free download.)
* If you want to go deep into the different decision tree algorithms, this slide deck contains [A Brief History of Classification and Regression Trees](https://drive.google.com/file/d/0B-BKohKl-jUYQ3RpMEF0OGRUU3RHVGpHY203NFd3Z19Nc1ZF/view).
* [The Science of Singing Along](http://www.doc.gold.ac.uk/~mas03dm/papers/PawleyMullensiefen_Singalong_2012.pdf) contains a neat regression tree (page 136) for predicting the percentage of an audience at a music venue that will sing along to a pop song.
* Decision trees are common in the medical field for differential diagnosis, such as this classification tree for [identifying psychosis](http://www.psychcongress.com/sites/naccme.com/files/images/pcn/saundras/psychosis_decision_tree.pdf).
* [Induction of Decision Trees](http://hunch.net/~coms-4771/quinlan.pdf)
*[Top 10 algorithms in data mining](http://www.cs.uvm.edu/~icdm/algorithms/10Algorithms-08.pdf)

**Ensembling Resources:**
* scikit-learn's documentation on [ensemble methods](http://scikit-learn.org/stable/modules/ensemble.html) covers both "averaging methods" (such as bagging and Random Forests) as well as "boosting methods" (such as AdaBoost and Gradient Tree Boosting).
* MLWave's [Kaggle Ensembling Guide](http://mlwave.com/kaggle-ensembling-guide/) is very thorough and shows the many different ways that ensembling can take place.
* Browse the excellent [solution paper](https://docs.google.com/viewer?url=https://raw.githubusercontent.com/ChenglongChen/Kaggle_CrowdFlower/master/Doc/Kaggle_CrowdFlower_ChenglongChen.pdf) from the winner of Kaggle's [CrowdFlower competition](https://www.kaggle.com/c/crowdflower-search-relevance) for an example of the work and insight required to win a Kaggle competition.
* [Interpretable vs Powerful Predictive Models: Why We Need Them Both](https://medium.com/@chris_bour/interpretable-vs-powerful-predictive-models-why-we-need-them-both-990340074979) is a short post on how the tactics useful in a Kaggle competition are not always useful in the real world.
* [Not Even the People Who Write Algorithms Really Know How They Work](http://www.theatlantic.com/technology/archive/2015/09/not-even-the-people-who-write-algorithms-really-know-how-they-work/406099/) argues that the decreased interpretability of state-of-the-art machine learning models has a negative impact on society.
* For an intuitive explanation of Random Forests, read Edwin Chen's answer to [How do random forests work in layman's terms?](http://www.quora.com/Random-Forests/How-do-random-forests-work-in-laymans-terms/answer/Edwin-Chen-1)
* [Large Scale Decision Forests: Lessons Learned](http://blog.siftscience.com/blog/2015/large-scale-decision-forests-lessons-learned) is an excellent post from Sift Science about their custom implementation of Random Forests.
* [Unboxing the Random Forest Classifier](http://nerds.airbnb.com/unboxing-the-random-forest-classifier/) describes a way to interpret the inner workings of Random Forests beyond just feature importances.
* [Understanding Random Forests: From Theory to Practice](http://arxiv.org/pdf/1407.7502v3.pdf) is an in-depth academic analysis of Random Forests, including details of its implementation in scikit-learn.
* [Global model tuned to user preference over time](http://time.com/3950525/facebook-news-feed-algorithm/)

**Class Resources**
* CHAPTER 9 - [Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn//) - This book is by most of the same authors as the previous book, but goes into more detail. PDF available to download on the website.
* CHAPTER 8 - [Applied Predictive Modeling](https://www.amazon.com/Applied-Predictive-Modeling-Max-Kuhn/dp/1461468485) - While this book features R code, the discussion of different predictive models and sampling methodologies are hard to beat.

---
### Class 15: Natural Language Processing (NLP) 1

**NLP Resources:**
* If you want to learn a lot more NLP, check out the excellent [video lectures](https://class.coursera.org/nlp/lecture) and [slides](http://web.stanford.edu/~jurafsky/NLPCourseraSlides.html) from this [Coursera course](https://www.coursera.org/course/nlp) (which is no longer being offered).
* This slide deck defines many of the [key NLP terms](https://github.com/ga-students/DAT_SF_9/blob/master/16_Text_Mining/DAT9_lec16_Text_Mining.pdf).
* [Natural Language Processing with Python](http://www.nltk.org/book/) is the most popular book for going in-depth with the [Natural Language Toolkit](http://www.nltk.org/) (NLTK).
* [A Smattering of NLP in Python](https://github.com/charlieg/A-Smattering-of-NLP-in-Python/blob/master/A%20Smattering%20of%20NLP%20in%20Python.ipynb) provides a nice overview of NLTK, as does this [notebook from DAT5](https://github.com/justmarkham/DAT5/blob/master/notebooks/14_nlp.ipynb).
* [spaCy](http://spacy.io/) is a newer Python library for text processing that is focused on performance (unlike NLTK).
* If you want to get serious about NLP, [Stanford CoreNLP](http://nlp.stanford.edu/software/corenlp.shtml) is a suite of tools (written in Java) that is highly regarded.
* When working with a large text corpus in scikit-learn, [HashingVectorizer](http://scikit-learn.org/stable/modules/feature_extraction.html#vectorizing-a-large-text-corpus-with-the-hashing-trick) is a useful alternative to CountVectorizer.
* [Automatically Categorizing Yelp Businesses](http://engineeringblog.yelp.com/2015/09/automatically-categorizing-yelp-businesses.html) discusses how Yelp uses NLP and scikit-learn to solve the problem of uncategorized businesses.
* [Modern Methods for Sentiment Analysis](http://districtdatalabs.silvrback.com/modern-methods-for-sentiment-analysis) shows how "word vectors" can be used for more accurate sentiment analysis.
* [Identifying Humorous Cartoon Captions](http://www.cs.huji.ac.il/~dshahaf/pHumor.pdf) is a readable paper about identifying funny captions submitted to the New Yorker Caption Contest.
* [DC Natural Language Processing](http://www.meetup.com/DC-NLP/) is an active Meetup group in our local area.
* BM25 (a better tfidf) [takes document length into account when determining term importance](https://berlinbuzzwords.de/sites/berlinbuzzwords.de/files/media/documents/bm25_buzzwords_britta_weber.pdf)

------

### Natural Language Processing (NLP) Part 2

**Naive Bayes Resources:**
* Sebastian Raschka's article on [Naive Bayes and Text Classification](http://sebastianraschka.com/Articles/2014_naive_bayes_1.html) covers the conceptual material from today's class in much more detail.
* For more on conditional probability, read these [slides](https://docs.google.com/presentation/d/1psUIyig6OxHQngGEHr3TMkCvhdLInnKnclQoNUr4G4U/edit#slide=id.gfc69f484_00), or read section 2.2 of the [OpenIntro Statistics textbook](https://www.openintro.org/stat/textbook.php?stat_book=os) (15 pages).
* For an intuitive explanation of Naive Bayes classification, read this post on [airport security](http://www.quora.com/In-laymans-terms-how-does-Naive-Bayes-work/answer/Konstantin-Tt).
* For more details on Naive Bayes classification, Wikipedia has two excellent articles ([Naive Bayes classifier](http://en.wikipedia.org/wiki/Naive_Bayes_classifier) and [Naive Bayes spam filtering](http://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering)), and Cross Validated has a good [Q&A](http://stats.stackexchange.com/questions/21822/understanding-naive-bayes).
* When applying Naive Bayes classification to a dataset with continuous features, it is better to use [GaussianNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) rather than [MultinomialNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html). This [notebook](notebooks/14_types_of_naive_bayes.ipynb) compares their performances on such a dataset. Wikipedia has a short [description](https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Gaussian_naive_Bayes) of Gaussian Naive Bayes, as well as an excellent [example](https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Sex_classification) of its usage.
* These [slides](http://www.umiacs.umd.edu/~jbg/teaching/DATA_DIGGING/lecture_05.pdf) from the University of Maryland provide more mathematical details on both logistic regression and Naive Bayes, and also explain how Naive Bayes is actually a "special case" of logistic regression.
* Andrew Ng has a [paper](http://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf) comparing the performance of logistic regression and Naive Bayes across a variety of datasets.
* If you enjoyed Paul Graham's article, you can read [his follow-up article](http://www.paulgraham.com/better.html) on how he improved his spam filter and this [related paper](http://www.merl.com/publications/docs/TR2004-091.pdf) about state-of-the-art spam filtering in 2004.
* Yelp has found that Naive Bayes is more effective than Mechanical Turks at [categorizing businesses](http://engineeringblog.yelp.com/2011/02/towards-building-a-high-quality-workforce-with-mechanical-turk.html).


--- 
### Class 16: Time Series Analysis

**Class Resources** 

* If you are interested in more resources, check out the following:
     * In Pandas' datetime library, search for more information on .dt [here](http://pandas.pydata.org/pandas-docs/stable/api.html).
     * For additional review of these concepts, see some inspiration from the [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/03.11-working-with-time-series.html).
     * There are lots of additional tutorials on ARIMA models out there; here is a [good one](http://www.statsref.com/HTML/index.html?arima.html).

**Additional Resources**
* Facebook Prophet is a phenomenal package allowing powerful, fast and efficient time series analysis. 
     * Overview of [Prophet](https://facebook.github.io/prophet)
     * Learning [Notebooks](https://github.com/facebook/prophet/tree/master/notebooks)
     * Prophet [Resources](https://github.com/facebook/prophet)
* Overview of [time series applications](https://towardsdatascience.com/time-series-analysis-in-python-an-introduction-70d5a5b1d52a)

---
### Class 17: Working with data - Api's and Web Scraping

**Class Resources:**
* APIs ([code](https://github.com/ga-students/ds-dc-24/blob/master/1_lessons/Lesson06_Web_Scraping_and_APIs/starter-code/api.py))
    * [OMDb API](http://www.omdbapi.com/)
* Web scraping ([code](https://github.com/ga-students/ds-dc-24/blob/master/1_lessons/Lesson06_Web_Scraping_and_APIs/starter-code/web_scraping.py))
    * [IMDb: robots.txt](http://www.imdb.com/robots.txt)
    * [Example web page](https://github.com/ga-students/ds-dc-24/blob/master/2_dataset/example.html)
    * [IMDb: The Shawshank Redemption](http://www.imdb.com/title/tt0111161/)
* [Autocomplete in Spyder](http://stackoverflow.com/questions/18044312/spyder-does-not-autocomplete-local-variables)

**Web Scraping Resources:**
* The [Beautiful Soup documentation](http://www.crummy.com/software/BeautifulSoup/bs4/doc/) is incredibly thorough, but is hard to use as a reference guide. However, the section on [specifying a parser](http://www.crummy.com/software/BeautifulSoup/bs4/doc/#specifying-the-parser-to-use) may be helpful if Beautiful Soup appears to be parsing a page incorrectly.
* For more Beautiful Soup examples and tutorials, see [Web Scraping 101 with Python](http://www.gregreda.com/2013/03/03/web-scraping-101-with-python/), this [notebook](http://web.stanford.edu/~zlotnick/TextAsData/Web_Scraping_with_Beautiful_Soup.html) from Stanford's Text As Data course, and this [notebook](http://nbviewer.ipython.org/github/cs109/2014/blob/master/lectures/2014_09_23-lecture/data_scraping_transcript.ipynb) and associated [video](http://cm.dce.harvard.edu/2015/01/14328/L07/screen_H264LargeTalkingHead-16x9.shtml) from Harvard's Data Science course.
* For a much longer web scraping tutorial covering Beautiful Soup, lxml, XPath, and Selenium, watch [Web Scraping with Python](https://www.youtube.com/watch?v=p1iX0uxM1w8) (3 hours 23 minutes) from PyCon 2014. The [slides](https://docs.google.com/presentation/d/1uHM_esB13VuSf7O1ScGueisnrtu-6usGFD3fs4z5YCE/edit#slide=id.p) and [code](https://github.com/kjam/python-web-scraping-tutorial) are also available.
* For more complex web scraping projects, [Scrapy](http://scrapy.org/) is a popular application framework that works with Python. It has excellent [documentation](http://doc.scrapy.org/en/1.0/index.html), and here's a [tutorial](https://github.com/rdempsey/ddl-data-wrangling) with detailed slides and code.
* [robotstxt.org](http://www.robotstxt.org/robotstxt.html) has a concise explanation of how to write (and read) the `robots.txt` file.
* [import.io](https://import.io/) and [Kimono](https://www.kimonolabs.com/) claim to allow you to scrape websites without writing any code.
* [How a Math Genius Hacked OkCupid to Find True Love](http://www.wired.com/2014/01/how-to-hack-okcupid/all/) and [How Netflix Reverse Engineered Hollywood](http://www.theatlantic.com/technology/archive/2014/01/how-netflix-reverse-engineered-hollywood/282679/?single_page=true) are two fun examples of how web scraping has been used to build interesting datasets.
* [Be Suspicious Of Online Movie Ratings, Especially Fandango’s](https://fivethirtyeight.com/features/fandango-movies-ratings/) is a interesting example on the application of web scraping from FiveThirtyEight

**API Resources:**
* [Mashape](https://www.mashape.com/explore) and [Apigee](https://apigee.com/providers) allow you to explore tons of different APIs. Alternatively, a [Python API wrapper](http://www.pythonforbeginners.com/api/list-of-python-apis) is available for many popular APIs.
* [API Integration in Python](https://realpython.com/blog/python/api-integration-in-python/) provides a very readable introduction to REST APIs.
* Microsoft's [Face Detection API](https://www.projectoxford.ai/demo/face#detection), which powers [How-Old.net](http://how-old.net/), is a great example of how a machine learning API can be leveraged to produce a compelling web application.

**Selenium Resources:**
* [What is Selenium](http://www.seleniumhq.org/)
* [Chromedriver download](https://chromedriver.storage.googleapis.com/index.html?path=2.25/)
* [Selenium with Python Documentation](http://selenium-python.readthedocs.io/)
* [Selenium Webdriver Python Tutorial For Web Automation](http://www.techbeamers.com/selenium-webdriver-python-tutorial/)

# Additional Resources
### Databases

### Databases and SQL
* This [GA slide deck](https://github.com/justmarkham/DAT5/blob/master/slides/20_sql.pdf) provides a brief introduction to databases and SQL. The [Python script](https://github.com/justmarkham/DAT5/blob/master/code/20_sql.py) from that lesson demonstrates basic SQL queries, as well as how to connect to a SQLite database from Python and how to query it using Pandas.
* The repository for this [SQL Bootcamp](https://github.com/brandonmburroughs/sql_bootcamp) contains an extremely well-commented SQL script that is suitable for walking through on your own.
* This [GA notebook](https://github.com/podopie/DAT18NYC/blob/master/classes/17-relational_databases.ipynb) provides a shorter introduction to databases and SQL that helpfully contrasts SQL queries with Pandas syntax.
* [SQLZOO](http://sqlzoo.net/wiki/SQL_Tutorial), [Mode Analytics](http://sqlschool.modeanalytics.com/), [Khan Academy](https://www.khanacademy.org/computing/computer-programming/sql), [Codecademy](https://www.codecademy.com/courses/learn-sql), [Datamonkey](http://datamonkey.pro/guess_sql/lessons/), and [Code School](http://campus.codeschool.com/courses/try-sql/contents) all have online beginner SQL tutorials that look promising. Code School also offers an [advanced tutorial](https://www.codeschool.com/courses/the-sequel-to-sql/), though it's not free.
* [w3schools](http://www.w3schools.com/sql/trysql.asp?filename=trysql_select_all) has a sample database that allows you to practice SQL from your browser. Similarly, Kaggle allows you to query a large SQLite database of [Reddit Comments](https://www.kaggle.com/c/reddit-comments-may-2015/data) using their online "Scripts" application.
* [What Every Data Scientist Needs to Know about SQL](http://joshualande.com/data-science-sql/) is a brief series of posts about SQL basics, and [Introduction to SQL for Data Scientists](http://bensresearch.com/downloads/SQL.pdf) is a paper with similar goals.
* [10 Easy Steps to a Complete Understanding of SQL](https://web.archive.org/web/20150402234726/http://tech.pro/tutorial/1555/10-easy-steps-to-a-complete-understanding-of-sql) is a good article for those who have some SQL experience and want to understand it at a deeper level.
* SQLite's article on [Query Planning](http://www.sqlite.org/queryplanner.html) explains how SQL queries "work".
* [A Comparison Of Relational Database Management Systems](https://www.digitalocean.com/community/tutorials/sqlite-vs-mysql-vs-postgresql-a-comparison-of-relational-database-management-systems) gives the pros and cons of SQLite, MySQL, and PostgreSQL.
* If you want to go deeper into databases and SQL, Stanford has a well-respected series of [14 mini-courses](https://lagunita.stanford.edu/courses/DB/2014/SelfPaced/about).
* [Blaze](http://blaze.pydata.org) is a Python package enabling you to use Pandas-like syntax to query data living in a variety of data storage systems.
* A data engineers guide to [non-traditional-data-storages](http://www.hadoop360.com/blog/a-data-engineer-s-guide-to-non-traditional-data-storages)
* SQL [Style Guide](http://www.sqlstyle.guide/#naming-conventions)

-----

###  Advanced scikit-learn 

**scikit-learn Resources:**
* This is a longer example of [feature scaling](https://github.com/rasbt/pattern_classification/blob/master/preprocessing/about_standardization_normalization.ipynb) in scikit-learn, with additional discussion of the types of scaling you can use.
* [Practical Data Science in Python](http://radimrehurek.com/data_science_python/) is a long and well-written notebook that uses a few advanced scikit-learn features: pipelining, plotting a learning curve, and pickling a model.
* Sebastian Raschka has a number of excellent resources for scikit-learn users, including a repository of [tutorials and examples](https://github.com/rasbt/pattern_classification), a library of machine learning [tools and extensions](http://rasbt.github.io/mlxtend/), a new [book](https://github.com/rasbt/python-machine-learning-book), and a semi-active [blog](http://sebastianraschka.com/blog/).
* scikit-learn has an incredibly active [mailing list](https://www.mail-archive.com/scikit-learn-general@lists.sourceforge.net/index.html) that is often much more useful than Stack Overflow for researching functions and asking questions.
* If you forget how to use a particular scikit-learn function that we have used in class, don't forget that this repository is fully searchable!

**Pipelines**
* Helper functions: [Pipeline](http://scikit-learn.org/stable/modules/pipeline.html), [GridSearchCV](http://scikit-learn.org/stable/modules/grid_search.html)
* To learn how to use [GridSearchCV and RandomizedSearchCV](http://scikit-learn.org/stable/modules/grid_search.html) for parameter tuning, watch [How to find the best model parameters in scikit-learn](https://www.youtube.com/watch?v=Gol_qOgRqfA) (28 minutes) or read the [associated notebook](https://github.com/justmarkham/scikit-learn-videos/blob/master/08_grid_search.ipynb).
* Pipeline and FeatureUnion: [combining estimators](http://scikit-learn.org/stable/modules/pipeline.html#pipeline)
* Feature Union with [Heterogeneous Data Sourse](http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html#sphx-glr-auto-examples-hetero-feature-union-py)
-----



**Tidy Data**
* [Good Data Management Practices for Data Analysis](https://www.prometheusresearch.com/good-data-management-practices-for-data-analysis-tidy-data-part-2/) briefly summarizes the principles of "tidy data".
* [Hadley Wickham's paper](http://www.jstatsoft.org/article/view/v059i10) explains tidy data in detail and includes lots of good examples.
* Example of a tidy dataset: [Bob Ross](https://github.com/fivethirtyeight/data/blob/master/bob-ross/elements-by-episode.csv)
* Examples of untidy datasets: [NFL ticket prices](https://github.com/fivethirtyeight/data/blob/master/nfl-ticket-prices/2014-average-ticket-price.csv), [airline safety](https://github.com/fivethirtyeight/data/blob/master/airline-safety/airline-safety.csv), [Jets ticket prices](https://github.com/fivethirtyeight/data/blob/master/nfl-ticket-prices/jets-buyer.csv), [Chipotle orders](https://github.com/TheUpshot/chipotle/blob/master/orders.tsv)
* If your co-workers tend to create spreadsheets that are [unreadable by computers](https://bosker.wordpress.com/2014/12/05/the-government-statistical-services-terrible-spreadsheet-advice/), they may benefit from reading these [tips for releasing data in spreadsheets](http://www.clean-sheet.org/). (There are some additional suggestions in this [answer](http://stats.stackexchange.com/questions/83614/best-practices-for-creating-tidy-data/83711#83711) from Cross Validated.)

**Regular Expressions Resources:**
* Google's Python Class includes an excellent [introductory lesson](https://developers.google.com/edu/python/regular-expressions) on regular expressions (which also has an associated [video](https://www.youtube.com/watch?v=kWyoYtvJpe4&index=4&list=PL5-da3qGB5IA5NwDxcEJ5dvt8F9OQP7q5)).
* Python for Informatics has a nice [chapter](http://www.pythonlearn.com/html-270/book012.html) on regular expressions. (If you want to run the examples, you'll need to download [mbox.txt](http://www.py4inf.com/code/mbox.txt) and [mbox-short.txt](http://www.py4inf.com/code/mbox-short.txt).)
* [Breaking the Ice with Regular Expressions](https://www.codeschool.com/courses/breaking-the-ice-with-regular-expressions/) is an interactive Code School course, though only the first "level" is free.
* If you want to go really deep with regular expressions, [RexEgg](http://www.rexegg.com/) includes endless articles and tutorials.
* [5 Tools You Didn't Know That Use Regular Expressions](http://blog.codeschool.io/2015/07/30/5-tools-you-didnt-know-that-use-regular-expressions/) demonstrates how regular expressions can be used with Excel, Word, Google Spreadsheets, Google Forms, text editors, and other tools.
* [Exploring Expressions of Emotions in GitHub Commit Messages](http://geeksta.net/geeklog/exploring-expressions-emotions-github-commit-messages/) is a fun example of how regular expressions can be used for data analysis, and [Emojineering](http://instagram-engineering.tumblr.com/post/118304328152/emojineering-part-2-implementing-hashtag-emoji) explains how Instagram uses regular expressions to detect emoji in hashtags.

**Feature Selection**
* [Feature Selection](http://blog.datadive.net/selecting-good-features-part-i-univariate-selection/)

**Dimensionality Reduction Resources:**
* A more thorough and friendly introduction to [PCA](http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html)
* Chapters 6 and 10 in [ISLR](http://www-bcf.usc.edu/~gareth/ISL/) cover feature selection and dimensionality reduction in an accessible manner.
* A slightly more in-depth discussion of the [kernel trick](http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html).
* [introduction-to-principal-component-analysis](http://www.datasciencecentral.com/profiles/blogs/introduction-to-principal-component-analysis)
